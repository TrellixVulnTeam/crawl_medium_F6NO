date,title,subtitle,claps,responses,author_url,story_url,reading_time (mins),number_sections,section_titles,number_paragraphs,paragraphs
01/2020,How I went from zero coding skills to data scientist in 6 months,The 4 tools I used to teach myself…,13.6K,84,https://towardsdatascience.com/@katemarielewis,https://towardsdatascience.com/how-i-went-from-zero-coding-skills-to-data-scientist-in-6-months-c2207b65f2f3?source=collection_archive---------0-----------------------,8,10,"['How I went from zero coding skills to data scientist in 6 months', 'Baseline skills', 'Getting started coding', 'Learning to Code', 'Learning Data Analysis', 'Informational Interviews', 'Learning to Query Databases', 'Solidifying concepts', 'Getting the data science job', 'What I wish I had learned']",38,"['I had just walked away from 8 years of study and hard work with no plan. You might be wondering why someone would do that. My boss was crushing my spirit and knew that I needed to make a change.', ""My boyfriend suggested becoming a data scientist. I said ‘you're crazy!’ I didn’t know the first thing about programming. Surely he was overestimating what I was capable of. Imposter syndrome strikes again."", 'About two weeks later my friend Anna suggested the exact same thing, I thought about it some more and began to entertain the idea. Why not? I decided to become a beginner again and reinvent myself as a data scientist.', 'I wanted to learn at my own pace so I decided to take online courses. I figured that with a PhD in Neuroscience I probably had enough formal training to get a data science job. I just needed the practical skills.', 'This story will describe the 4 different courses that I took and how they led to a data science job at a healthcare startup in Silicone Valley.', 'At the time, most of the online courses I came across were free. So I challenged myself to gain the skills I needed without spending any money. What can I say, I am pretty stingy 😜', 'When I quit my post doc at UCSF I had zero programming experience. I had used statistics throughout all my research, but only on a small scale. All the datasets I had previously analysed were ones I had generated myself in the lab. Therefore the number of observations were very small. I needed to learn to code and analyse data on a much larger scale.', 'When I decided I wanted to become a data scientist, the first thing I wanted to learn was how to write computer code. Since I had never coded before it was a complete unknown. I figured that if I really hated writing code, then data science would not be a great fit for me. So it seemed like a good place to start.', 'I was lucky that my partner Ben has worked in many areas of tech and was able to point me in the right direction. He suggested that Python might suit me best. Python is excellent for data analysis, versatile and deals well with large datasets, so that is where I began.', 'To begin learning to code I used Codecademy. I started with Introduction to Python, but I am not sure if the course I completed still exists as it was back in 2014. If I was to use Codecademy to start learning python now I would probably choose the Analyze Data with Python course.', 'I found Codecademy an excellent starting point. The main advantage for me was to be able to write code right in my browser. Installing programming environments on my computer correctly is still my Achilles heel. So I was pleased to avoid it when starting off. It was comforting to know that if my code didn’t work it was because of the syntax and not because of an environment setup error.', 'I also liked how you could do a few minutes of work at a time with Codecademy. If I had some spare time I would log in and do a few problems because it was all there waiting for me. This piecemeal progression meant that I was not too intimidated to get stuck into it.', 'At the time that I completed the course, there were only a handful of Codecademy courses on offer and this one was free. I was so amazed at the quality of free courses available online.', 'Once I had learned the basics of Python, I needed to start to level up my stats experience and learn to analyse data on a larger scale.', 'Second I took the Coursera Data Science Specialisation from Johns Hopkins. At the time, you could do an honour code certificate version for free and only had to pay if you wanted the verified certificate.', 'For me, the verified certificate did not seem important. Instead, I needed to be able to demonstrate the skills taught in the course during tech interviews. So I took the free version of the specialisation.', 'One drawback for me was that this series of courses is taught in R. R is an excellent programming language for statistical analysis and is favoured by academia. However, I wanted to learn Python for data science. I thought Python would be more useful in the startups where I wanted to work.', 'I looked into a few data analysis courses in Python but they seemed to assume quite a bit of knowledge that I did not yet have. I believe most of these courses were aimed at software engineers who were wanting to transition into data science. So they assumed that you had solid programming skills and already knew how to set up your python environment.', 'The main aspect that I liked about the Coursera Data Science Specialisation is that it started from the very beginning. There were step by step instructions on how to install R and R studio in the first course. This made it easy to tackle the subsequent courses knowing that there would not be any technical issues.', 'Another facet of the Johns Hopkins Data Science specialisation that suited me is that it was taught by the Public Health department. My health science domain expertise made it easy for me to follow the examples that they set out. They had examples using air quality impacts on asthma and other datasets related to healthcare. Therefore I could focus on the course content rather than figuring out the scenarios presented for data analysis.', 'This series of courses really set me up well with a base level understanding in the main aspects of data science work. It touched on programming in R, basic data cleaning, analysis, regression and machine learning. I really enjoyed learning to code and how to use code to analyse data, so that encouraged me to continue learning.', 'At this stage of my retraining, I started asking people in my network if they could introduce me to other people who had made the transition from academia to data science in San Francisco. A few were able to connect me so I set up as many informational interviews as I could.', 'A friend introduced me to a data scientist from Modcloth who had taken a similar pathway to me. She used to be a neuroscientist and I found her advice particularly helpful.', 'Her major recommendation was to learn SQL.', 'SQL was not covered at all in the Coursera data science specialisation from Johns Hopkins. She said that most of her day to day work was querying databases. She had to extract insights for the business development and marketing teams. Only a small portion of her time was spent doing statistical analysis and machine learning.', 'I took her advice and started a self-paced SQL course with Stanford Online. Of all the courses that I did, this was my favourite. I enjoyed it because the teacher was excellent and used simple examples to explain the concepts. She also explained each concept in multiple different ways.', 'I have since recommended this course to so many people because I think that a good foundation in SQL is essential for any data scientist. Data science courses I have come across do not cover how to get the data from a database using SQL. I think this is a huge oversight. Most courses have a CSV of data prepared for the student to use, but in my experience that is rarely the case in industry data science jobs.', 'Once I had completed the Stanford SQL course I started applying for data science positions. By that stage, I was living back in Australia and started doing Skype interviews with startups in the San Francisco bay area. Whilst interviewing I wanted to continue developing my skills.', 'I then took the Foundations of data analysis course using R by edX. It was so helpful to revise a lot of the concepts that I had already learned in the Coursera course.', 'I am a firm believer that learning concepts from different teachers can provide new insights. It was much easier to follow the statistics and machine learning concepts learning them the second time around. I felt like I got a deeper understanding through this course.', 'While I was finishing off the course, I was successful in one of my interviews with Amino, a healthcare startup in San Francisco and proceeded to get a working visa and move to the USA.', 'I think I was successful in that final interview because I had passable coding skills and a decent statistical understanding, but more importantly I had healthcare domain knowledge, experimental design and scientific method expertise.', 'In my opinion, it was these additional aspects that put my application over the top and led this startup to take a chance on me. I was very junior and required a lot more on the job training. I think that all the courses I did were just enough to get the hiring team to consider me and that my experience specific to the healthcare space got me over the line.', 'So if you are looking to change career paths into data science, I would recommend looking for a company where your existing domain knowledge is valuable.', 'The major gap in my knowledge that I wish I had filled before commencing my new data science job was using git from the command line. I had never used the terminal or command line before and I had no idea how to use git to commit my code to the company’s Github repository.', 'It took several engineers quite a bit of time to get me up to speed. I would have liked to at least have an idea of how to use it before I started so that I would not have wasted their valuable time. My colleagues were awesome and didn’t seem to mind teaching me but I did feel like a bit of a burden in the first few days.', 'I did eventually catch up and found Learn Code the Hard Way Command Line extremely useful.', 'If you are thinking of following a similar pathway into data science I would encourage you to go for it! It was absolutely the right choice for me. Different people learn in different ways, but if you have the self-discipline to study and complete what you start it is certainly feasible to teach yourself data science through online courses. If that is your goal I wish you the best of luck and would be happy to answer any questions if I can.']"
01/2020,How To Create A Chatbot with Python & Deep Learning In Less Than An Hour,Obviously don’t expect it to…,860,10,https://towardsdatascience.com/@jeretigerxu,https://towardsdatascience.com/how-to-create-a-chatbot-with-python-deep-learning-in-less-than-an-hour-56a063bdfc44?source=collection_archive---------1-----------------------,8,10,"['How To Create A Chatbot with Python & Deep Learning In Less Than An Hour', 'Agenda', 'Libraries & Data', 'Initializing Chatbot Training', 'Building the Deep Learning Model', 'Building Chatbot GUI', 'Running Chatbot', 'Conclusion', 'Areas of Improvement', 'Resources']",37,"['Some people genuinely dislike human interaction. Whenever they are forced to socialize or go to events that involve lots of people, they feel detached and awkward. Personally, I believe that I’m most extroverted because I gain energy from interacting with other people. There are plenty of people on this Earth who are the exact opposite, who get very drained from social interaction.', 'I’m reminded of a very unique film called Her (2013). The basic premise of the film is that a man who suffers from loneliness, depression, a boring job, and an impending divorce, ends up falling in love with an AI (artificial intelligence) on his computer’s operating system. Maybe at the time this was a very science-fictiony concept, given that AI back then wasn’t advanced enough to become a surrogate human, but now? 2020? Things have changed a LOT. I fear that people will give up on finding love (or even social interaction) among humans and seek it out in the digital realm. Don’t believe me? I won’t tell you what it means, but just search up the definition of the term waifu and just cringe.', 'Now isn’t this an overly verbose introduction to a simple machine learning project? Possibly. Now that I’ve detailed an issue that has grounds for actual concern for many men (and women) in this world, let’s switch gears and build something simple and fun!', 'Here’s what the finished product will look like.', 'If you want a more in-depth view of this project, or if you want to add to the code, check out the GitHub repository.', 'All of the necessary components to run this project are on the GitHub repository. Feel free to fork the repository and clone it to your local machine. Here’s a quick breakdown of the components:', 'The full code is on the GitHub repository, but I’m going to walk through the details of the code for the sake of transparency and better understanding.', 'Now let’s begin by importing the necessary libraries. (When you run the python files on your terminal, be sure to make sure they are installed properly. I use pip3 to install the packages.)', 'We have a whole bunch of libraries like nltk (Natural Language Toolkit), which contains a whole bunch of tools for cleaning up text and preparing it for deep learning algorithms, json, which loads json files directly into Python, pickle, which loads pickle files, numpy, which can perform linear algebra operations very efficiently, and keras, which is the deep learning framework we’ll be using.', 'Now it’s time to initialize all of the lists where we’ll store our natural language data. We have our json file I mentioned earlier which contains the “intents”. Here’s a snippet of what the json file actually looks like.', 'We use the json module to load in the file and save it as the variable intents.', 'If you look carefully at the json file, you can see that there are sub-objects within objects. For example, “patterns” is an attribute within “intents”. So we will use a nested for loop to extract all of the words within “patterns” and add them to our words list. We then add to our documents list each pair of patterns within their corresponding tag. We also add the tags into our classes list, and we use a simple conditional statement to prevent repeats.', 'Next, we will take the words list and lemmatize and lowercase all the words inside. In case you don’t already know, lemmatize means to turn a word into its base meaning, or its lemma. For example, the words “walking”, “walked”, “walks” all have the same lemma, which is just “walk”. The purpose of lemmatizing our words is to narrow everything down to the simplest level it can be. It will save us a lot of time and unnecessary error when we actually process these words for machine learning. This is very similar to stemming, which is to reduce an inflected word down to its base or root form.', 'Next we sort our lists and print out the results. Alright, looks like we’re set to build our deep learning model!', 'Let’s initialize our training data with a variable training. We’re creating a giant nested list which contains bags of words for each of our documents. We have a feature called output_row which simply acts as a key for the list. We then shuffle our training set and do a train-test-split, with the patterns being the X variable and the intents being the Y variable.', 'Now that we have our training and test data ready, we will now use a deep learning model from keras called Sequential. I don’t want to overwhelm you with all of the details about how deep learning models work, but if you are curious, check out the resources at the bottom of the article.', 'The Sequential model in keras is actually one of the simplest neural networks, a multi-layer perceptron. If you don’t know what that is, I don’t blame you. Here’s the documentation in keras.', 'This particular network has 3 layers, with the first one having 128 neurons, the second one having 64 neurons, and the third one having the number of intents as the number of neurons. Remember, the point of this network is to be able to predict which intent to choose given some data.', 'The model will be trained with stochastic gradient descent, which is also a very complicated topic. Stochastic gradient descent is more efficient than normal gradient descent, that’s all you need to know.', 'After the model is trained, the whole thing is turned into a numpy array and saved as chatbot_model.h5.', 'We will use this model to form our chatbot interface!', 'Once again, we need to extract the information from our files.', 'Here are some functions that contain all of the necessary processes for running the GUI and encapsulates them into units. We have the clean_up_sentence() function which cleans up any sentences that are inputted. This function is used in the bow() function, which takes the sentences that are cleaned up and creates a bag of words that are used for predicting classes (which are based off the results we got from training our model earlier).', 'In our predict_class() function, we use an error threshold of 0.25 to avoid too much overfitting. This function will output a list of intents and the probabilities, their likelihood of matching the correct intent. The function getResponse() takes the list outputted and checks the json file and outputs the most response with the highest probability.', 'Finally our chatbot_response() takes in a message (which will be inputted through our chatbot GUI), predicts the class with our predict_class() function, puts the output list into getResponse(), then outputs the response. What we get is the foundation of our chatbot. We can now tell the bot something, and it will then respond back.', 'Here comes the fun part (if the other parts weren’t fun already). We can create our GUI with tkinter, a Python library that allows us to create custom interfaces.', 'We create a function called send() which sets up the basic functionality of our chatbot. If the message that we input into the chatbot is not an empty string, the bot will output a response based on our chatbot_response() function.', 'After this, we build our chat window, our scrollbar, our button for sending messages, and our textbox to create our message. We place all the components on our screen with simple coordinates and heights.', 'Finally it’s time to run our chatbot!', 'Because I run my program on a Windows 10 machine, I had to download a server called Xming. If you run your program and it gives you some weird errors about the program failing, you can download Xming.', 'Before you run your program, you need to make sure you install python or python3 with pip (or pip3). If you are unfamiliar with command line commands, check out the resources below.', 'Once you run your program, you should get this.', 'Congratulations on completing this project! Building a simple chatbot exposes you to a variety of useful skills for data science and general programming. I feel that the best way (for me, at least) to learn anything is to just build and tinker around. If you want to become good at something, you need to get in lots of practice, and the best way to practice is to just get your hands dirty and build!', 'Thank you for taking the time to read through this article! Feel free to check out my portfolio site or my GitHub.', 'We used the simplest keras neural network, so there is a LOT of room for improvement. Feel free to try out convolutional networks or recurrent networks for your projects.', 'Our json file was extremely tiny in terms of the variety of possible intents and responses. Human language is billions of times more complex than this, so creating JARVIS from scratch will require a lot more.', 'There are many more deep learning frameworks than just keras. There’s tensorflow, Apache Spark, PyTorch, Sonnet, and more. Don’t limit yourself to just one tool!']"
01/2020,Understanding Singular Value Decomposition and its Application in Data Science,"In linear algebra, the…",941,11,https://towardsdatascience.com/@reza.bagheri79,https://towardsdatascience.com/understanding-singular-value-decomposition-and-its-application-in-data-science-388a54be95d?source=collection_archive---------2-----------------------,52,2,"['Understanding Singular Value Decomposition and its Application in Data Science', 'Applications']",289,"['In linear algebra, the Singular Value Decomposition (SVD) of a matrix is a factorization of that matrix into three matrices. It has some interesting algebraic properties and conveys important geometrical and theoretical insights about linear transformations. It also has some important applications in data science. In this article, I will try to explain the mathematical intuition behind SVD and its geometrical meaning. Instead of manual calculations, I will use the Python libraries to do the calculations and later give you some examples of using SVD in data science applications. In this article, bold-face lower-case letters (like a) refer to vectors. Bold-face capital letters (like A) refer to matrices, and italic lower-case letters (like a) refer to scalars.', 'To understand SVD we need to first understand the Eigenvalue Decomposition of a matrix. We can think of a matrix A as a transformation that acts on a vector x by multiplication to produce a new vector Ax. We use [A]ij or aij to denote the element of matrix A at row i and column j. If A is an m×p matrix and B is a p×n matrix, the matrix product C=AB (which is an m×n matrix) is defined as:', 'For example, the rotation matrix in a 2-d space can be defined as:', 'This matrix rotates a vector about the origin by the angle θ (with counterclockwise rotation for a positive θ). Another example is the stretching matrix B in a 2-d space which is defined as:', 'This matrix stretches a vector along the x-axis by a constant factor k but does not affect it in the y-direction. Similarly, we can have a stretching matrix in y-direction:', 'As an example, if we have a vector', 'then y=Ax is the vector which results after rotation of x by θ, and Bx is a vector which is the result of stretching x in the x-direction by a constant factor k.', 'Listing 1 shows how these matrices can be applied to a vector x and visualized in Python. We can use the NumPy arrays as vectors and matrices.', 'Here the rotation matrix is calculated for θ=30⁰ and in the stretching matrix k=3. y is the transformed vector of x. To plot the vectors, the quiver() function in matplotlib has been used. Figure 1 shows the output of the code.', 'The matrices are represented by a 2-d array in NumPy. We can use the np.matmul(a,b) function to the multiply matrix a by b However, it is easier to use the @ operator to do that. The vectors can be represented either by a 1-d array or a 2-d array with a shape of (1,n) which is a row vector or (n,1) which is a column vector.', 'Now we are going to try a different transformation matrix. Suppose that', 'However, we don’t apply it to just one vector. Initially, we have a circle that contains all the vectors that are one unit away from the origin. These vectors have the general form of', 'Now we calculate t=Ax. So t is the set of all the vectors in x which have been transformed by A. Listing 2 shows how this can be done in Python.', 'Figure 2 shows the plots of x and t and the effect of transformation on two sample vectors x1 and x2 in x.', 'The initial vectors (x) on the left side form a circle as mentioned before, but the transformation matrix somehow changes this circle and turns it into an ellipse.', 'The sample vectors x1 and x2 in the circle are transformed into t1 and t2 respectively. So:', 'Eigenvalues and Eigenvectors', 'A vector is a quantity which has both magnitude and direction. The general effect of matrix A on the vectors in x is a combination of rotation and stretching. For example, it changes both the direction and magnitude of the vector x1 to give the transformed vector t1. However, for vector x2 only the magnitude changes after transformation. In fact, x2 and t2 have the same direction. Matrix A only stretches x2 in the same direction and gives the vector t2 which has a bigger magnitude. The only way to change the magnitude of a vector without changing its direction is by multiplying it with a scalar. So if we have a vector u, and λ is a scalar quantity then λu has the same direction and a different magnitude. So for a vector like x2 in figure 2, the effect of multiplying by A is like multiplying it with a scalar quantity like λ.', 'This is not true for all the vectors in x. In fact, for each matrix A, only some of the vectors have this property. These special vectors are called the eigenvectors of A and their corresponding scalar quantity λ is called an eigenvalue of A for that eigenvector. So the eigenvector of an n×n matrix A is defined as a nonzero vector u such that:', 'where λ is a scalar and is called the eigenvalue of A, and u is the eigenvector corresponding to λ. In addition, if you have any other vectors in the form of au where a is a scalar, then by placing it in the previous equation we get:', 'which means that any vector which has the same direction as the eigenvector u (or the opposite direction if a is negative) is also an eigenvector with the same corresponding eigenvalue.', 'For example, the eigenvalues of', 'are λ1=-1 and λ2=-2 and their corresponding eigenvectors are:', 'and we have:', 'This means that when we apply matrix B to all the possible vectors, it does not change the direction of these two vectors (or any vectors which have the same or opposite direction) and only stretches them. So for the eigenvectors, the matrix multiplication turns into a simple scalar multiplication. Here I am not going to explain how the eigenvalues and eigenvectors can be calculated mathematically. Instead, I will show you how they can be obtained in Python.', 'We can use the LA.eig() function in NumPy to calculate the eigenvalues and eigenvectors. It returns a tuple. The first element of this tuple is an array that stores the eigenvalues, and the second element is a 2-d array that stores the corresponding eigenvectors. In fact, in Listing 3 the column u[:,i] is the eigenvector corresponding to the eigenvalue lam[i]. Now if we check the output of Listing 3, we get:', 'You may have noticed that the eigenvector for λ=-1 is the same as u1, but the other one is different. That is because LA.eig() returns the normalized eigenvector. A normalized vector is a unit vector whose length is 1. But before explaining how the length can be calculated, we need to get familiar with the transpose of a matrix and the dot product.', 'Transpose', 'The transpose of the column vector u (which is shown by u superscript T) is the row vector of u (in this article sometimes I show it as u^T). The transpose of an m×n matrix A is an n×m matrix whose columns are formed from the corresponding rows of A. For example if we have', 'then the transpose of C is:', 'So the transpose of a row vector becomes a column vector with the same elements and vice versa. In fact, the element in the i-th row and j-th column of the transposed matrix is equal to the element in the j-th row and i-th column of the original matrix. So', 'In NumPy you can use the transpose() method to calculate the transpose. For example to calculate the transpose of matrix C we write C.transpose(). We can also use the transpose attribute T, and write C.T to get its transpose. The transpose has some important properties. First, the transpose of the transpose of A is A. So:', 'In addition, the transpose of a product is the product of the transposes in the reverse order.', 'To prove it remember the matrix multiplication definition:', 'and based on the definition of matrix transpose, the left side is:', 'and the right side is', 'so both sides of the equation are equal.', 'Dot product', 'If we have two vectors u and v:', 'The dot product (or inner product) of these vectors is defined as the transpose of u multiplied by v:', 'Based on this definition the dot product is commutative so:', 'Partitioned matrix', 'When calculating the transpose of a matrix, it is usually useful to show it as a partitioned matrix. For example, the matrix', 'can be also written as:', 'where', 'So we can think of each column of C as a column vector, and C can be thought of as a matrix with just one row. Now to write the transpose of C, we can simply turn this row into a column, similar to what we do for a row vector. The only difference is that each element in C is now a vector itself and should be transposed too.', 'Now we know that', 'So:', 'Now each row of the C^T is the transpose of the corresponding column of the original matrix C.', 'Now let matrix A be a partitioned column matrix and matrix B be a partitioned row matrix:', 'where each column vector ai is defined as the i-th column of A:', 'Here for each element, the first subscript refers to the row number and the second subscript to the column number. So A is an m×p matrix. In addition, B is a p×n matrix where each row vector in bi^T is the i-th row of B:', 'Again, the first subscript refers to the row number and the second subscript to the column number. Please note that by convection, a vector is written as a column vector. So to write a row vector, we write it as the transpose of a column vector. So bi is a column vector, and its transpose is a row vector that captures the i-th row of B. Now we can calculate AB:', 'so the product of the i-th column of A and the i-th row of B gives an m×n matrix, and all these matrices are added together to give AB which is also an m×n matrix. In fact, we can simply assume that we are multiplying a row A vector by a column vector B. As a special case, suppose that x is a column vector. Now we can calculate Ax similarly:', 'So Ax is simply a linear combination of the columns of A.', 'To calculate the dot product of two vectors a and b in NumPy, we can write np.dot(a,b) if both are 1-d arrays, or simply use the definition of the dot product and write a.T @ b .', 'Now that we are familiar with the transpose and dot product, we can define the length (also called the 2-norm) of the vector u as:', 'To normalize a vector u, we simply divide it by its length to have the normalized vector n:', 'The normalized vector n is still in the same direction of u, but its length is 1. Now we can normalize the eigenvector of λ=-2 that we saw before:', 'which is the same as the output of Listing 3. As shown before, if you multiply (or divide) an eigenvector by a constant, the new vector is still an eigenvector for the same eigenvalue, so by normalizing an eigenvector corresponding to an eigenvalue, you still have an eigenvector for that eigenvalue.', 'But why eigenvectors are important to us? As mentioned before an eigenvector simplifies the matrix multiplication into a scalar multiplication. In addition, they have some more interesting properties. Let me go back to matrix A that was used in Listing 2 and calculate its eigenvectors:', 'As you remember this matrix transformed a set of vectors forming a circle into a new set forming an ellipse (Figure 2). We will use LA.eig() to calculate the eigenvectors in Listing 4.', 'The output is :', 'So we have two eigenvectors:', 'and the corresponding eigenvalues are:', 'Now we plot the eigenvectors on top of the transformed vectors:', 'There is nothing special about these eigenvectors in Figure 3. Now let me try another matrix:', 'Here we have two eigenvectors:', 'and the corresponding eigenvalues are:', 'Now we can plot the eigenvectors on top of the transformed vectors by replacing this new matrix in Listing 5. The result is shown in Figure 4.', 'This time the eigenvectors have an interesting property. We see that the eigenvectors are along the major and minor axes of the ellipse (principal axes). An ellipse can be thought of as a circle stretched or shrunk along its principal axes as shown in Figure 5, and matrix B transforms the initial circle by stretching it along u1 and u2, the eigenvectors of B.', 'But why the eigenvectors of A did not have this property? That is because B is a symmetric matrix. A symmetric matrix is a matrix that is equal to its transpose. So the elements on the main diagonal are arbitrary but for the other elements, each element on row i and column j is equal to the element on row j and column i (aij = aji). Here is an example of a symmetric matrix:', 'A symmetric matrix is always a square matrix (n×n). You can now easily see that A was not symmetric. A symmetric matrix transforms a vector by stretching or shrinking it along its eigenvectors. In addition, we know that all the matrices transform an eigenvector by multiplying its length (or magnitude) by the corresponding eigenvalue. We know that the initial vectors in the circle have a length of 1 and both u1 and u2 are normalized, so they are part of the initial vectors x. Now their transformed vectors are:', 'So the amount of stretching or shrinking along each eigenvector is proportional to the corresponding eigenvalue as shown in Figure 6.', 'So when you have more stretching in the direction of an eigenvector, the eigenvalue corresponding to that eigenvector will be greater. In fact, if the absolute value of an eigenvalue is greater than 1, the circle x stretches along it, and if the absolute value is less than 1, it shrinks along it. Let me try this matrix:', 'The eigenvectors and corresponding eigenvalues are:', 'Now if we plot the transformed vectors we get:', 'As you see now we have stretching along u1 and shrinking along u2. The other important thing about these eigenvectors is that they can form a basis for a vector space.', 'Basis', 'A set of vectors {v1, v2, v3 …, vn} form a basis for a vector space V, if they are linearly independent and span V. A vector space is a set of vectors that can be added together or multiplied by scalars. This is a closed set, so when the vectors are added or multiplied by a scalar, the result still belongs to the set. The operations of vector addition and scalar multiplication must satisfy certain requirements which are not discussed here. Euclidean space R² (in which we are plotting our vectors) is an example of a vector space.', 'When a set of vectors is linearly independent, it means that no vector in the set can be written as a linear combination of the other vectors. So it is not possible to write', 'when some of a1, a2, .., an are not zero. In other words, none of the vi vectors in this set can be expressed in terms of the other vectors. A set of vectors spans a space if every other vector in the space can be written as a linear combination of the spanning set. So every vector s in V can be written as:', 'A vector space V can have many different vector bases, but each basis always has the same number of basis vectors. The number of basis vectors of vector space V is called the dimension of V. In Euclidean space R², the vectors:', 'is the simplest example of a basis since they are linearly independent and every vector in R² can be expressed as a linear combination of them. They are called the standard basis for R². As a result, the dimension of R² is 2. It can have other bases, but all of them have two vectors that are linearly independent and span it. For example, vectors:', 'can also form a basis for R². An important reason to find a basis for a vector space is to have a coordinate system on that. If the set of vectors B ={v1, v2, v3 …, vn} form a basis for a vector space, then every vector x in that space can be uniquely specified using those basis vectors :', 'Now the coordinate of x relative to this basis B is:', 'In fact, when we are writing a vector in R², we are already expressing its coordinate relative to the standard basis. That is because any vector', 'can be written as', 'Now a question comes up. If we know the coordinate of a vector relative to the standard basis, how can we find its coordinate relative to a new basis?', 'The equation:', 'can be also written as:', 'The matrix:', 'is called the change-of-coordinate matrix. The columns of this matrix are the vectors in basis B. The equation', 'gives the coordinate of x in R² if we know its coordinate in basis B. If we need the opposite we can multiply both sides of this equation by the inverse of the change-of-coordinate matrix to get:', 'Now if we know the coordinate of x in R² (which is simply x itself), we can multiply it by the inverse of the change-of-coordinate matrix to get its coordinate relative to basis B. For example, suppose that our basis set B is formed by the vectors:', 'and we have a vector:', 'To calculate the coordinate of x in B, first, we form the change-of-coordinate matrix:', 'Now the coordinate of x relative to B is:', 'Listing 6 shows how this can be calculated in NumPy. To calculate the inverse of a matrix, the function np.linalg.inv() can be used.', 'The output shows the coordinate of x in B:', 'Figure 8 shows the effect of changing the basis.', 'To find the u1-coordinate of x in basis B, we can draw a line passing from x and parallel to u2 and see where it intersects the u1 axis. u2-coordinate can be found similarly as shown in Figure 8. In an n-dimensional space, to find the coordinate of ui, we need to draw a hyper-plane passing from x and parallel to all other eigenvectors except ui and see where it intersects the ui axis. As Figure 8 (left) shows when the eigenvectors are orthogonal (like i and j in R²), we just need to draw a line that passes through point x and is perpendicular to the axis that we want to find its coordinate.', 'Properties of symmetric matrices', 'As figures 5 to 7 show the eigenvectors of the symmetric matrices B and C are perpendicular to each other and form orthogonal vectors. This is not a coincidence and is a property of symmetric matrices.', 'An important property of the symmetric matrices is that an n×n symmetric matrix has n linearly independent and orthogonal eigenvectors, and it has n real eigenvalues corresponding to those eigenvectors. It is important to note that these eigenvalues are not necessarily different from each other and some of them can be equal. Another important property of symmetric matrices is that they are orthogonally diagonalizable.', 'Eigendecomposition', 'A symmetric matrix is orthogonally diagonalizable. It means that if we have an n×n symmetric matrix A, we can decompose it as', 'where D is an n×n diagonal matrix comprised of the n eigenvalues of A. P is also an n×n matrix, and the columns of P are the n linearly independent eigenvectors of A that correspond to those eigenvalues in D respectively. In other words, if u1, u2, u3 …, un are the eigenvectors of A, and λ1, λ2, …, λn are their corresponding eigenvalues respectively, then A can be written as', 'This can also be written as', 'You should notice that each ui is considered a column vector and its transpose is a row vector. So the transpose of P has been written in terms of the transpose of the columns of P. This factorization of A is called the eigendecomposition of A.', 'Let me clarify it by an example. Suppose that', 'It has two eigenvectors:', 'and the corresponding eigenvalues were:', 'So D can be defined as', 'Now the columns of P are the eigenvectors of A that correspond to those eigenvalues in D respectively. So', 'The transpose of P is', 'So A can be written as', 'It is important to note that if you do the multiplications on the right side of the above equation, you will not get A exactly. That is because we have the rounding errors in NumPy to calculate the irrational numbers that usually show up in the eigenvalues and eigenvectors, and we have also rounded the values of the eigenvalues and eigenvectors here, however, in theory, both sides should be equal. But what does it mean? To understand the eigendecomposition better, we can take a look at its geometrical interpretation.', 'Geometrical interpretation of eigendecomposition', 'To better understand the eigendecomposition equation, we need to first simplify it. If we assume that each eigenvector ui is an n × 1 column vector', 'then the transpose of ui is a 1 × n row vector', 'and their multiplication', 'becomes an n×n matrix. First, we calculate DP^T to simplify the eigendecomposition equation:', 'Now the eigendecomposition equation becomes:', 'So the n×n matrix A can be broken into n matrices with the same shape (n×n), and each of these matrices has a multiplier which is equal to the corresponding eigenvalue λi. Each of the matrices', 'is called a projection matrix. Imagine that we have a vector x and a unit vector v. The inner product of v and x which is equal to v.x=v^T x gives the scalar projection of x onto v (which is the length of the vector projection of x into v), and if we multiply it by v again, it gives a vector which is called the orthogonal projection of x onto v. This is shown in Figure 9.', 'So when v is a unit vector, multiplying', 'by x, will give the orthogonal projection of x onto v, and that is why it is called the projection matrix. So multiplying ui ui^T by x, we get the orthogonal projection of x onto ui.', 'Now let me calculate the projection matrices of matrix A mentioned before.', 'We already had calculated the eigenvalues and eigenvectors of A.', 'Using the output of Listing 7, we get the first term in the eigendecomposition equation (we call it A1 here):', 'As you see it is also a symmetric matrix. In fact, all the projection matrices in the eigendecomposition equation are symmetric. That is because the element in row m and column n of each matrix', 'is equal to', 'and the element at row n and column m has the same value which makes it a symmetric matrix. This projection matrix has some interesting properties. First, we can calculate its eigenvalues and eigenvectors:', 'As you see, it has two eigenvalues (since it is a 2×2 symmetric matrix). One of them is zero and the other is equal to λ1 of the original matrix A. In addition, the eigenvectors are exactly the same eigenvectors of A. This is not a coincidence. Suppose we get the i-th term in the eigendecomposition equation and multiply it by ui.', 'We know that ui is an eigenvector and it is normalized, so its length and its inner product with itself are both equal to 1. So:', 'Now if you look at the definition of the eigenvectors, this equation means that one of the eigenvalues of the matrix', 'is λi and the corresponding eigenvector is ui. But this matrix is an n×n symmetric matrix and should have n eigenvalues and eigenvectors. Now we can multiply it by any of the remaining (n-1) eigenvalues of A to get:', 'where i ≠ j. We know that the eigenvalues of A are orthogonal which means each pair of them are perpendicular. The inner product of two perpendicular vectors is zero (since the scalar projection of one onto the other should be zero). So the inner product of ui and uj is zero, and we get', 'which means that uj is also an eigenvector and its corresponding eigenvalue is zero. So we conclude that each matrix', 'in the eigendecomposition equation is a symmetric n×n matrix with n eigenvectors. The eigenvectors are the same as the original matrix A which are u1, u2, … un. The corresponding eigenvalue of ui is λi (which is the same as A), but all the other eigenvalues are zero. Now, remember how a symmetric matrix transforms a vector. It will stretch or shrink the vector along its eigenvectors, and the amount of stretching or shrinking is proportional to the corresponding eigenvalue. So this matrix will stretch a vector along ui. But since the other eigenvalues are zero, it will shrink it to zero in those directions. Let me go back to matrix A and plot the transformation effect of A1 using Listing 9.', 'As you see, the initial circle is stretched along u1 and shrunk to zero along u2. So the result of this transformation is a straight line, not an ellipse. This is consistent with the fact that A1 is a projection matrix and should project everything onto u1, so the result should be a straight line along u1.', 'Rank', 'Figure 10 shows an interesting example in which the 2×2 matrix A1 is multiplied by a 2-d vector x, but the transformed vector Ax is a straight line. Here is another example. Suppose that we have a matrix:', 'Figure 11 shows how it transforms the unit vectors x.', 'So it acts as a projection matrix and projects all the vectors in x on the line y=2x. That is because the columns of F are not linear independent. In fact, if the columns of F are called f1 and f2 respectively, then we have f1=2f2. Remember that we write the multiplication of a matrix and a vector as:', 'So unlike the vectors in x which need two coordinates, Fx only needs one coordinate and exists in a 1-d space. In general, an m×n matrix does not necessarily transform an n-dimensional vector into anther m-dimensional vector. The dimension of the transformed vector can be lower if the columns of that matrix are not linearly independent.', 'The column space of matrix A written as Col A is defined as the set of all linear combinations of the columns of A, and since Ax is also a linear combination of the columns of A, Col A is the set of all vectors in Ax. The number of basis vectors of Col A or the dimension of Col A is called the rank of A. So the rank of A is the dimension of Ax.', 'The rank of A is also the maximum number of linearly independent columns of A. That is because we can write all the dependent columns as a linear combination of these linearly independent columns, and Ax which is a linear combination of all the columns can be written as a linear combination of these linearly independent columns. So they span Ax and form a basis for col A, and the number of these vectors becomes the dimension of col of A or rank of A.', 'In the previous example, the rank of F is 1. In addition, in the eigendecomposition equation, the rank of each matrix', 'is 1. Remember that they only have one non-zero eigenvalue and that is not a coincidence. It can be shown that the rank of a symmetric matrix is equal to the number of its non-zero eigenvalues.', 'Now we go back to the eigendecomposition equation again. Suppose that we apply our symmetric matrix A to an arbitrary vector x. Now the eigendecomposition equation becomes:', 'Each of the eigenvectors ui is normalized, so they are unit vectors. Now in each term of the eigendecomposition equation', 'gives a new vector which is the orthogonal projection of x onto ui. Then this vector is multiplied by λi. Since λi is a scalar, multiplying it by a vector, only changes the magnitude of that vector, not its direction. So λi only changes the magnitude of', 'Finally all the n vectors', 'are summed together to give Ax. This process is shown in Figure 12.', 'So the eigendecomposition mathematically explains an important property of the symmetric matrices that we saw in the plots before. A symmetric matrix transforms a vector by stretching or shrinking it along its eigenvectors, and the amount of stretching or shrinking along each eigenvector is proportional to the corresponding eigenvalue.', 'In addition, the eigendecomposition can break an n×n symmetric matrix into n matrices with the same shape (n×n) multiplied by one of the eigenvalues. The eigenvalues play an important role here since they can be thought of as a multiplier. The projection matrix only projects x onto each ui, but the eigenvalue scales the length of the vector projection (ui ui^Tx). The bigger the eigenvalue, the bigger the length of the resulting vector (λiui ui^Tx) is, and the more weight is given to its corresponding matrix (ui ui^T). So we can approximate our original symmetric matrix A by summing the terms which have the highest eigenvalues. For example, if we assume the eigenvalues λi have been sorted in descending order,', 'then we can only take the first k terms in the eigendecomposition equation to have a good approximation for the original matrix:', 'where Ak is the approximation of A with the first k terms. If we only include the first k eigenvalues and eigenvectors in the original eigendecomposition equation, we get the same result:', 'Now Dk is a k×k diagonal matrix comprised of the first k eigenvalues of A, Pk is an n×k matrix comprised of the first k eigenvectors of A, and its transpose becomes a k×n matrix. So their multiplication still gives an n×n matrix which is the same approximation of A.', 'If in the original matrix A, the other (n-k) eigenvalues that we leave out are very small and close to zero, then the approximated matrix is very similar to the original matrix, and we have a good approximation. Matrix', 'with', 'is an example. Here λ2 is rather small. We call the vectors in the unit circle x, and plot the transformation of them by the original matrix (Cx). Then we approximate matrix C with the first term in its eigendecomposition equation which is:', 'and plot the transformation of s by that. As you see in Figure 13, the result of the approximated matrix which is a straight line is very close to the original matrix.', 'Why the eigendecomposition equation is valid and why it needs a symmetric matrix? Remember the important property of symmetric matrices. Suppose that x is an n×1 column vector. If A is an n×n symmetric matrix, then it has n linearly independent and orthogonal eigenvectors which can be used as a new basis. So we can now write the coordinate of x relative to this new basis:', 'and based on the definition of basis, any vector x can be uniquely written as a linear combination of the eigenvectors of A.', 'But the eigenvectors of a symmetric matrix are orthogonal too. So to find each coordinate ai, we just need to draw a line perpendicular to an axis of ui through point x and see where it intersects it (refer to Figure 8). As mentioned before this can be also done using the projection matrix. So each term ai is equal to the dot product of x and ui (refer to Figure 9), and x can be written as', 'So we need a symmetric matrix to express x as a linear combination of the eigenvectors in the above equation. Now if we multiply A by x, we can factor out the ai terms since they are scalar quantities. So we get:', 'and since the ui vectors are the eigenvectors of A, we finally get:', 'which is the eigendecomposition equation. Whatever happens after the multiplication by A is true for all matrices, and does not need a symmetric matrix. We need an n×n symmetric matrix since it has n real eigenvalues plus n linear independent and orthogonal eigenvectors that can be used as a new basis for x. When you have a non-symmetric matrix you do not have such a combination. For example, suppose that you have a non-symmetric matrix:', 'If you calculate the eigenvalues and eigenvectors of this matrix, you get:', 'which means you have no real eigenvalues to do the decomposition. Another example is:', 'and you get:', 'Here the eigenvectors are not linearly independent. In fact u1= -u2. So you cannot reconstruct A like Figure 11 using only one eigenvector. In addition, it does not show a direction of stretching for this matrix as shown in Figure 14.', 'Finally, remember that for', 'we had:', 'Here the eigenvectors are linearly independent, but they are not orthogonal (refer to Figure 3), and they do not show the correct direction of stretching for this matrix after transformation.', 'The eigendecomposition method is very useful, but only works for a symmetric matrix. A symmetric matrix is always a square matrix, so if you have a matrix that is not square, or a square but non-symmetric matrix, then you cannot use the eigendecomposition method to approximate it with other matrices. SVD can overcome this problem.', 'Singular Values', 'Before talking about SVD, we should find a way to calculate the stretching directions for a non-symmetric matrix. Suppose that A is an m×n matrix which is not necessarily symmetric. Then it can be shown that', 'is an n×n symmetric matrix. Remember that the transpose of a product is the product of the transposes in the reverse order. So', 'So A^T A is equal to its transpose, and it is a symmetric matrix. we want to calculate the stretching directions for a non-symmetric matrix., but how can we define the stretching directions mathematically?', 'So far, we only focused on the vectors in a 2-d space, but we can use the same concepts in an n-d space. Here I focus on a 3-d space to be able to visualize the concepts. Now the column vectors have 3 elements. Initially, we have a sphere that contains all the vectors that are one unit away from the origin as shown in Figure 15. If we call these vectors x then ||x||=1. Now if we multiply them by a 3×3 symmetric matrix, Ax becomes a 3-d oval. The first direction of stretching can be defined as the direction of the vector which has the greatest length in this oval (Av1 in Figure 15). In fact, Av1 is the maximum of ||Ax|| over all unit vectors x. This vector is the transformation of the vector v1 by A.', 'The second direction of stretching is along the vector Av2. Av2 is the maximum of ||Ax|| over all vectors in x which are perpendicular to v1. So among all the vectors in x, we maximize ||Ax|| with this constraint that x is perpendicular to v1. Finally, v3 is the vector that is perpendicular to both v1 and v2 and gives the greatest length of Ax with these constraints. The direction of Av3 determines the third direction of stretching. So generally in an n-dimensional space, the i-th direction of stretching is the direction of the vector Avi which has the greatest length and is perpendicular to the previous (i-1) directions of stretching.', 'Now let A be an m×n matrix. We showed that A^T A is a symmetric matrix, so it has n real eigenvalues and n linear independent and orthogonal eigenvectors which can form a basis for the n-element vectors that it can transform (in R^n space). We call these eigenvectors v1, v2, … vn and we assume they are normalized. For each of these eigenvectors we can use the definition of length and the rule for the product of transposed matrices to have:', 'Now we assume that the corresponding eigenvalue of vi is λi', 'But vi is normalized, so', 'As a result:', 'This result shows that all the eigenvalues are positive. Now assume that we label them in decreasing order, so:', 'Now we define the singular value of A as the square root of λi (the eigenvalue of A^T A), and we denote it with σi.', 'So the singular values of A are the length of vectors Avi. Now we can summarize an important result which forms the backbone of the SVD method. It can be shown that the maximum value of ||Ax|| subject to the constraints', 'is σk, and this maximum is attained at vk. For the constraints, we used the fact that when x is perpendicular to vi, their dot product is zero.', 'So if vi is the eigenvector of A^T A (ordered based on its corresponding singular value), and assuming that ||x||=1, then Avi is showing a direction of stretching for Ax, and the corresponding singular value σi gives the length of Avi.', 'The singular values can also determine the rank of A. Suppose that the number of non-zero singular values is r. Since they are positive and labeled in decreasing order, we can write them as', 'which correspond to', 'and each λi is the corresponding eigenvalue of vi. Then it can be shown that rank A which is the number of vectors that form the basis of Ax is r. It can be also shown that the set {Av1, Av2, …, Avr} is an orthogonal basis for Ax (the Col A). So the vectors Avi are perpendicular to each other as shown in Figure 15.', 'Now we go back to the non-symmetric matrix', 'We plotted the eigenvectors of A in Figure 3, and it was mentioned that they do not show the directions of stretching for Ax. In Figure 16 the eigenvectors of A^T A have been plotted on the left side (v1 and v2). Since A^T A is a symmetric matrix, these vectors show the directions of stretching for it. On the right side, the vectors Av1 and Av2 have been plotted, and it is clear that these vectors show the directions of stretching for Ax.', 'So Avi shows the direction of stretching of A no matter A is symmetric or not.', 'Now imagine that matrix A is symmetric and is equal to its transpose. In addition, suppose that its i-th eigenvector is ui and the corresponding eigenvalue is λi. If we multiply A^T A by ui we get:', 'which means that ui is also an eigenvector of A^T A, but its corresponding eigenvalue is λi². So when A is symmetric, instead of calculating Avi (where vi is the eigenvector of A^T A) we can simply use ui (the eigenvector of A) to have the directions of stretching, and this is exactly what we did for the eigendecomposition process. Now that we know how to calculate the directions of stretching for a non-symmetric matrix, we are ready to see the SVD equation.', 'Singular Value Decomposition (SVD)', 'Let A be an m×n matrix and rank A = r. So the number of non-zero singular values of A is r. Since they are positive and labeled in decreasing order, we can write them as', 'where', 'We know that each singular value σi is the square root of the λi (eigenvalue of A^TA), and corresponds to an eigenvector vi with the same order. Now we can write the singular value decomposition of A as:', 'where V is an n×n matrix that its columns are vi. So:', 'We call a set of orthogonal and normalized vectors an orthonormal set. So the set {vi} is an orthonormal set. A matrix whose columns are an orthonormal set is called an orthogonal matrix, and V is an orthogonal matrix.', 'Σ is an m×n diagonal matrix of the form:', 'So we first make an r × r diagonal matrix with diagonal entries of σ1, σ2, …, σr. Then we pad it with zero to make it an m × n matrix.', 'We also know that the set {Av1, Av2, …, Avr} is an orthogonal basis for Col A, and σi = ||Avi||. So we can normalize the Avi vectors by dividing them by their length:', 'Now we have a set {u1, u2, …, ur} which is an orthonormal basis for Ax which is r-dimensional. We know that A is an m × n matrix, and the rank of A can be m at most (when all the columns of A are linearly independent). Since we need an m×m matrix for U, we add (m-r) vectors to the set of ui to make it a normalized basis for an m-dimensional space R^m (There are several methods that can be used for this purpose. For example we can use the Gram-Schmidt Process. However, explaining it is beyond the scope of this article). So now we have an orthonormal basis {u1, u2, … ,um}. These vectors will be the columns of U which is an orthogonal m×m matrix', 'So in the end, we can decompose A as', 'To better understand this equation, we need to simplify it:', 'We know that σi is a scalar; ui is an m-dimensional column vector, and vi is an n-dimensional column vector. So each σiui vi^T is an m×n matrix, and the SVD equation decomposes the matrix A into r matrices with the same shape (m×n).', 'First, let me show why this equation is valid. If we multiply both sides of the SVD equation by x we get:', 'We know that the set {u1, u2, …, ur} is an orthonormal basis for Ax. So the vector Ax can be written as a linear combination of them.', 'and since ui vectors are orthogonal, each term ai is equal to the dot product of Ax and ui (scalar projection of Ax onto ui):', 'but we also know that', 'So by replacing that into the previous equation, we have:', 'We also know that vi is the eigenvector of A^T A and its corresponding eigenvalue λi is the square of the singular value σi', 'But dot product is commutative, so', 'Notice that vi^Tx gives the scalar projection of x onto vi, and the length is scaled by the singular value. Now if we replace the ai value into the equation for Ax, we get the SVD equation:', 'So each ai = σivi ^Tx is the scalar projection of Ax onto ui, and if it is multiplied by ui, the result is a vector which is the orthogonal projection of Ax onto ui. The singular value σi scales the length of this vector along ui. Remember that in the eigendecomposition equation, each ui ui^T was a projection matrix that would give the orthogonal projection of x onto ui. Here σivi ^T can be thought as a projection matrix that takes x, but projects Ax onto ui. Since it projects all the vectors on ui, its rank is 1. Figure 17 summarizes all the steps required for SVD. We start by picking a random 2-d vector x1 from all the vectors that have a length of 1 in x (Figure 17–1). Then we try to calculate Ax1 using the SVD method.', 'First, we calculate the eigenvalues (λ1, λ2) and eigenvectors (v1, v2) of A^TA. We know that the singular values are the square root of the eigenvalues (σi²=λi) as shown in (Figure 17–2). Av1 and Av2 show the directions of stretching of Ax, and u1 and u2 are the unit vectors of Av1 and Av2 (Figure 17–4). The orthogonal projection of Ax1 onto u1 and u2 are', 'respectively (Figure 17–5), and by simply adding them together we get Ax1', 'as shown in (Figure 17–6).', 'Here is an example showing how to calculate the SVD of a matrix in Python. We want to find the SVD of', 'This is a 2×3 matrix. So x is a 3-d column vector, but Ax is a not 3-dimensional vector, and x and Ax exist in different vector spaces. First, we calculate the eigenvalues and eigenvectors of A^T A.', 'The output is:', 'As you see the 2nd eigenvalue is zero. Since A^T A is a symmetric matrix and has two non-zero eigenvalues, its rank is 2. Figure 18 shows two plots of A^T Ax from different angles. Since the rank of A^TA is 2, all the vectors A^TAx lie on a plane.', 'Listing 11 shows how to construct the matrices Σ and V. We first sort the eigenvalues in descending order. The columns of V are the corresponding eigenvectors in the same order.', 'Then we filter the non-zero eigenvalues and take the square root of them to get the non-zero singular values. We know that Σ should be a 3×3 matrix. So we place the two non-zero singular values in a 2×2 diagonal matrix and pad it with zero to have a 3 × 3 matrix. The output is:', 'To construct V, we take the vi vectors corresponding to the r non-zero singular values of A and divide them by their corresponding singular values. Since A is a 2×3 matrix, U should be a 2×2 matrix. We have 2 non-zero singular values, so the rank of A is 2 and r=2. As a result, we already have enough vi vectors to form U.', 'The output is:', 'Finally, we get the decomposition of A:', 'We really did not need to follow all these steps. NumPy has a function called svd() which can do the same thing for us. Listing 13 shows how we can use this function to calculate the SVD of matrix A easily.', 'The output is:', 'You should notice a few things in the output. First, This function returns an array of non-zero singular values, not the matrix Σ. In addition, it returns V^T, not V, so I have printed the transpose of the array VT that it returns. Finally, the ui and vi vectors reported by svd() have the opposite sign of the ui and vi vectors that were calculated in Listing 10-12. Remember that if vi is an eigenvector for an eigenvalue, then (-1)vi is also an eigenvector for the same eigenvalue, and its length is also the same. So if vi is normalized, (-1)vi is normalized too. In fact, in Listing 10 we calculated vi with a different method and svd() is just reporting (-1)vi which is still correct. Since ui=Avi/σi, the set of ui reported by svd() will have the opposite sign too.', 'You can easily construct the matrix Σ and check that multiplying these matrices gives A.', 'In Figure 19, you see a plot of x which is the vectors in a unit sphere and Ax which is the set of 2-d vectors produced by A. The vectors u1 and u2 show the directions of stretching. The ellipse produced by Ax is not hollow like the ones that we saw before (for example in Figure 6), and the transformed vectors fill it completely.', 'Similar to the eigendecomposition method, we can approximate our original symmetric matrix A by summing the terms which have the highest singular values. So we can use the first k terms in the SVD equation, using the k highest singular values which means we only include the first k vectors in U and V matrices in the decomposition equation:', 'We know that the set {u1, u2, …, ur} forms a basis for Ax. So when we pick k vectors from this set, Ak x is written as a linear combination of u1, u2, … uk. So they span Ak x and since they are linearly independent they form a basis for Ak x (or col A). So the rank of Ak is k, and by picking the first k singular values, we approximate A with a rank-k matrix.', 'As an example, suppose that we want to calculate the SVD of matrix', 'Again x is the vectors in a unit sphere (Figure 19 left). The singular values are σ1=11.97, σ2=5.57, σ3=3.25, and the rank of A is 3. So Ax is an ellipsoid in 3-d space as shown in Figure 20 (left). If we approximate it using the first singular value, the rank of Ak will be one and Ak multiplied by x will be a line (Figure 20 right). If we only use the first two singular values, the rank of Ak will be 2 and Ak multiplied by x will be a plane (Figure 20 middle).', 'It is important to note that if we have a symmetric matrix, the SVD equation is simplified into the eigendecomposition equation. Suppose that the symmetric matrix A has eigenvectors vi with the corresponding eigenvalues λi. So we', 'We already showed that for a symmetric matrix, vi is also an eigenvector of A^TA with the corresponding eigenvalue of λi². So the singular values of A are the square root of λi² and σi=λi. now we can calculate ui:', 'So ui is the eigenvector of A corresponding to λi (and σi). Now we can simplify the SVD equation to get the eigendecomposition equation:', 'Finally, it can be shown that SVD is the best way to approximate A with a rank-k matrix. The Frobenius norm of an m × n matrix A is defined as the square root of the sum of the absolute squares of its elements:', 'So this is like the generalization of the vector length for a matrix. Now if the m×n matrix Ak is the approximated rank-k matrix by SVD, we can think of', 'as the distance between A and Ak. The smaller this distance, the better Ak approximates A. Now if B is any m×n rank-k matrix, it can be shown that', 'In other words, the difference between A and its rank-k approximation generated by SVD has the minimum Frobenius norm, and no other rank-k matrix can give a better approximation for A (with a closer distance in terms of the Frobenius norm).', 'Now that we are familiar with SVD, we can see some of its applications in data science.', 'Dimensionality reduction', 'We can store an image in a matrix. Every image consists of a set of pixels which are the building blocks of that image. Each pixel represents the color or the intensity of light in a specific location in the image. In a grayscale image with PNG format, each pixel has a value between 0 and 1, where zero corresponds to black and 1 corresponds to white. So a grayscale image with m×n pixels can be stored in an m×n matrix or NumPy array. Here we use the imread() function to load a grayscale image of Einstein which has 480 × 423 pixels into a 2-d array. Then we use SVD to decompose the matrix and reconstruct it using the first 30 singular values.', 'The original matrix is 480×423. So we need to store 480×423=203040 values. After SVD each ui has 480 elements and each vi has 423 elements. To be able to reconstruct the image using the first 30 singular values we only need to keep the first 30 σi, ui, and vi which means storing 30×(1+480+423)=27120 values. This is roughly 13% of the number of values required for the original image. So using SVD we can have a good approximation of the original image and save a lot of memory. Listing 16 and calculates the matrices corresponding to the first 6 singular values. Each matrix σiui vi ^T has a rank of 1 and has the same number of rows and columns as the original matrix. Figure 22 shows the result.', ""Please note that unlike the original grayscale image, the value of the elements of these rank-1 matrices can be greater than 1 or less than zero, and they should not be interpreted as a grayscale image. So I did not use cmap='gray' and did not display them as grayscale images. When plotting them we do not care about the absolute value of the pixels. Instead, we care about their values relative to each other."", 'To understand how the image information is stored in each of these matrices, we can study a much simpler image. In Listing 17, we read a binary image with five simple shapes: a rectangle and 4 circles. The result is shown in Figure 23.', 'The image has been reconstructed using the first 2, 4, and 6 singular values. Now we plot the matrices corresponding to the first 6 singular values:', 'Each matrix (σi ui vi ^T) has a rank of 1 which means it only has one independent column and all the other columns are a scalar multiplication of that one. So if call the independent column c1 (or it can be any of the other column), the columns have the general form of:', 'where ai is a scalar multiplier. In addition, this matrix projects all the vectors on ui, so every column is also a scalar multiplication of ui. This can be seen in Figure 25. Two columns of the matrix σ2u2 v2^T are shown versus u2. Both columns have the same pattern of u2 with different values (ai for column #300 has a negative value).', 'So using the values of c1 and ai (or u2 and its multipliers), each matrix captures some details of the original image. In figure 24, the first 2 matrices can capture almost all the information about the left rectangle in the original image. The 4 circles are roughly captured as four rectangles in the first 2 matrices in Figure 24, and more details on them are added in the last 4 matrices. This can be also seen in Figure 23 where the circles in the reconstructed image become rounder as we add more singular values. These rank-1 matrices may look simple, but they are able to capture some information about the repeating patterns in the image. For example in Figure 26, we have the image of the national monument of Scotland which has 6 pillars (in the image), and the matrix corresponding to the first singular value can capture the number of pillars in the original image.', 'Eigenfaces', 'In this example, we are going to use the Olivetti faces dataset in the Scikit-learn library. This data set contains 400 images. The images were taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The images show the face of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions, and facial details. These images are grayscale and each image has 64×64 pixels. The intensity of each pixel is a number on the interval [0, 1]. First, we load the dataset:', 'The fetch_olivetti_faces() function has been already imported in Listing 1. We call it to read the data and stores the images in the imgs array. This is a (400, 64, 64) array which contains 400 grayscale 64×64 images. We can show some of them as an example here:', 'In the previous example, we stored our original image in a matrix and then used SVD to decompose it. Here we take another approach. We know that we have 400 images, so we give each image a label from 1 to 400. Now we use one-hot encoding to represent these labels by a vector. We use a column vector with 400 elements. For each label k, all the elements are zero except the k-th element. So label k will be represented by the vector:', 'Now we store each image in a column vector. Each image has 64 × 64 = 4096 pixels. So we can flatten each image and place the pixel values into a column vector f with 4096 elements as shown in Figure 28:', 'So each image with label k will be stored in the vector fk, and we need 400 fk vectors to keep all the images. Now we define a transformation matrix M which transforms the label vector ik to its corresponding image vector fk. The vectors fk will be the columns of matrix M:', 'This matrix has 4096 rows and 400 columns. We can simply use y=Mx to find the corresponding image of each label (x can be any vectors ik, and y will be the corresponding fk). For example for the third image of this dataset, the label is 3, and all the elements of i3 are zero except the third element which is 1. Now, remember the multiplication of partitioned matrices. When we multiply M by i3, all the columns of M are multiplied by zero except the third column f3, so:', 'Listing 21 shows how we can construct M and use it to show a certain image from the dataset.', 'The length of each label vector ik is one and these label vectors form a standard basis for a 400-dimensional space. In this space, each axis corresponds to one of the labels with the restriction that its value can be either zero or one. The vectors fk live in a 4096-dimensional space in which each axis corresponds to one pixel of the image, and matrix M maps ik to fk. Now we can use SVD to decompose M. Remember that when we decompose M (with rank r) to', 'the set {u1, u2, …, ur} which are the first r columns of U will be a basis for Mx. Each vector ui will have 4096 elements. Since y=Mx is the space in which our image vectors live, the vectors ui form a basis for the image vectors as shown in Figure 29. In this figure, I have tried to visualize an n-dimensional vector space. This is, of course, impossible when n≥3, but this is just a fictitious illustration to help you understand this method.', ""So we can reshape ui into a 64 ×64 pixel array and try to plot it like an image. The value of the elements of these vectors can be greater than 1 or less than zero, and when reshaped they should not be interpreted as a grayscale image. So I did not use cmap='gray' when displaying them."", 'The output is:', 'You can check that the array s in Listing 22 has 400 elements, so we have 400 non-zero singular values and the rank of the matrix is 400. As a result, we need the first 400 vectors of U to reconstruct the matrix completely. We can easily reconstruct one of the images using the basis vectors:', 'Here we take image #160 and reconstruct it using different numbers of singular values:', 'The vectors ui are called the eigenfaces and can be used for face recognition. As you see in Figure 30, each eigenface captures some information of the image vectors. For example, u1 is mostly about the eyes, or u6 captures part of the nose. When reconstructing the image in Figure 31, the first singular value adds the eyes, but the rest of the face is vague. By increasing k, nose, eyebrows, beard, and glasses are added to the face. Some people believe that the eyes are the most important feature of your face. It seems that SVD agrees with them since the first eigenface which has the highest singular value captures the eyes.', 'Reducing noise', 'SVD can be used to reduce the noise in the images. Listing 24 shows an example:', 'Here we first load the image and add some noise to it. Then we reconstruct the image using the first 20, 55 and 200 singular values. As you see in Figure 32, the amount of noise increases as we increase the rank of the reconstructed matrix. So if we use a lower rank like 20 we can significantly reduce the noise in the image. It is important to understand why it works much better at lower ranks.', 'Here is a simple example to show how SVD reduces the noise. Imagine that we have 3×15 matrix defined in Listing 25:', 'A color map of this matrix is shown below:', 'The matrix columns can be divided into two categories. In the first 5 columns, only the first element is not zero, and in the last 10 columns, only the first element is zero. We also have a noisy column (column #12) which should belong to the second category, bit its first and last element do not have the right values. We can assume that these two elements contain some noise. Now we decompose this matrix using SVD. The rank of the matrix is 3, and it only has 3 non-zero singular values. Now we reconstruct it using the first 2 and 3 singular values.', 'As Figure 34 shows, by using the first 2 singular values column #12 changes and follows the same pattern of the columns in the second category. However, the actual values of its elements are a little lower now. If we use all the 3 singular values, we get back the original noisy column. Figure 35 shows a plot of these columns in 3-d space.', 'First look at the ui vectors generated by SVD. u1 shows the average direction of the column vectors in the first category. Of course, it has the opposite direction, but it does not matter (Remember that if vi is an eigenvector for an eigenvalue, then (-1)vi is also an eigenvector for the same eigenvalue, and since ui=Avi/σi, then its sign depends on vi). What is important is the stretching direction not the sign of the vector. Similarly, u2 shows the average direction for the second category.', 'The noisy column is shown by the vector n. It is not along u1 and u2. Now if we use ui as a basis, we can decompose n and find its orthogonal projection onto ui. As you see it has a component along u3 (in the opposite direction) which is the noise direction. This direction represents the noise present in the third element of n. It has the lowest singular value which means it is not considered an important feature by SVD. When we reconstruct n using the first two singular values, we ignore this direction and the noise present in the third element is eliminated. Now we only have the vector projections along u1 and u2. But the scalar projection along u1 has a much higher value. That is because vector n is more similar to the first category.', 'So the projection of n in the u1-u2 plane is almost along u1, and the reconstruction of n using the first two singular values gives a vector which is more similar to the first category. It is important to note that the noise in the first element which is represented by u2 is not eliminated. In addition, though the direction of the reconstructed n is almost correct, its magnitude is smaller compared to the vectors in the first category. In fact, in the reconstructed vector, the second element (which did not contain noise) has now a lower value compared to the original vector (Figure 36).', 'So SVD assigns most of the noise (but not all of that) to the vectors represented by the lower singular values. If we reconstruct a low-rank matrix (ignoring the lower singular values), the noise will be reduced, however, the correct part of the matrix changes too. The result is a matrix that is only an approximation of the noiseless matrix that we are looking for. This can be seen in Figure 32. The image background is white and the noisy pixels are black. When we reconstruct the low-rank image, the background is much more uniform but it is gray now. In fact, what we get is a less noisy approximation of the white background that we expect to have if there is no noise in the image.', 'I hope that you enjoyed reading this article. Please let me know if you have any questions or suggestions. All the Code Listings in this article are available for download as a Jupyter notebook from GitHub at: https://github.com/reza-bagheri/SVD_article']"
01/2020,Stop Hiring Data Scientists.,Your ROI is suffering from an inability to hire properly.,5.1K,28,https://towardsdatascience.com/@posey,https://towardsdatascience.com/stop-hiring-data-scientists-30514028e202?source=collection_archive---------3-----------------------,6,1,['Stop Hiring Data Scientists.'],22,"['We are hiring armies of Data Scientists when what we need are armies of analysts and engineers (or developer or however else you self-identify).', 'At the moment there’s a colossal mismatch between what hiring managers and executives want delivered and who they’re hiring to do the job. There are still lots of situations where a Data Scientist is the right person for the job, but we need to get a better understanding of what those situations are.', 'The world is very data rich and insights poor. We need more passionate people mining and delivering insights and fewer people doing surface level analysis that is ungrounded in real science. Many times, an analyst with strong knowledge of the business can deliver far more insights than a Data Scientist who is new to the business. For many true “Data Science” roles, we’re probably well-suited with a Statistician. For most mislabeled Data Science roles, we’re probably better off with an analyst or engineer.', 'Sounds simple! Let’s dive in. (Link to further commentary)', 'I’m not entirely sure what the right ratio of analysts to scientists is on a data team. I think it’s totally dependent on the problem you’re trying to solve. What I do know is that hiring 5 Data Scientists to build basic reports on a business’s baseline metrics is a misguided pursuit.', 'There are lots of proper Statisticians out there who do great work, day in and day out. We really never hear about those folks anymore in the data world. Lots of the really good Data Scientists out there are really just statisticians with a new title. Others are a mix of stats, engineering, math, and programming that are a jack of all trades, master of none.', 'It’s no secret why everyone is calling themselves a Data Scientist in 2020. Imagine you’re a Statistician who does great, honest work (science?), day in and day out. According to Indeed, Glassdoor, etc, your salary is somewhere between $70k and $110k each year. And then you look over your shoulder and see a Data Scientist bringing in somewhere between $90k and $165k each year. And in your heart you know that “Data Scientist” doesn’t have nearly the Statistics chops that you’ve got. Maybe they can write a little bit more Python than you or maybe a little bit of R, probably nothing you couldn’t pick up in a few months... Maybe they can do SELECT * FROM Table; and get some data but nothing too sophisticated.', 'So you go to a few company websites, a few job boards, email a few friends and colleagues, and all of a sudden you’ve landed a shiny new job at a different company where you’re now the proud wearer of the title “Data Scientist.”', 'Congrats! Conveniently, you also came in with your shiny graduate degree, your prestigious research background, all of a sudden your $95k salary at your previous company is $130k at your new company. Wow! What an upgrade. And then your boss meets with you and assigns you your work plan. You toil over the work and very quickly realize your day-to-day has gone from a highly skilled statistician workload to that of a SQL warrior. All of a sudden you’re spending 90% of your time building reports, delivering PowerPoints, and building Power BI dashboards to share daily user metrics. You half laugh, half cry as you realize you’re now doing the work of an entry-level data analyst. 10 years of graduate school and another 5 as a postdoc to spend your day writing a few SQL queries and maintain old dashboards.', 'That’s not to say you don’t appreciate the work of analysts. In fact, you believe analysts are foundational to the business, without them the data world wouldn’t go round. If the analysts of the world suddenly stopped showing up to work, leadership would lose their minds; many worlds would collapse into excel spreadsheets from hell. You just know that at your core you’re a statistician, and you prefer to provide the big win at the end of a long research cycle or small one-off consultancies, rather than a bunch of small wins day to day or week to week as an analyst. One isn’t greater than the other; it’s just a different mindset.', 'And yet again, Data Science has claimed another highly skilled worker and given them the job of an analyst with the salary of a mid-level manager. This story isn’t unique, and plenty of people from various backgrounds can relate to this experience.', 'Not only has this statistician found herself in a win-lose situation (win because she’s making lots of money, lose because she hates her job), the business has found itself in a lose-lose situation. The business is paying $130k for a role they could have filled with a highly skilled analyst for $75–100k. They’re also getting a lower quality of work because the statistician just isn’t interested in doing the work. The statistician is clearly overqualified for this work, but studies show that overqualified workers underperform when their heart isn’t in the work.', 'In today’s world, an analyst many times is just someone humble enough to not label themself a Data Scientist. And many “Data Scientists” are analysts with enough confidence to slap a more prestigious role on their nameplate. Heck, their boss can’t tell the difference, so why not change their title and ask for $40k more/yr?', 'I’m hearing story after story come out of data teams that reflect this second reality. Many data teams are at the core of building applications that leverage state of the art models to deliver some novel use-case. Let’s imagine a data team that is building a text classifier using BERT. Imagine this is a core feature of a soon to be deployed web application.', 'It’s hilarious to me the amount of team building that involves gathering a bunch of Data Scientists in a room to build this sort of application. What you clearly need is a team of engineers — maybe some front-end devs, a few back-end folks, maybe a ML Engineer or two whose worked with these models in the past. The last thing you need is a bunch of Data Scientists running around trying to tell you how the model works or sleeping on the desk while they await the opportunity to run model validation.', 'It probably doesn’t hurt to have 1 Data Scientist in the room. However, you probably don’t need someone with intermediate R knowledge hopping in at every turn. You need someone with really great statistical chops that can do really great science and keep the team honest as they deliver a working solution. Leave the code writing to the engineers. A dashboard isn’t going to deliver inference at scale.', 'Data Scientists are not here to build you scalable applications, build you front-ends, build your data pipeline, or really any of the tasks that are foundational to building a shippable piece of software. Where a good Data Scientist can chip in is in making sure you’re performing good science, making sure the application has the capability needed to deliver real-world results, etc… They’re not there to write performant code. They’re essential in certain scenarios, a hinderance in others. Good management is essential for delegating the work and building these teams properly.', 'In today’s world, lots of developers and engineers are finding it convenient to label themselves a Data Scientist. They’ve got a CS undergrad and maybe even an MS in CS. Their Statistics background is a few quick Wikipedia articles or maybe an Engineering Statistics course. But meh, they don’t really need to understand it because their boss just wants an application that works by using existing technologies and pre-built Stats libraries. So they slap the Data Scientist role on their nameplate and try and command a few extra bucks. And they’re a great engineer so they’re gonna ship some really great software.', 'The problem is that engineer’s (labeled a Data Scientist) boss now thinks they need to go hire 3 more Data Scientists to deliver more of the same end-product. All of a sudden they go out and hire some really great Stats people who 3 months later realize they’re trying to do an engineer’s job with a Statistician’s background. Fast forward a little while and they’ve lost interest and it’s a win-lose (win because they’re making a great salary, lose because they hate their job) for them and a lose-lose for the business.', 'Sound familiar?', 'Let’s continue the conversation on Twitter.', 'Check out Dataset Daily, a newsletter where we study companies, industries, and markets each week.']"
01/2020,5 Ways Julia Is Better Than Python,Why Julia is better than Python for DS/ML,3.1K,23,https://towardsdatascience.com/@emmettgb,https://towardsdatascience.com/5-ways-julia-is-better-than-python-334cc66d64ae?source=collection_archive---------4-----------------------,4,7,"['5 Ways Julia Is Better Than Python', '№1: Speed', '№2: Versatility', '№3: Multiple Dispatch', '№4: Made For ML', '№5: Package Manager', 'Conclusion']",11,"['Julia is a multi-paradigm, primarily functional programming language that was created for machine-learning and statistical programming. Python is another multi-paradigm programming language that is used for machine-learning, though generally Python is considered to be object-oriented. Julia, on the other hand, is more based on the functional paradigm. Though Julia certainly isn’t as popular as Python, there are some huge benefits to using Julia for Data Science that make it a better choice in a lot of situations that Python.', 'It’s hard to talk about Julia without talking about speed. Julia prides itself on being very fast. Julia, unlike Python which is interpreted, is a compiled language that is primarily written in its own base. However, unlike other compiled languages like C, Julia is compiled at run-time, whereas traditional languages are compiled prior to execution. Julia, especially when written well, can be as fast and sometimes even faster than C. Julia uses the Just In Time (JIT) compiler and compiles incredibly fast, though it compiles more like an interpreted language than a traditional low-level compiled language like C, or Fortran.', 'You might have noticed that I said Python was versatile as an advantage to Julia, and this is true — there are a lot of things that can be done with Python that you just can’t do with Julia. Of course, this is only natively speaking, because the versatility we’re talking about now is versatility in language. Julia code is universally executable in R, Latex, Python, and C. This means that typical Data Science projects have the potential to be written once, and compiled in Julia natively from another language in a wrapper, or just by sending strings.', 'PyCall and RCall are also pretty big deals. Given that a serious downside to Julia is in fact the packages, it’s really convenient to be able to call on Python and R whenever you need them. PyCall is very well implemented into Julia, and is definitely sincerely well-done, and very usable.', 'Julia is a very uniquely typed language and has its own quirks and features, but among one of the coolest features is Julia’s multiple dispatch. First and foremost, Julia’s multiple dispatch is fast. On top of that, using Julia’s polymorphic dispatch allows for applying function definitions as properties of a struct. This, of course, makes inheritance viable inside of Julia.', 'Not only that, but using Julia’s multiple dispatch makes a function extendable. This is a great benefit for package extensions, as whenever a method is explicitly imported, it can be changed by a user. It would be easy to explicitly import your method and extend it to route structs to a new function.', 'Unlike Python, Julia was made with the intention of being used in statistics and machine-learning. Python was created in the early 90s as an easy object-oriented language, though it has changed a lot since then. Given Python’s history, and the wide variety of uses for Python since it’s so popular, using a language that was made specifically for high-level statistical work could show a lot of benefits.', 'One way I see this benefiting Julia over Python is in linear algebra. Vanilla Python can chug through linear algebra, but vanilla Julia can fly through linear algebra. This is because of course Python was never meant to support all of the matrices and equations that go along with machine-learning. By no means at all is Python bad, especially with NumPy, but in terms of a no-package experience, Julia feels a lot more catered towards these sorts of mathematics. Julia’s operand system is a lot closer to that of R than Python’s, and that’s a big benefit. Most linear algebra is quicker and easier to do. Let’s show a dot-product equation, just to illustrate this further:', 'I’ll be the first to say it, Julia’s Pkg package manager is an entire world above Python’s Pip package manager. Pkg comes loaded with its own REPL and Julia package from which you can build, add, remove, and instantiate packages. This is especially convenient because of Pkg’s tie-in with Git. Updating is easy, adding packages is always easy, and overall Pkg is a pleasure to use over Python’s Pip any day.', 'It doesn’t really matter which language you use, be it R, Julia, Python, or Scala. It is important to note, however that every language has its downsides, and no language is ever going to be the “ perfect language.” This is especially true if you are versatile in your programming, from machine-learning to GUIs to APIs. With that being said, Julia is certainly one of my favorites in my arsenal, as well as Python. Python has better packages, and with that typically if the project is small enough, I’ll veer towards Python, but for data-sets with millions of observations, it can be hard to even get that kind of data read in Python.', 'Overall, I look forward into the future of Julia. Julia’s a lot of fun to write, and will likely become even more viable for Data Science in the future.']"
01/2020,"30 Python Best Practices, Tips, And Tricks",Improve your Python knowledge and skills,3.6K,14,https://towardsdatascience.com/@eriky,https://towardsdatascience.com/30-python-best-practices-tips-and-tricks-caefb9f8c5f5?source=collection_archive---------5-----------------------,9,31,"['30 Python Best Practices, Tips, And Tricks', '1. Use Python 3', '2. Check for a minimum required Python version', '3. Use IPython', '4. List Comprehensions', '5. Check memory usage of your objects', '6. Return multiple values', '7. Use data classes', '8. In place variable swapping', '9. Merging dictionaries (Python 3.5+)', '10. String to title case', '11. Split a string into a list', '12. Create a string from a list of strings', '13. Emoji', '14. Slicing a list', '15. Reversing strings and lists', '16. Display kittens', '17. Using map()', '18. Get unique elements from a list or string', '19. Find the most frequently occurring value', '20. Create a progress bar', '21. Use the _ in an interactive shell', '22. Quickly create a web server', '23. Multi-Line Strings', '24. Ternary Operator For Conditional Assignment', '25. Counting occurrences', '26. Chaining of comparison operators', '27. Add some color', '28. Working with dates', '29. Integer division', '30. Charset detection with chardet']",84,"['Here are 30 Python best practices, tips, and tricks. I’m sure they’ll help you procrastinate your actual work, and still learn something useful in the process.', 'For more Python articles and a terrific Python tutorial, head over to my website Python Land.', 'In case you missed it: Python 2 is officially not supported as of January 1, 2020. This guide has a bunch of examples that only work in Python 3. If you’re still on Python 2.7, upgrade now.', 'If you’re on MacOS, you can use Homebrew to painlessly upgrade Python.', 'You can check for the Python version in your code, to make sure your users are not running your script with an incompatible version. Use this simple check:', 'IPython is basically an enhanced shell. It’s worth it just for the autocompletion alone, but there is much more. I like it too for all the magic commands that are built-in. Here are a few :', 'Read the full list here.', 'Another useful feature is referencing the output of a previous command. In and Out are actual objects. You can use the output of the 3rd command by using Out[3].', 'Install IPython with:', 'pip3 install ipython', 'A list comprehension can replace ugly for loops used to fill a list. The basic syntax for a list comprehension is:', 'A very basic example to fill a list with a sequence of numbers:', 'And because you can use an expression, you can also do some math:', 'Or even call an external function:', 'And finally, you can use the ‘if’ to filter the list. In this case, we only keep the values that are dividable by 2:', 'With sys.getsizeof() you can check the memory usage of an object:', 'Woah… wait… why is this huge list only 48 bytes?', 'It’s because the range function returns a class that only behaves like a list. A range is a lot more memory efficient than using an actual list of numbers.', 'You can see for yourself by using a list comprehension to create an actual list of numbers from the same range:', 'Functions in Python can return more than one variable without the need for a dictionary, a list or a class. It works like this:', 'This is alright for a limited number of return values. But anything past 3 values should be put into a (data) class.', 'Since version 3.7, Python offers data classes. There are several advantages over regular classes or other alternatives like returning multiple values or dictionaries:', 'Here’s an example of a data class at work:', 'An in-depth guide can be found here.', 'A neat little trick that can save a few lines of code:', 'Since Python 3.5, it became easier to merge dictionaries:', 'If there are overlapping keys, the keys from the first dictionary will be overwritten.', 'This is just one of those lovely gems:', 'You can split a string into a list of strings. In this case, we split on the space character:', 'To split on whitespace, you actually don’t have to give split any arguments. By default, all runs of consecutive whitespace are regarded as a single whitespace separator by split. So we could just as well use mystring.split().', 'And vice versa from the previous trick, create a string from a list and put a space character between each word:', 'If you were wondering why it’s not mylist.join("" "") — good question!', 'It comes down to the fact that the String.join() function can join not just lists, but any iterable. Putting it inside String prevents implementing the same functionality in multiple places.', 'This one will either impress or repulse, depending on who’s looking. On a more serious note, this can come in handy especially when analyzing social media data.', 'First, install the emoji module:', 'With this installed, you can do as follows:', 'Visit the emoji package page for more examples and documentation.', 'The basic syntax of list slicing is:', 'Start, stop and step are optional. If you don’t fill them in, they will default to:', 'Here are some examples:', 'You can use the slice notation from above to reverse a string or list. By using a negative stepping value of -1, the elements are reversed:', 'I finally found a good excuse to include kittens in one of my articles! You, however, might use it to display graphs and such. First, install Pillow, a fork of the Python Image Library:', 'Now download this image to a file called kittens.jpg:', 'You can use the following code to display the image from your Python code:', 'Or you can do it right from IPython:', 'Pillow can do a lot more than displaying the image. It can analyze, resize, filter, enhance, morph, etcetera. See the documentation for all its features.', 'One of Python’s built-in functions is called map(). The syntax for map() is:', 'map(function, something_iterable)', 'So you give it a function to execute, and something to execute on. This can be anything that’s iterable. In the examples below I’ll use a list.', 'Take a look at your own code and see if you can use map() instead of a loop somewhere!', 'By creating a set with the set() function, you get all the unique elements from a list or list-like object:', 'To find the most frequently occurring value in a list or string:', 'Do you understand why this works? Try to figure it out for yourself before reading on.', 'You didn’t try, did you? I’ll tell you anyway:', 'So what we do in this single line of code is take all the unique values of test, which is {1, 2, 3, 4}. Next, max will apply the list.count function to them and return the maximum value.', 'And no — I didn’t invent this one-liner.', 'You can create your own progress bar, which is fun to do. But it’s quicker to use the progress package:', 'pip3 install progress', 'Now you can create a progress bar with minimal effort:', 'The following animation demonstrates all the available progress types:', 'You can obtain the result of the last expression with the underscore operator, e.g. in IPython this looks like:', 'This works in the Python shell too. In addition, the IPython shell allows you to use Out[n] to get the value of the expression In[n]. E.g., Out[1] would give us the number 9 in the example above.', 'You can quickly start a web server, serving the contents of the current directory:', 'This is useful if you want to share some stuff with a co-worker or want to test a simple HTML site.', 'Although you can use triple quotes to include multi-line strings in your code, it’s not ideal. Everything you put between the triple quotes becomes the string, including the formatting, as you can see below.', 'I prefer the second way, which concatenates multiple lines together, allowing you to format your code nicely. The only downside is that you need to explicitly put in newlines.', 'This is another one of those ways to make your code more concise while still keeping it readable:', 'As an example:', 'You can use Counter from the collections library to get a dictionary with counts of all the unique elements in a list:', 'You can chain comparison operators in Python, creating more readable and concise code:', 'With Colorama, you can add some color to your terminal.', 'The python-dateutil module provides powerful extensions to the standard datetime module. Install it with:', 'You can do so much cool stuff with this library. I’ll limit the examples to just this one that I found particularly useful: fuzzy parsing of dates from log files and such.', 'Just remember: where the regular Python datetime functionality ends, python-dateutil comes in!', 'In Python 2, the division operator ( / ) defaults to an integer division, unless one of the operands is a floating-point number. So you have this behavior:', 'In Python 3, the division operator defaults to a floating-point division and the // operator has become an integer division. So we get:', 'For the complete motivation behind this change, you should read PEP-0238.', 'You can use the chardet module to detect the charset of a file. This can come in very useful when analyzing big piles of random text. Install with:', 'pip install chardet', 'You now have an extra command-line tool called chardetect, which can be used like this:', 'You can also use the library programmatically, check out the docs.', 'That’s it! 30 tips, tricks, and best practices to start off the new year. I hope you enjoyed them as much as I enjoyed creating the list. If you have anything to add, feel free to leave a comment!', 'You’ll probably like this following article as well:', 'If you enjoyed this article, please subscribe to my Substack.']"
01/2020,Google just published 25 million free datasets,Here’s what you need to know about the largest data…,8.9K,29,https://towardsdatascience.com/@tjwaterman99,https://towardsdatascience.com/google-just-published-25-million-free-datasets-d83940e24284?source=collection_archive---------6-----------------------,2,1,['Google just published 25 million free datasets'],12,"['Google recently released datasetsearch, a free tool for searching 25 million publicly available datasets.', 'The search tool includes filters to limit results based on their license (free or paid), format (csv, images, etc), and update time.', 'The results also include descriptions of the dataset’s contents as well as author citations.', 'Google’s dataset aggregation methodology differs from other dataset repositories like Amazon’s open data registry. Unlike other repositories that curate and host the datasets themselves, Google does not curate or provide direct access to the 25 million datasets directly.', 'Instead, Google relies on the dataset publishers to use the open standards of schema.org to describe their dataset’s metadata. Google then indexes and makes that metadata searchable across publishers.', 'Since publishers are still required to host the datasets themselves, for-profit publishers that conform to schema.org standards will also have their datasets indexed by Google. In my anecdotal experience, I found about half of the datasets in the search results were from for-profit aggregators, with an even higher percentage when searching for market-related datasets.', 'Other popular dataset publishers on the platform include government agencies and research institutions. Google claims that US government agencies alone have published over 2 million datasets.', 'According to Google, most of the datasets are related to “geosciences, biology, and agriculture.”', 'To publish your own datasets, you can simply use the open-standards of schema.org. The number of publicly available datasets is likely to continue growing as more publishers conform to the standard.', 'At this time, Google does not provide an API for searching or downloading the free datasets.', 'More information about the release is available on Google’s blog.', 'Thanks for reading! I write on Medium about data science related topics, so you can follow me to get notified when I write more articles like this one.']"
01/2020,How to Supercharge Excel With Python,How to integrate Python and Excel with xlwings,2.7K,8,https://towardsdatascience.com/@costasandreou,https://towardsdatascience.com/how-to-supercharge-excel-with-python-726b0f8e22c2?source=collection_archive---------7-----------------------,6,7,"['How to Supercharge Excel With Python', 'Why integrate Python with Excel VBA?', 'Getting Set Up to Use xlwings', 'Getting Started with xlwings', 'VBA to Python', 'User-Defined Functions with xlwings', 'Conclusion']",35,"['As much as Excel is a blessing, it is also a curse. When it comes to smallish enough data and simple enough operations Excel is king. Once you find yourself endeavoring outside of those zones however, it becomes a pain. Sure enough, you can use Excel VBA to get around such issues, but in the year 2020, you can thank your lucky stars because you don’t have to!', 'If only there was a way to integrate Excel with Python to give Excel… wings! Well, now there is. A python library called xlwings allows you to call Python scripts through VBA and pass data between the two.', 'The truth of the matter is, you can pretty much do anything in VBA. So, if that is the case, why would you want to use Python? Well, there are a number of reasons.', 'The first thing we need to do, as with any new library we want to use, is to install it. It’s super easy to do; with two commands we’ll be set up in no time. So, go ahead and type in your terminal:', 'Once the library has been downloaded and installed, we need to install the Excel integration part. Ensure you’ve closed down all your Excel instances and in any terminal type:', 'Assuming you experience no errors, you should be able to proceed. However, oftentimes on Win10 with Excel 2016, people will see the following error:', 'If you are one of the lucky ones to experience the above error, all you need to do is create the missing directory. You can do that easily by using the mkdir command. In my case, I did:', 'Assuming the successful installation of the excel integration with the python library, the main difference you will immediately notice is in Excel:', 'First up, we need to load the Excel Add-in. You can hit Alt, L, H and then navigate to the directory above to load the plugin. Once you’re done, you should be able to see the following:', 'Finally, you need to Enable Trust access to the VBA project object model. You can do that by navigating to File > Options > Trust Center > Trust Center Settings > Macro Settings:', 'There are two main ways you can go from Excel to Python (and back). The first one is to call a Python script directly from VBA, while the other one is through a User Defined Function. Let us have a quick look at both.', 'In order to avoid any confusion and to have the correct set up every time, xlwings offers to create your Excel spreadsheet, ready to go. Let us then use this functionality. Using the terminal, we navigate to the directory we like and type:', 'I am calling this MyFirstPythonXL. The above command will create a new folder in your pre-navigated directory with an Excel worksheet and a python file.', 'Opening the .xlsm file, you immediately notice a new Excel sheet called _xlwings.conf. Should you wish to override the default settings of xlwings, all you have to do is rename this sheet and remove the starting underscore. And with that, we are all set up and ready to begin using xlwings.', 'Before we jump into the coding, let us first ensure we are all on the same page. To bring up our Excel VBA editor, hit Alt + F11. This should return the following screen:', 'The key things to note here is that this code will do the following:', 'Without any further ado, let us look at a few examples of how this can be used.', 'In this example, we will see how you carry operations outside of Excel, but then return your results in the spreadsheet. This can have an infinite amount of use cases.', 'We will source data from a CSV file, do a modification on said data, and then pass the output to Excel. Let’s review how easy it is:', 'First up, the VBA code:', 'I have left this completely unchanged from the default.', 'Then, the Python code:', 'Which results in the following:', 'In this example, we will read inputs from Excel, do something with it in Python, and then pass the result back to Excel.', 'More specifically, we are going to read a Greeting, a Name and a file location of where we can find jokes. Our Python script will then take a random line from the file, and return us a joke.', 'First up, the VBA code:', 'I have left this completely unchanged from the default.', 'Then, the Python code:', 'Which gives us:', 'In pretty much the same fashion as before, we will be changing the code in the python file. In order to turn something into an Excel User Defined Function, all we need to do is include ‘@xw.func’ before the line the function is on:', 'The Python code:', 'The result:', 'I think you would agree that this is a nifty little library. If you are like me and you much prefer to work in Python rather than VBA but need to work in spreadsheets, then this can be an exceptional tool.', 'Want to stay up to date with my blogs? Don’t forget to follow me!', 'Here are some of the other articles which you might find interesting:']"
01/2020,"Understanding Audio data, Fourier Transform, FFT, Spectrogram and Speech Recognition",-,1.2K,11,https://towardsdatascience.com/@kartikgill96,https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520?source=collection_archive---------8-----------------------,12,8,"['Understanding Audio data, Fourier Transform, FFT and Spectrogram features for a Speech Recognition System', 'Overview', '1. Reading Audio Files', '2. Fourier Transform (FT)', '3. Fast Fourier Transform (FFT)', '4. Spectrogram', '5. Speech Recognition using Spectrogram Features', '6. Conclusion']",49,"['A huge amount of audio data is being generated every day in almost every organization. Audio data yields substantial strategic insights when it is easily accessible to the data scientists for fuelling AI engines and analytics. Organizations that have already realized the power and importance of the information coming from the audio data are leveraging the AI (Artificial Intelligence) transcribed conversations to improve their staff training, customer services and enhancing overall customer experience.', 'On the other hand, there are organizations that are not able to put their audio data to better use because of the following barriers — 1. They are not capturing it. 2. The quality of the data is bad. These barriers can limit the potential of Machine Learning solutions (AI engines) they are going to implement. It is really important to capture all possible data and also in good quality.', 'This article provides a step-wise guide to start with audio data processing. Though this will help you get started with basic analysis, It is never a bad idea to get a basic understanding of the sound waves and basic signal processing techniques before jumping into this field. You can click here and check out my article on sound waves. That article provides a basic understanding of sound waves and also explains a bit about different audio codecs.', 'Before going further, let’s list out the content we are going to cover in this article. Let’s go through each of the following topics sequentially—', 'LibROSA is a python library that has almost every utility you are going to need while working on audio data. This rich library comes up with a large number of different functionalities. Here is a quick light on the features —', 'As this library is huge, we are not going to talk about all the features it carries. We are just going to use a few common features for our understanding.', 'Here is how you can install this library real quick —', 'Librosa supports lots of audio codecs. Although .wav(lossless) is widely used when audio data analysis is concerned. Once you have successfully installed and imported libROSA in your jupyter notebook. You can read a given audio file by simply passing the file_path to librosa.load() function.', 'librosa.load() —> function returns two things — 1. An array of amplitudes. 2. Sampling rate. The sampling rate refers to ‘sampling frequency’ used while recording the audio file. If you keep the argument sr = None , it will load your audio file in its original sampling rate. (Note: You can specify your custom sampling rate as per your requirement, libROSA can upsample or downsample the signal for you). Look at the following image —', 'sampling_rate = 16k says that this audio was recorded(sampled) with a sampling frequency of 16k. In other words, while recording this file we were capturing 16000 amplitudes every second. Thus, If we want to know the duration of the audio, we can simply divide the number of samples (amplitudes) by the sampling-rate as shown below —', '“Yes you can play the audio inside your jupyter-notebook.”', 'IPython gives us a widget to play audio files through notebook.', 'We have got amplitudes and sampling-rate from librosa. We can easily plot these amplitudes with time. LibROSA provides a utility function waveplot() as shown below —', 'This visualization is called the time-domain representation of a given signal. This shows us the loudness (amplitude) of sound wave changing with time. Here amplitude = 0 represents silence. (From the definition of sound waves — This amplitude is actually the amplitude of air particles which are oscillating because of the pressure change in the atmosphere due to sound).', 'These amplitudes are not very informative, as they only talk about the loudness of audio recording. To better understand the audio signal, it is necessary to transform it into the frequency-domain. The frequency-domain representation of a signal tells us what different frequencies are present in the signal. Fourier Transform is a mathematical concept that can convert a continuous signal from time-domain to frequency-domain. Let’s learn more about Fourier Transform.', 'An audio signal is a complex signal composed of multiple ‘single-frequency sound waves’ which travel together as a disturbance(pressure-change) in the medium. When sound is recorded we only capture the resultant amplitudes of those multiple waves. Fourier Transform is a mathematical concept that can decompose a signal into its constituent frequencies. Fourier transform does not just give the frequencies present in the signal, It also gives the magnitude of each frequency present in the signal.', 'Inverse Fourier Transform is just the opposite of the Fourier Transform. It takes the frequency-domain representation of a given signal as input and does mathematically synthesize the original signal.', 'Let’s see how we can use Fourier transformation to convert our audio signal into its frequency components —', 'Fast Fourier Transformation(FFT) is a mathematical algorithm that calculates Discrete Fourier Transform(DFT) of a given sequence. The only difference between FT(Fourier Transform) and FFT is that FT considers a continuous signal while FFT takes a discrete signal as input. DFT converts a sequence (discrete signal) into its frequency constituents just like FT does for a continuous signal. In our case, we have a sequence of amplitudes that were sampled from a continuous audio signal. DFT or FFT algorithm can convert this time-domain discrete signal into a frequency-domain.', 'To understand the output of FFT, let’s create a simple sine wave. The following piece of code creates a sine wave with a sampling rate = 100, amplitude = 1 and frequency = 3. Amplitude values are calculated every 1/100th second (sampling rate) and stored into a list called y1. We will pass these discrete amplitude values to calculate DFT of this signal using the FFT algorithm.', 'If you plot these discrete values(y1) keeping sample number on x-axis and amplitude value on y-axis, it generates a nice sine wave plot as the following screenshot shows —', 'Now we have a sequence of amplitudes stored in list y1. We will pass this sequence to the FFT algorithm implemented by scipy. This algorithm returns a list yf of complex-valued amplitudes of the frequencies found in the signal. The first half of this list returns positive-frequency-terms, and the other half returns negative-frequency-terms which are similar to the positive ones. You can pick out any one half and calculate absolute values to represent the frequencies present in the signal. Following function takes samples as input and plots the frequency graph —', 'In the following graph, we have plotted the frequencies for our sine wave using the above fft_plot function. You can see this plot clearly shows the single frequency value present in our sine wave, which is 3. Also, it shows amplitude related to this frequency which we kept 1 for our sine wave.', 'To check out the output of FFT for a signal having more than one frequency, Let’s create another sine wave. This time we will keep sampling rate = 100, amplitude = 2 and frequency value = 11. Following code generates this signal and plots the sine wave —', 'Generated sine wave looks like the below graph. It would have been smoother if we had increased the sampling rate. We have kept the sampling rate = 100 because later we are going to add this signal to our old sine wave.', 'Obviously FFT function will show a single spike with frequency = 11 for this wave also. But we want to see what happens if we add these two signals of the same sampling rate but the different frequency and amplitude values. Here sequence y3 will represent the resultant signal.', 'If we plot the signal y3, it looks something like this —', 'If we pass this sequence (y3) to our fft_plot function. It generates the following frequency graph for us. It shows two spikes for the two frequencies present in our resultant signal. So the presence of one frequency does not affect the other frequency in the signal. Also, one thing to notice is that the magnitudes of the frequencies are in line with our generated sine waves.', 'Now that we have seen how this FFT algorithm gives us all the frequencies in a given signal. let’s try to pass our original audio signal into this function. We are using the same audio clip we loaded earlier into the python with a sampling rate = 16000.', 'Now, look at the following frequency plot. This ‘3-second long’ signal is composed of thousands of different frequencies. Magnitudes of frequency values > 2000 are very small as most of these frequencies are probably due to the noise. We are plotting frequencies ranging from 0 to 8kHz because our signal was sampled at 16k sampling rate and according to the Nyquist sampling theorem, it should only posses frequencies ≤ 8000Hz (16000/2).', 'Strong frequencies are ranging from 0 to 1kHz only because this audio clip was human speech. We know that in a typical human speech this range of frequencies dominates.', 'We got frequencies But where is the Time information?', 'Suppose you are working on a Speech Recognition task. You have an audio file in which someone is speaking a phrase (for example: How are you). Your recognition system should be able to predict these three words in the same order (1. ‘how’, 2. ‘are’, 3. ‘you’). If you remember, in the previous exercise we broke our signal into its frequency values which will serve as features for our recognition system. But when we applied FFT to our signal, it gave us only frequency values and we lost the track of time information. Now our system won’t be able to tell what was spoken first if we use these frequencies as features. We need to find a different way to calculate features for our system such that it has frequency values along with the time at which they were observed. Here Spectrograms come into the picture.', 'Visual representation of frequencies of a given signal with time is called Spectrogram. In a spectrogram representation plot — one axis represents the time, the second axis represents frequencies and the colors represent magnitude (amplitude) of the observed frequency at a particular time. The following screenshot represents the spectrogram of the same audio signal we discussed earlier. Bright colors represent strong frequencies. Similar to earlier FFT plot, smaller frequencies ranging from (0–1kHz) are strong(bright).', ""Idea is to break the audio signal into smaller frames(windows) and calculate DFT (or FFT) for each window. This way we will be getting frequencies for each window and window number will represent the time. As window 1 comes first, window 2 next…and so on. It's a good practice to keep these windows overlapping otherwise we might lose a few frequencies. Window size depends upon the problem you are solving."", 'For a typical speech recognition task, a window of 20 to 30ms long is recommended. A human can’t possibly speak more than one phoneme in this time window. So keeping the window this much smaller we won’t lose any phoneme while classifying. The frame (window) overlap can vary from 25% to 75% as per your need, generally, it is kept 50% for speech recognition.', 'In our spectrogram calculation, we will keep the window duration 20ms and an overlap of 50% among the windows. Because our signal is sampled at 16k frequency, each window is going to have (16000 * 20 * 0.001) = 320 amplitudes. For an overlap of 50%, we need to go forward by (320/2) = 160 amplitude values to get to the next window. Thus our stride value is 160.', 'Have a look at the spectrogram function in the following image. In line-18 we are making a weighting window( Hanning ) and multiplying it with amplitudes before passing it to FFT function in line-20. Weighting window is used here to handle discontinuity of this small signal(small signal from a single frame) before passing it to the DFT algorithm. To learn more about why the weighting window is necessary — click here.', 'A python function to calculate spectrogram features —', 'The output of the FFT algorithm is a list of complex numbers (size = window_size /2) which represent amplitudes of different frequencies within the window. For our window of size 320, we will get a list of 160 amplitudes of frequency bins which represent frequencies from 0 Hz — 8kHz (as our sampling rate is 16k) in our case.', 'Going forward, Absolute values of those complex-valued amplitudes are calculated and normalized. The resulting 2D matrix is your spectrogram. In this matrix rows and columns represent window frame number and frequency bin while values represent the strength of the frequencies.', 'We know how to generate a spectrogram now, which is a 2D matrix representing the frequency magnitudes along with time for a given signal. Now think of this spectrogram as an image. You have converted your audio file into the following image.', 'This reduces it to an image classification problem. This image represents your spoken phrase from left to right in a timely manner. Or consider this as an image where your phrase is written from left to right, and all you need to do is identify those hidden English characters.', 'Given a parallel corpus of English text, we can train a deep learning model and build a speech recognition system of our own. Here are two well known open-source datasets to try out —', 'Popular choices of deep learning architectures can be understood from the following nice research papers —', 'This article shows how to deal with audio data and a few audio analysis techniques from scratch. Also, it gives a starting point for building speech recognition systems. Although, Above research shows very promising results for the recognition systems, still many don’t see speech recognition as a solved problem because of the following pitfalls —', 'There are huge opportunities in this field of research. Improvements can be done from the data preparation point of view (by creating better features) and also from the model architecture point of view (by presenting a more robust and scalable deep learning architecture).', 'Originally published here.', 'Thanks for reading, please let me know your comments/feedback.']"
01/2020,Making Python Programs Blazingly Fast,Let’s look at the performance of our Python programs and see how…,3.4K,3,https://towardsdatascience.com/@martin.heinz,https://towardsdatascience.com/making-python-programs-blazingly-fast-c1cd79bd1b32?source=collection_archive---------9-----------------------,5,4,"['Making Python Programs Blazingly Fast', 'Timing and Profiling', 'Making It Faster', 'Conclusion']",25,"['Python haters always say, that one of the reasons they don’t want to use it, is that it’s slow. Well, whether specific program — regardless of the programming language used — is fast or slow is very much dependent on the developer who wrote it and their skill and ability to write optimized and fast programs.', 'So, let’s prove some people wrong and let’s see how we can improve performance of our Python programs and make them really fast!', 'Before we start optimizing anything, we first need to find out which parts of our code actually slow down the whole program. Sometimes the bottleneck of the program might be obvious, but in case you don’t know where it is, then here are options you have for finding out:', 'Note: This is the program I will be using for demonstration purposes, it computes e to power of X (taken from Python docs):', 'First off, the simplest and honestly very lazy solution — Unix time command:', 'This could work if you just want to time your whole program, which is usually not enough…', 'On the other end of the spectrum is cProfile, which will give you too much information:', 'Here, we ran the testing script with cProfile module and time argument, so that lines are ordered by internal time ( cumtime). This gives us a lot of information, the lines you can see above are about 10% of the actual output. From this, we can see that exp function is the culprit ( surprise, surprise) and now we can get little more specific with timing and profiling...', 'Now that we know where to direct our attention, we might want to time the slow function, without measuring the rest of the code. For that we can use simple decorator:', 'This decorator can be then applied to function under test like so:', 'This gives us output like this:', 'One thing to consider is what kind of time we actually (want to) measure. Time package provides time.perf_counter and time.process_time. The difference here is that perf_counter returns absolute value, which includes time when your Python program process is not running, therefore it might be impacted by machine load. On the other hand process_time returns only user time (excluding system time), which is only the time of your process.', 'Now, for the fun part. Let’s make your Python programs run faster. I’m (mostly) not going to show you some hacks, tricks and code snippets that will magically solve your performance issues. This is more about general ideas and strategies, which when used, can make a huge impact on performance, in some cases up to 30% speed-up.', 'This one is pretty obvious. Built-in data types are very fast, especially in comparison to our custom types like trees or linked lists. That’s mainly because the built-ins are implemented in C, which we can’t really match in speed when coding in Python.', 'I have already shown this one in a previous blog post here, but I think it’s worth repeating it with simple example:', 'The function above simulates heavy computation using time.sleep. When called first time with parameter 1, it waits for 2 seconds and only then returns the result. When called again, the result is already cached so it skips the body of the function and returns the result immediately. For more real life example see previous blog posts here.', 'This has to do with the speed of lookup of variables in each scope. I’m writing each scope, because it’s not just about using local vs. global variables. There’s actually a difference in speed of lookup even between — let’s say — local variable in function (fastest), class-level attribute (e.g. self.name - slower) and global for example imported function like time.time (slowest).', 'You can improve performance, by using seemingly unnecessary (straight-up useless) assignments like this:', 'This might seem counter-intuitive, as calling function will put more stuff onto the stack and create overhead from function returns, but it relates to the previous point. If you just put your whole code into one file without putting it into function, it will be much slower because of global variables. Therefore you can speed up your code just by wrapping whole code in main function and calling it once, like so:', 'Another thing that might slow down your programs is dot operator (.) which is used when accessing object attributes. This operator triggers dictionary lookup using __getattribute__, which creates extra overhead in your code. So, how can we actually avoid (limit) using it?', ""Operations on strings can get quite slow when ran in loop using for example modulus (%s) or .format(). What better options do we have? Based on recent tweet from Raymond Hettinger, the only thing we should be using is f-string, it's most readable, concise AND the fastest method. So, based on that tweet, this is the list of methods you can use - fastest to slowest:"", 'Generators are not inherently faster as they were made to allow for lazy computation, which saves memory rather than time. However, the saved memory can be cause for your program to actually run faster. How? Well, if you have a large dataset and you don’t use generators (iterators), then the data might overflow CPUs L1 cache, which will slow down lookup of values in memory significantly.', 'When it comes to performance, it’s very import that CPU can save all the data it’s working on, as close as possible, which is in the cache. You can watch Raymond Hettingers talk, where he mentions these issues.', 'The first rule of optimization is to not do it. But, if you really have to, then I hope these few tips help you with that. However, be mindful when optimizing your code as it might end up making your code hard to read and therefore hard to maintain, which might outweigh benefits of optimization.', 'Note: This was originally posted at martinheinz.dev']"
02/2020,Top 10 In-Demand programming languages to learn in 2020,The Definitive Guide,3.3K,20,https://towardsdatascience.com/@md.kamaruzzaman,https://towardsdatascience.com/top-10-in-demand-programming-languages-to-learn-in-2020-4462eb7d8d3e?source=collection_archive---------0-----------------------,21,12,"['Top 10 In-Demand programming languages to learn in 2020', '1. Python', '2. JavaScript', '3. Java', '4. C#', '5. C', '6. C++', '7. PHP', '8. Swift', '9. Go', '10. Ruby', 'Similar articles:']",154,"['There are around 600 programming languages out there. The demand and popularity of programming languages fluctuate every year. Also, new programming languages are coming with attractive features.', 'So, which programming language should you learn? Learning a new programming language is always an investment of your time and brainpower. If you are a seasoned developer or already know several programming languages, you can learn a niche, modern one. Recently, I have written a blog post where I have short-listed seven modern programming languages worth learning:', 'But if you were starting your programming career in 2020 or learning your first or second programming language, then it is wise to learn one of the mainstream and established programming languages. Here I will list programming languages based on the following criteria:', 'Also, I will summarize the programming languages along with historical context, key features, and primary use cases.', 'When Guido van Rossum developed Python in the 1990s as his side project, nobody has thought it would be the most popular programming language one day. Considering all well-recognized rankings and industry trends, I put Python as the number one programming language overall.', 'Python has not seen a meteoric rise in popularity like Java or C/C++. Also, Python is not a disruptive programming language. But from the very beginning, Python has focused on developer experience and tried to lower the barrier to programming so that school kids can also write production-grade code.', 'In 2008, Python went through a massive overhaul and improvement with the cost of introducing significant breaking changes by introducing Python 3.', 'Today, Python is omnipresent and used in many software development areas, with no sign of slowing down.', '3 Key Features:', 'Popularity:', 'In the last several years, Python has seen enormous growth in demand with no sign of slowing down. The programming language ranking site PYPL has ranked Python as the number one programming language with a considerable popularity gain in 2019:', 'Also, Python has surpassed Java and became the 2nd most popular language according to GitHub repositories contributions:', 'Also, StackOverflow developer survey has ranked Python as the 2nd most popular programming language (4th most popular Technology):', 'Another programming language ranking site TIOBE has ranked Python the 3rd most popular language with a massive gain in last year:', 'Python still has the chance to go further up in ranking this year as Python saw a 50% growth last year according to GitHub Octoverse:', 'StackOverflow developer survey has listed Python as the second most loved programming language:', 'Most of the older and mainstream programming languages have stable or downward traction. Also, Python is an exception here and has an increasingly upward trending during the last five years, as clear from Google trends:', 'Job Market:', 'According to Indeed, Python is the most demanding programming language in the USA job market, with the highest 74 K job posting in January 2020. Also, Python ranked third with a $120 K yearly salary.', 'Also, StackOverflow developer survey has shown that Python developers earn a high salary with relatively low experience compared to other mainstream programming languages:', 'Main Use Cases:', 'During the first browser war, Netscape had assigned Brendan Eich to develop a new programming language for its Browser. Brendan Eich had developed the initial prototype in only ten days, and the rest is history. Software developers often ridiculed JavaScript in its early days because of its poor language design and lack of features.', 'Over the years, JavaScript has evolved into a multi-paradigm, high-level, dynamic programming language. The first significant breakthrough of JavaScript came in 2009 when Ryan Dahl has released cross-platform JavaScript runtime Node.js and enabled JavaScript to run on Server Side.', 'The other enormous breakthrough of JavaScript came around 2010 when Google has released a JavaScript-based Web development framework AngularJS.', 'Today, JavaScript is one of the most widely used programming languages globally and runs virtually everywhere: Browsers, Servers, Mobile Devices, Cloud, Containers, Microcontrollers.', '3 Key Features:', 'Popularity:', 'JavaScript is one of the most top-ranked programming languages because of its ubiquitous use on all platforms and mass adoption.', 'Octoverse has put JavaScript as the number one programming language for five consecutive years by GitHub repositories contributions:', 'Also, StackOverflow developer survey 2019 has ranked JavaScript as the most popular programming language and Technology:', 'Another programming language popularity site PYPL has ranked JavaScript as the 3rd most popular programming language:', 'The programming language popularity site TIOBE has ranked JavaScript as the 7th most popular programming language:', 'Once dreaded by the developers, JavaScript also ranked as the 11th most loved programming language according to StackOverflow Developer survey:', 'The trending of JavaScript is relatively stable, as shown by Google Trends:', 'Job Market:', 'In the USA Job market, Indeed has ranked JavaScript as the third most demanding programming language with 57 K Job posting in January 2020. With $114 K average yearly salary, JavaScript ranks 4th in terms of salary:', 'Also, StackOverflow developer survey has shown that JavaScript developers can earn a modest salary with relatively low experience:', 'Main Use Cases:', 'Java is one of the most disruptive programming languages to date. In the ’90s, business applications were mainly developed using C++, which was quite complicated and platform dependent. James Gosling and his team in Sun lowered the barrier to developing business applications by offering a much simpler, object-oriented, interpreted programming language that supports Multi-threading programming.', 'Java has achieved Platform independence by developing Java Virtual Machine (JVM), which abstracted the low-level Operating System from developers and gave the first “Write Once, Run anywhere” programming language. Also, JVM offered generation garbage collection, which manages the Object life cycle.', 'In recent years, Java has lost some of its markets to highly developer-friendly modern languages and the rise of other languages, especially Python, JavaScript. Also, JVM is not quite Cloud friendly because of its bulky size. Oracle has recently introduced hefty licensing fees for JDK, which will dent Java’s popularity further.', 'Fortunately, Java is working on its shortcomings and making Java fit for Cloud via the GraalVM initiative. Also, in OpenJDK, there is a free alternative to the proprietary Oracle JDK.', 'Java is still the number one programming language for enterprises.', '3 Key Features:', 'Popularity:', 'After five years of its release, Java became the 3rd most popular programming language and always remained in the top 3 lists in the next two decades. Here is the long-term history of Java in the popular TIOBE ranking:', 'Java’s popularity has waned in the last few years, but it is still the most popular programming language, according to TIOBE, as shown below:', 'According to the GitHub repository contribution, Java was in the number one spot during the 2014–2018 and only slipped to number 3rd position in last year:', 'The other popular programming language ranking website PYPL has ranked Java as 2nd most popular programming language:', 'StackOverflow developer survey also ranked Java high and only superseded by JavaScript and Python programming languages:', 'According to Google trends, Java is losing its traction constantly in the past five years:', 'Job Market:', 'According to Indeed, Java is the second most demanding programming language in the USA, with 69 K Job posting in January 2020. Also, Java developers earn the 6th highest annual salary ($104 K):', 'As per StackOverflow Developers survey 2019, Java offers a modest salary after few years of experience:', 'Main Use Cases:', 'In 2000, Tech giant Microsoft decided to create their Object Oriented C like programming language C# as part of their .NET initiative, which will be managed (run on a Virtual Machine like Java). The veteran language designer Anders Hejlsberg designed C# as part of Microsoft’s Common Language Initiative (CLI) platform where many other (mainly Microsoft’s languages) compiled into an intermediate format which runs on a Runtime named Common Language Runtime (CLR).', 'During the early days, C# was criticized as an imitation of Java. But later, both of the languages diverged. Also, Microsoft’s licensing of C# compiler/runtime is not always clear. Although Microsoft is currently not enforcing its patents under the Microsoft Open Specification Project, it may change.', 'Today, C# is a multi-paradigm programming language widely used on the Windows platform and the iOS/Android platform (thanks to Xamarin), and the Linux platform.', '3 Key Features:', 'Popularity:', 'The popular language ranking site TIOBE has ranked 5th in January 2020 with huge gain:', 'Also, Octoverse has listed C# as the 5th popular programming language by GitHub repositories contribution:', 'StackOverflow developer survey has placed C# as the 4th most popular language (7th most popular Technology for 2019:', 'It is interesting to note that the StackOverflow developer survey has ranked C# as the 10th most loved programming language (well above Java):', 'As clear from Google trends, C# is not being much hyped in the last few years, as shown below:', 'Job Market:', 'Indeed has posted 32 K openings for C# developers in the USA, making C# the 5th most demanding programming language in this list. With an annual salary of $96 K, C# ranks 8th in this list:', 'StackOverflow developer survey has placed C# above Java (albeit with more experience) in terms of global average salary:', 'Main Use Cases:', 'During the 1960s and 1970s, every cycle of the CPU and every byte of memory was expensive. Dennis Ritchie, a Bell lab engineer, has developed a procedural, general-purpose programming language compiled directly to machine language during 1969–1973. C programming offers low-level memory access and gives full control over the underlying hardware.', 'Over the years, C became one of the most used programming languages. Besides, C is arguably the most disruptive and influential programming language in history and has influenced almost all other languages on this list.', 'Although C is often criticized for its accidental complexity, unsafe programming, and lack of features. Also, C is platform-dependent, i.e., C code is not portable. But if you want to make the most use of your hardware, then C/C++ or Rust is your only option.', '3 Key Features:', 'Popularity:', 'C is the oldest programming language on this list and has dominated the industry for 47 years. C has also ruled the programming language popularity ranking more than any other language, as clear from TIOBE’s long-term ranking history:', 'According to TIOBE ranking, C is the second most popular language with a huge popularity gain in 2019:', 'Octoverse has also ranked C as the 9th most popular language according to the GitHub repository contribution:', 'StackOverflow developer survey has also ranked C in 12th (8th considering programming language) place:', 'Google trending also shows a relatively stable interest in C over the last five years.', 'Job Market:', 'According to Indeed, there are 28K job postings for C developers in the USA, making C the 6th most demanding programming language. In terms of salary, C ranks 6th with Java ($104 K):', 'StackOverflow developer survey showed C developers can earn an average wage but needs a longer time to achieve that compared to, e.g., Java, Python:', 'Main Use Cases:', 'Bjarne Stroustrup has worked with Dennis Ritchie (creator of C) in Bell Lab during the 1970s. Heavily influenced by C, he first created C++ as an extension of C, adding Object-Oriented features. Over time, C++ has evolved into a multi-paradigm, general-purpose programming language. Like C, C++ also offers low-level memory access and is directly compiled to machine instructions.', 'C++ also offers full control over hardware but with the cost of accidental complexity and does not provide language-level support for memory safety and concurrency safety. Also, C++ offers too many features and is one of the most complicated programming languages to master.', 'For all these factors and its platform dependency, C++ has lost its popularity to Java in especially enterprise software development and Big Data domain in the early 2000s.', 'C++ is gaining popularity with the rise of GPU, Containerization, Cloud computing, as it can quickly adapt itself to take advantage of Hardware or Ecosystem changes.', 'Today, C++ is one of the most important and heavily used programming languages in the industry.', '3 Key Features:', 'Popularity:', 'C++ is the second oldest programming language in this list and ranked 4th in the TIOBE programming language ranking:', 'Octoverse has ranked C++ in 6th position by GitHub repository contributions:', 'Also, StackOverflow Developer Survey in 2019 has listed C++ as the 9th most popular Technology (6th most popular language):', 'Although C++ is facing massive competition from modern programming languages like Rust or Go, it is still generating stable interest in the last five years:', 'Job Market:', 'Indeed has ranked C++ as the 4th most demanding programming language with 41 K job posting. Also, C++ developers earn $108 K per annum, which places it in 5th place:', 'StackOverflow developer survey has shown that C++ developers can draw a higher salary compared to Java, albeit with a longer experience:', 'Main Use Cases:', 'Like Python, PHP is another programming language developed by a single developer as a side project during the ’90s. Software Engineer Rasmus Lerdorf has initially created PHP as a set of Common Gateway Interface binaries written in C to create dynamic Web Applications. Later, more functionalities were added to the PHP product, and it organically evolved into a fully-fledged programming language.', 'At present, PHP is a general-purpose, dynamic programming language mainly used to develop server-side Web applications.', 'PHP is losing its appeal and popularity with the rise of JavaScript-based client-side Web application development, and PHP is past its prime. Contrary to popular belief, PHP will not die soon, although its popularity will gradually diminish.', '3 Key Features:', 'Popularity:', 'The programming language ranking site TIOBE has ranked PHP as the 8th most popular programming language in January 2020. Although the long term ranking history of PHP shows that PHP is past its prime and slowly losing its appeal:', 'Octoverse has ranked PHP as the 4th most popular programming language by GitHub repositories contribution:', 'As per StackOverflow developer survey 2019, PHP is the 5th most popular programming language (8th most popular Technology):', 'Although PHP is still one of the most widely used programming languages, it’s trending is slowly going down, as clear from Google Trends:', 'Job Market:', 'Job Search site Indeed has ranked PHP as the 7th most demanding programming language in the USA job market with 18 K positions in January 2020. Also, PHP developers can expect a reasonable salary ($90 K), which places them in 10th position in this category:', 'StackOverflow developer survey shows PHP as the lowest-paid programming language in 2019:', 'Main Use Cases:', 'Swift is one of the only two programming languages that has also appeared in my list: “Top 7 modern programming languages to learn now”. A group of Apple engineers led by Chris Lattner has developed a new programming language Swift mainly to replace Objective-C in the Mac and iOS platforms.', 'It is a multi-paradigm, general-purpose, compiled programming language that also offers high developer productivity. Swift supports LLVM (developer by Chris Lattner) compiler toolchain like C/C++, Rust.', 'Swift has excellent interoperability with Objective-C codebase and has already established itself as the primary programming language in iOS App development. As a compiled and powerful language, Swift is gaining increasing popularity in other domains as well.', '3 Main Features:', 'Popularity:', 'Like other modern programming languages, Swift is hugely popular among developers and ranked 6th in the list of most beloved languages:', 'Swift also has propelled to top 10 lists of most popular programming languages in TIOBE index only in 5 years of its first stable release:', 'Another popular programming language ranking site PYPL has ranked Swift as 9th most popular programming language:', 'StackOverflow developer survey has ranked Swift as the 15th most popular Technology (12th most popular programming language):', 'Google trends also show a sharp rise in the Popularity of Swift:', 'Job Market:', 'Indeed has ranked Swift as the 9th most demanding language in the USA with 6 K openings. In terms of Salary, Indeed has ranked Swift in 2nd place with a $125 K yearly salary:', 'StackOverflow developer survey has also revealed that Swift developer can earn a high salary with relatively fewer years of experience compared to Objective-C:', 'Main Use Cases:', 'Like Swift, Go is only the second programming language from the last decade in this list. Also, like Swift, Go is created by a Tech giant.', 'In the last decade, Google has frustratingly discovered that existing programming languages cannot take the seemingly unlimited hardware, human resources of Google. For example, compiling the C++ codebase of Google took half an hour. Also, they wanted to tackle the development scaling issue in the new language.', 'Renowned Software Engineers Rob Pike (UTF-8) and Ken Thompson (UNIX OS) in Google has created a new, pragmatic, easy-to-learn, highly scalable system programming language Go and released in 2012. Go has a runtime and Garbage collector (a few Megabytes), but this runtime is packed in the generated executable.', 'Although Go is a bit feature anemic, it has become a mainstream programming language quickly.', '3 Key Features:', 'Popularity:', 'Like Swift, Go has also seen a meteoric rise in popularity.', 'In almost all popular programming languages comparing websites, Go ranks high and has surpassed many existing languages. Here is the TIOBE index ranking from January 2020, where Go ranks 14th:', 'StackOverflow developer survey 2019 has also ranked Go as the 13th most popular Technology (10th most popular programming language):', 'According to the Stackoverflow survey, Go is the 9th most loved programming languages:', 'Go is also one of the top 10 fastest growing languages, according to GitHub Octoverse:', 'The increasing popularity of Go is also reflected in Google trends, which show increasing traction for Go over the last five years:', 'Job Market:', 'Indeed has ranked Go as the 10th most demanding language with 4 K openings in January 2020. In terms of salary, Go is ranked in 9th position:', 'StackOverflow developer survey 2019 has shown Go as one of the highest-paid programming languages:', 'Main Use Cases:', 'Ruby is the third programming language in this list developed by an individual developer during the 1990s. Japanese computer scientist Yukihiro Matsumoto has created Ruby as an “Object-Oriented Scripting language” and was released in 1995.', 'Ruby has later evolved into an interpreted, dynamically typed, high-level, multiple-paradigm general-purpose programming language. Ruby is implemented in C and offers garbage collection.', 'Like Python, Ruby focused heavily on developer productivity and developer happiness. Although Ruby is not one of the hyped languages, it is an excellent language for new developers for a flat learning curve.', '3 Key Features:', 'Popularity:', 'TIOBE ranked Ruby as the 11th most popular programming language in January 2020 with a hugely positive move:', 'Source: TIOBE', 'Octoverse has also ranked Ruby as the 10th most popular programming language in 2019 by GitHub repositories contributions:', 'StackOverflow Developer survey 2019 has listed Ruby as the 9th most popular programming language (12th most popular Technology):', 'Ruby has not been a hyped language in recent years but has maintained its traction as per Google trends:', 'Job Market:', 'In the USA job market, Ruby developers can draw huge salaries and ranked 1st by Indeed. Also, Indeed posted 16 K openings for Ruby developers in January 2020, which put Ruby 8th most demanding programming language in this list.', 'StackOverflow developer survey 2019 has also shown that Ruby developers can earn a high salary with relatively low experience:']"
02/2020,8 Common Data Structures every Programmer must know,Data Structures are a specialized means of…,6.2K,15,https://towardsdatascience.com/@vijini,https://towardsdatascience.com/8-common-data-structures-every-programmer-must-know-171acf6a1a42?source=collection_archive---------1-----------------------,11,11,"['8 Common Data Structures every Programmer must know', '1. Arrays', '2. Linked Lists', '3. Stacks', '4. Queues', '5. Hash Tables', '6. Trees', '7. Heaps', '8. Graphs', 'Final Thoughts', 'References']",47,"['Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. Data structures have a wide and diverse scope of usage across the fields of Computer Science and Software Engineering.', 'Data structures are being used in almost every program or software system that has been developed. Moreover, data structures come under the fundamentals of Computer Science and Software Engineering. It is a key topic when it comes to Software Engineering interview questions. Hence as developers, we must have good knowledge about data structures.', 'In this article, I will be briefly explaining 8 commonly used data structures every programmer must know.', 'An array is a structure of fixed-size, which can hold items of the same data type. It can be an array of integers, an array of floating-point numbers, an array of strings or even an array of arrays (such as 2-dimensional arrays). Arrays are indexed, meaning that random access is possible.', 'Inserting elements to an array and deleting elements from an array cannot be done straight away as arrays are fixed in size. If you want to insert an element to an array, first you will have to create a new array with increased size (current size + 1), copy the existing elements and add the new element. The same goes for the deletion with a new array of reduced size.', 'A linked list is a sequential structure that consists of a sequence of items in linear order which are linked to each other. Hence, you have to access data sequentially and random access is not possible. Linked lists provide a simple and flexible representation of dynamic sets.', 'Let’s consider the following terms regarding linked lists. You can get a clear idea by referring to Figure 2.', 'Following are the various types of linked lists available.', 'A stack is a LIFO (Last In First Out — the element placed at last can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as “stack” because it resembles a real-world stack — a stack of plates.', 'Given below are the 2 basic operations that can be performed on a stack. Please refer to Figure 3 to get a better understanding of the stack operations.', 'Furthermore, the following additional functions are provided for a stack in order to check its status.', 'A queue is a FIFO (First In First Out — the element placed at first can be accessed at first) structure which can be commonly found in many programming languages. This structure is named as “queue” because it resembles a real-world queue — people waiting in a queue.', 'Given below are the 2 basic operations that can be performed on a queue. Please refer to Figure 4 to get a better understanding of the queue operations.', 'A Hash Table is a data structure that stores values which have keys associated with each of them. Furthermore, it supports lookup efficiently if we know the key associated with the value. Hence it is very efficient in inserting and searching, irrespective of the size of the data.', 'Direct Addressing uses the one-to-one mapping between the values and keys when storing in a table. However, there is a problem with this approach when there is a large number of key-value pairs. The table will be huge with so many records and may be impractical or even impossible to be stored, given the memory available on a typical computer. To avoid this issue we use hash tables.', 'A special function named as the hash function (h) is used to overcome the aforementioned problem in direct addressing.', 'In direct accessing, a value with key k is stored in the slot k. Using the hash function, we calculate the index of the table (slot) to which each value goes. The value calculated using the hash function for a given key is called the hash value which indicates the index of the table to which the value is mapped.', 'h(k) = k % m', 'Consider the hash function h(k) = k % 20, where the size of the hash table is 20. Given a set of keys, we want to calculate the hash value of each to determine the index where it should go in the hash table. Consider we have the following keys, the hash and the hash table index.', 'From the last two examples given above, we can see that collision can arise when the hash function generates the same index for more than one key. We can resolve collisions by selecting a suitable hash function h and use techniques such as chaining and open addressing.', 'A tree is a hierarchical structure where data is organized hierarchically and are linked together. This structure is different than a linked list whereas, in a linked list, items are linked in a linear order.', 'Various types of trees have been developed throughout the past decades, in order to suit certain applications and meet certain constraints. Some examples are binary search tree, B tree, treap, red-black tree, splay tree, AVL tree and n-ary tree.', 'A binary search tree (BST), as the name suggests, is a binary tree where data is organized in a hierarchical structure. This data structure stores values in sorted order.', 'Every node in a binary search tree comprises the following attributes.', 'A binary search tree exhibits a unique property that distinguishes it from other trees. This property is known as the binary-search-tree property.', 'Let x be a node in a binary search tree.', 'Check my articles below on 8 useful tree data structures and self-balancing binary search trees.', 'A Heap is a special case of a binary tree where the parent nodes are compared to their children with their values and are arranged accordingly.', 'Let us see how we can represent heaps. Heaps can be represented using trees as well as arrays. Figures 7 and 8 show how we can represent a binary heap using a binary tree and an array.', 'Heaps can be of 2 types.', 'Check my article below on implementing a heap using the python heapq module.', 'A graph consists of a finite set of vertices or nodes and a set of edges connecting these vertices.', 'The order of a graph is the number of vertices in the graph. The size of a graph is the number of edges in the graph.', 'Two nodes are said to be adjacent if they are connected to each other by the same edge.', 'A graph G is said to be a directed graph if all its edges have a direction indicating what is the start vertex and what is the end vertex.', 'We say that (u, v) is incident from or leaves vertex u and is incident to or enters vertex v.', 'Self-loops: Edges from a vertex to itself.', 'A graph G is said to be an undirected graph if all its edges have no direction. It can go in both ways between the two vertices.', 'If a vertex is not connected to any other node in the graph, it is said to be isolated.', 'You can read more about graph algorithms from my article 10 Graph Algorithms Visually Explained.', 'A cheat sheet for the time complexities of the data structure operations can be found in this link. Moreover, check out my article below where I have implemented a few common data structures from scratch using C++.', 'Finally, I would like to thank Mr. A Alkaff Ahamed for providing valuable feedback and suggestions to improve this article.', 'I hope you found this article useful as a simple introduction to data structures. I would love to hear your thoughts. 😇', 'Thanks a lot for reading. 😊', 'Cheers! 😃', '[1] Introduction to Algorithms, Third Edition By Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein.', '[2] List of data structures from Wikipedia (https://en.wikipedia.org/wiki/List_of_data_structures)']"
02/2020,Could Nim Replace Python?,Why this fledgeling computer language might become the new scripting king,1.8K,9,https://towardsdatascience.com/@emmettgb,https://towardsdatascience.com/could-nim-replace-python-547145afcfd5?source=collection_archive---------2-----------------------,6,9,"['Could Nim Replace Python?', 'Introduction', 'Python', 'Introducing: Nim', 'Compiled Executables', 'Universal', 'Features', 'Speed', 'Conclusion']",28,"['For many years now, no programming language has been better suited for scripting than Python. Python is an interpreted language written in the late 1980s by Guido Van Rossum written in the language C. Van Rossum, like many other famous Computer Scientists, is from the Netherlands, where he wrote Python from within Centrum Wiskunde & Informatica, or in a rough English translation,', 'National Research Institute for Mathematics and Computer Science', 'Python has massive benefits of traditionally lower-level languages that populated computers at the time, such as C, FORTRAN, BASIC, C++, and Lisp. Firstly, Python’s syntax is far more simple and easy to grasp. This makes Python perfectly usable by end-users to perform required tasks around their system. Additionally, though Python is widely considered “ slow” by today’s standards, in 1989 it was quite a feat to have a language that reads like English be even remotely as performance packing to its competitors.', 'Even more recently,', 'Python and the jobs that correspond with its popularity have overtaken “ the big boys” with Python becoming more popular than both Java, as well as Javascript. Python’s ecosystem is absolutely exceptional, not only in regards to programming in general but also very much in regard to the ever-expanding market of', 'Machine-Learning', 'With Python’s growth in popularity, the flaws and problems with the languages have become more and more prevalent. Though Python is certainly a great language, it’s speed, which I touched on earlier is its', '“ Achilles Heel”', 'Though this isn’t to say that Python is not still viable for machine-learning, there certainly are some performance benefits to be had with a change from the most popular statistical language on Earth. This is also not necessarily Python’s fault, as the language has been developed for machine-learning only because of its sheer popularity. For most applications, Python works marvelously; but this can often change whenever many observations are loaded. For this reason, Scala is typically selected as the language for driving big-data inside of corporate America. Scala is not without its issues as well, although Scala certainly has a better ecosystem than the other two competitors,', 'R and Julia', 'Despite the similarities between both Nim and Python, Nim is significantly younger than Python, being rendered just 12 years ago in 2008. To this arises some significant advantages to Nim. Firstly, Nim is faster than Python while still being interpreted by the same language, C. Though Nim is technically run with an interpreter, it should be noted that Nim also has its own compiler. That being said, there are lots of cool features that make Nim a great potential replacement for Python that you might not have expected.', 'A common theme with Python is requiring Python in order to run Python, and this includes an application’s dependencies. This is problematic because it means that Python applications to be packages in one way or another with said dependencies. On top of that, it’s very likely that virtual environments will be frequented. While this isn’t terrible, and to confess most statistical languages do exactly the same, Nim does this significantly better by packaging an executable with the included dependencies needed to run. This not only makes managing dependencies from system to system a breeze, but also makes deployment', 'EASIER than Py (see what I did there?)', 'These compiled executables are also compatible universally across the Unix-like systems, Linux, Mac, and Berkley Software Distribution, but also the Windows NT kernel. Compiled executables take care of dependency issues and make it incredibly easy to publish an application, or even deploy an API with a simple “.” or “source” command.', 'Nim has a serious advantage to Python in that not only is Nim capable of being compiled in C, but also C++, and more excitingly:', 'Javascript', 'This means that not only does Nim have the potential to fill Python’s role as the scripting language that runs the data-based back-ends of the web, but Nim can also be used as a front-end similarly to Javascript. This is a huge benefit over Python. While Python is certainly great for deploying endpoints, and often does the job fine, having single-language fluidity across the board certainly has its advantages!', 'Nim’s code-base is primarily structured on the functional paradigm. This means that Nim can be a very expressive language, and furthermore can easily implement far more cool features than Python can. One of these features is one of my favorite features of all-time to be implemented into programming back in 1958 with the release of Lisp,', 'macros.', 'Macros and meta-programming have been around for nearly as long as computing itself, and can be very useful, especially on the grounds of machine-learning.', 'It’s no secret that as scale goes up, using Python for everything can be very problematic. This is because many training algorithms utilize a recursive Cost or Loss function that is intensive to run for any language. There are lots of languages and ideas with the intent to counter-act this, such as Julia, Python In Python (that’s a rabbit hole in and of itself), and more successfully: Cython.', 'With these solutions, however, come their own problems as well. Though Julia, which is, in fact, my favorite language, and likely the most apt to be a replacement for Python, does not have anywhere near the ecosystem that Python flaunts. Though there is a PyCall.jl, typically performance when using it dips below that of Python’s, and in that case,', 'why not just use Python?', 'Python in Python is an interesting concept, but has yet to see great implementations, as the concept itself is quite complex. Even worse, Python in Python is much more difficult to implement than a solution like Julia or Nim. As for Cython, contrary to popular belief, Cython does not work universally, and relying on it probably isn’t a good idea (been there, done that.)', 'Nim has the advantage of being faster than Python. For scripting, Nim’s added speed could certainly change the way that system-maintenance and various scripts are run. Using Nim might not be as fast as Julia, or C, but with the simple similarity to both Python and Bash that it boasts, it could certainly be a lot easier.', 'Though Nim is certainly a really cool, and even useful language, I highly doubt that a “ Python takeover” is imminent. Nim has a fraction of an ecosystem in comparison to Python, and furthermore would require a lot more work to get up to the grounds of Python, leaving it in somewhat of a limbo among the other programming languages. Though Nim certainly has features that outshine Python’s, it’s hard to argue with an established library of packages that everyone knows how to use.', 'I think Nim’s story is one similar to Julia. Nim is a beautiful language that is both expressive, easy, and efficient, but without a stable back-end, I consider it unlikely that Nim will ever become top-dog in the scripting and more importantly to me: statistical world. But all of this isn’t to say that Nim isn’t a valuable language to learn. Nim is a super cool, easy to use and fast high-level and functional programming language. Anything that combines all of those words is most likely beneficial to your programming toolbox.', 'Though Nim might not be the world’s next big thing, I’ve certainly been enjoying the time I’ve been spending with it. What’s more exciting to me is the potential that Julia shows, as opposed to Nim, and with a little more development, I think we could certainly increase the speed of ML as we know it. In this regard, the analogy of a processor certainly fits: in years past there was no processor with 64-bit registries, no processor with a clock speed above 1 GhZ (a total slug by today’s standards.) The future is exciting, and I simply can’t wait to see what technology stack it will include.']"
02/2020,Transition from Mechanical Engineer to Machine Learning Engineer (or Data Scientist),-,1.2K,3,https://towardsdatascience.com/@huan.do,https://towardsdatascience.com/transition-from-mechanical-engineer-to-machine-learning-engineer-or-data-scientist-786b9887537?source=collection_archive---------3-----------------------,13,4,"['Transition from Mechanical Engineer to Machine Learning Engineer (or Data Scientist)', 'Is Data Science really better than Mechanical Engineering?', 'How to do the transition?', 'Final words']",48,"['I have a Mechanical Engineering (ME) background as all of my degrees are in ME. After my Bachelor, I was doing my higher education in the field of Robotics when the Data revolution took shape. People were more familiar with the word “Big Data” at the time rather than Data Science. Then, I got hooked up with Machine Learning and started steering my career path towards Data Science since. I had a bit of good start with a Robotics background, especially in programming, so I didn’t have to start from scratch. Despite that, I encountered numerous obstacles trying to make the cut into the field of Data Science.', 'It was quite confusing to me at the time since I didn’t know where to start, what skills I need to sharpen, how I should form my resume, etc. If you are reading this and have a similar ME background, I hope you find this helpful.', 'Before we hop on the board of investing time and money for online courses or other irreversible actions, it might be good to take a moment and seriously question your motivations. If it is merely due to the sexiest job of the 21st century, it is very likely that when the enthusiasm passes out, you will be left with confusion and frustration after wasting your investments and being lost in the sail.', 'In my opinion, comparison between any two jobs are all irrelevant as any field has its pros and cons. However, as a person who has spent some times in both of the fields, I would like to point out a bit of characteristics of both, reflecting my personal opinions only.', 'Mechanical Engineering:', 'Data Science has nearly exact opposite characteristics.', 'Data Science:', 'I didn’t categorize above characteristics into pros and cons, since I think it highly depends on individuals. For instance, if you want a stable life and a routine work space, then the boredom of ME is not a cons at all.', 'Executing the transition takes a good amount of time and effort. As any kind of investment, you might want to consider how long the return of interest can be. For some person, the transition might be natural and take place over time, months, even years as in this article. For others, it might be fueled by a moment of epiphany and executed with a hustled plan. In my experience, it often take years if you are already in a full time job, and a bit shorter if you are in college. Mainly because you need free time to develop new skills. Again, it also greatly depends on what you already have, so this time frame is just for reference.', 'I really like the word “transition”. You transit from one state to another, not “hop to” or “start a new”. In other words, by realizing what you have with an ME background, and strategically developing the required new skills in Data Science, you will make the cut.', 'Since skill sets in Data Science are commonly categorized into 3 main cohorts: Math/Stats, Domain Knowledge, and Programming, I will structure this section accordingly, each with a “Re-usability Score”, that I think how hard we can re-use the ME background in the transition.', 'Re-usability Score: Easy.', 'If there is one thing that I can be the most confident about my ME background, it would be a rigorous and solid Math and Stats understandings. You might be surprised looking back to your academic transcript and count the number of Math-related courses that you’d taken. The challenge is that, whenever I have a conversation with my friends about learning Math in college, I get replied with “yeah yeah, I’m still waiting for the day I can use the Green’s theorem in my real life”. No, you won’t. But that’s not the purpose of studying Math. It gives you the “food for thoughts”. It gives you the fuel to burn when it comes to problem solving.', 'Let me give you an example. If I ask you to compute the first order derivative of this function y = x^2 + 1. Easy, it’s y = 2x. But how can you make any money out of that skill (apart from being a tutor for a high school student)?', 'Here is the trick, what matters is not with the numbers and Math formula, is with the intuition behind the Math. The first order derivative tells you the speed with which the function is increasing or decreasing. Hence, it’s around 0 in the region that a function is a constant, and non-zero elsewhere. Let’s see what we can do with that intuition.', 'Now, let’s say you are an engineer, and the manufacturing manager of an automobile manufacturing site complains with you that they are paying a good amount of money for a tester to count the number of screws on machines, before let them out of the factory. If you can automate that task, you get 20% of the margin, after decommissioning the tester role. Now we are talking real money.', 'You take a photo of the view from the tester. You observe that the screws have different colors than the machine body, so if there is a way to make it “pop out” in the photo, you can have a program to “count” them. Since you remember that we can use the first order derivative to find region where a function changes significantly, you realize that the edges of those screws should have a non-zero first order derivative.', 'You run a first order derivative filter of the image, and voila! the screws are as bright as stars.', 'Now you can develop a computer vision system to replace the (poor) tester and collect your share. All of that is based on the fundamental first order derivative intuition.', 'Another example: if I ask you to write down the Probability Chain Rule, it would be a piece of cake: P(A,B)=P(A|B)*P(B). But what of use is it?', 'Let’s say one day your senior manager asks this question: “How likely we have to pay $2000 to restore server XYZ this year? You know, if it goes down.” This is a valid and common business question that a Data Scientist is supposed to be able to answer. You would go back and define:', 'Thus, P(A|B) is the probability that you have to pay $2000 to fix the server XYZ, given it indeed breaks, and P(B) is the probability that the server breaks down. A quick and dirty way to determine those probabilities might be:', '(Please note that abnormality detection is a huge topic and this is just an over-simplified solution. But I think it’s enough to make the point.)', 'So, the final probability that your company has to pay $2000 to fix that server this year is 0.2 * 0.02 = 0.004 , or only 0.4%.', 'Anyone who took Probability & Statistics 101 can write the Chain Rule, but only being able to “translate” it to answer the business questions will grant you the job.', 'Re-usability Score: Medium.', 'Unlike other kind of engineerings, where the skills or the outcomes of the work of engineers directly generate products or services that the end users buy, Data Science doesn’t often directly create values. This is the reason why Domain Knowledge is essential. A Data Scientist needs to understand the business thoroughly before they can apply AI models to it.', 'Normally, I see AI models create profits by the following setups:', 'It might be obvious, but all those setups answer a single question:', 'How do you use the proposed AI model to make profits?', 'Either measurable by dollar counts, or other KPIs. When one first starts with Data Science, they might have a lot of concerns in their first project. It might be the novelty of the methods, model performance, computational complexity, degree of state-of-the-art, or model evaluation, etc. All of them do matter, for the whole solution to work. But they all should be guided by that single question. It might sound pragmatic, but it actually is. The question should have a satisfied answer at step 0, before anything happens.', 'This transition is a bit harder, since for a mechanical engineer, you don’t really have to care about how values are created. But what you possess, is the well-trained mindset of how to design and optimize processes: mechanism design, process control, thermodynamics, Otto cycle, Capstone design, etc. They are all processes! And to some extent, a business model is a process itself. There are lot more uncertainties, since humans get involved (humans are random), but you have been trained to model uncertainty in process too!', 'However, what you bring along (your process-oriented mindset) is just the vehicle, you still need the fuel to run. Have you ever wondered how the local food court near where you live makes money? or how the shirt you are wearing was made in a country half of the earth away, and got transported to you to buy? Just graph your ideas down onto a paper, and you will be surprised. Instead of “gas reservoir” (just a fancy thermodynamic jargon for a container of gas), you have “good inventory”; instead of “fluid conducting pipe”, you have “data extraction pipeline”; instead of “flow rate”, you have “data transfer rate”. The laws of physics are universal, and they are valid in the digital world as well.', 'Re-usability Score: for Data Scientist: Medium, for ML Engineer: Hard.', 'I differentiate between the two since in my opinion, ML Engineers are hardcore software engineers who can do modeling. Hence, the level of competency in programming needs to be extreme. This differentiation of course is relative, since certainly there are data scientists who write production code.', 'This is the part that takes most of the time to master. “Programming” is a bit poorly used here, since it doesn’t include the whole software engineering practices: networking, APIs, CI/CD, Dockerization, etc, you name it. Unfortunately, you need them all for the ML Engineer path (here I assume the context of Data Scientists is contained within analytics and visualization tasks, in other words, those who don’t have to write production code. Again, it’s subjective).', 'There is a myriad of online courses and great articles in TDS about how you should sharpen this skill set so I won’t be going through them again. There is only one point I would like to share: the great concept of encapsulation, and that I don’t mean just in OOP. This helps me greatly in learning.', 'You can learn almost any software engineering tool/skill in isolation, and a complete software application can be broken down to granular modules that you can learn each one separately. This is beautiful and I couldn’t do it in Mechanical Engineering. This leads to an extremely effective learning strategy: divide and conquer. I have numerous folders like: python-practice, spark-practice, Docker-practice, gRPC-practice, k8s-practice, etc. Each with no more than 5 files contains the most basic example code of the tool. Once you master a sufficient number of these “Lego blocks”, designing solution architect will be full of fun. You have the freedom to shape how your AI model will interact with other modules.', 'Note that it requires a great effort to bring all of the blocks together to have a functional application, otherwise, software engineers wouldn’t exist since people can just pick the blocks and an “universal” program will compile the desired application like you order in a McDonalds self-order kiosk. This is about an effective way that you could use to learn quickly.', 'There are two tools that I had to spend fairly amount of time to catch up with when stepping out from ME: git and SQL. It makes sense since in ME, you don’t have to mange code and you don’t have to manage relational databases. For me at the time, Dropbox was good enough to share code, folders followed by prefix such as “backup_20080202” were good enough for version control, and Excel was okay to keep tables. Life was easy and simple. So I have to up the game with git and SQL.', 'git: I had always thought that I am fluent in git by knowing some git command lines, until I needed to merge code with my colleagues. Hence, the best practice I would recommend is to do some pair programming projects with your friends: course projects, hobby projects, or develop a small feature in pair with your colleague. The sooner you do this, the sooner you realize that git is not just about version control, and the better off you will be.', 'SQL: In my opinion, it doesn’t matter what version of SQL you are using, as long as you understand all the different kinds of joins in SQL and know how to check the outcome, you are good. Why? Because syntax errors are easy to spot, the platform will say like “Error: can not convert a string type to integer.”, your query will fail, you will know. But the joins are silent killers. If you use a wrong type of join, or you trust your query outcome without double checking, chances are you will only find out when your manager calls you in and questions your “ridiculous” charts. For instance, if you join a sale table with some other product info table without checking for duplicates, your joined table will have duplicated rows. In other words, one sold product might appear more than one time, and in your visualization, the sale volume suggests that your company is a Fortune 500 rather than a startup. No one would take your presentation serious.', 'In short, we have discussed:', 'These are the things that helped me a great deal along my journey, and I hope they are useful for you as well.', 'Working in this field of Data is rewarding in many ways, but for me, it is the joy I have when a model is deployed in production and watch it goes, just like when I built a robot and saw the whole thing runs for the first time.', 'We are engineers after all:', '“Do machine learning like the great engineer you are, not like the great machine learning expert you aren’t.”', 'Happy (Machine) Learning!']"
02/2020,40 Statistics Interview Problems and Answers for Data Scientists,A resource to brush up your…,1.4K,6,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/40-statistics-interview-problems-and-answers-for-data-scientists-6971a02b7eee?source=collection_archive---------4-----------------------,19,43,"['40 Statistics Interview Problems and Answers for Data Scientists', '1. How do you assess the statistical significance of an insight?', '2. Explain what a long-tailed distribution is and provide three examples of relevant phenomena that have long tails. Why are they important in classification and regression problems?', '3. What is the Central Limit Theorem? Explain it. Why is it important?', '4. What is the statistical power?', '5. Explain selection bias (with regard to a dataset, not variable selection). Why is it important? How can data management procedures such as missing data handling make it worse?', '6. Provide a simple example of how an experimental design can help answer a question about behavior. How does experimental data contrast with observational data?', '7. Is mean imputation of missing data acceptable practice? Why or why not?', '8. What is an outlier? Explain how you might screen for outliers and what would you do if you found them in your dataset. Also, explain what an inlier is and how you might screen for them and what would you do if you found them in your dataset.', '9. How do you handle missing data? What imputation techniques do you recommend?', '10. You have data on the duration of calls to a call center. Generate a plan for how you would code and analyze these data. Explain a plausible scenario for what the distribution of these durations might look like. How could you test, even graphically, whether your expectations are borne out?', '11. Explain likely differences between administrative datasets and datasets gathered from experimental studies. What are likely problems encountered with administrative data? How do experimental methods help alleviate these problems? What problem do they bring?', '12. You are compiling a report for user content uploaded every month and notice a spike in uploads in October. In particular, a spike in picture uploads. What might you think is the cause of this, and how would you test it?', '13. You’re about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it’s raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that “Yes” it is raining. What is the probability that it’s actually raining in Seattle?', '14. There’s one box — has 12 black and 12 red cards, 2nd box has 24 black and 24 red; if you want to draw 2 cards at random from one of the 2 boxes, which box has the higher probability of getting the same color? Can you tell intuitively why the 2nd box has a higher probability', '15. What is: lift, KPI, robustness, model fitting, design of experiments, 80/20 rule?', '16. Define quality assurance, six sigma.', '17. Give examples of data that does not have a Gaussian distribution, nor log-normal.', '18. What is root cause analysis? How to identify a cause vs. a correlation? Give examples', '19. Give an example where the median is a better measure than the mean', '20. Given two fair dices, what is the probability of getting scores that sum to 4? to 8?', '21. What is the Law of Large Numbers?', '22. How do you calculate the needed sample size?', '23. When you sample, what bias are you inflicting?', '24. How do you control for biases?', '25. What are confounding variables?', '26. What is A/B testing?', '27. Infection rates at a hospital above a 1 infection per 100 person-days at risk are considered high. A hospital had 10 infections over the last 1787 person-days at risk. Give the p-value of the correct one-sided test of whether the hospital is below the standard.', '28. You roll a biased coin (p(head)=0.8) five times. What’s the probability of getting three or more heads?', '29. A random variable X is normal with mean 1020 and a standard deviation 50. Calculate P(X>1200)', '30. Consider the number of people that show up at a bus station is Poisson with mean 2.5/h. What is the probability that at most three people show up in a four hour period?', '31. An HIV test has a sensitivity of 99.7% and a specificity of 98.5%. A subject from a population of prevalence 0.1% receives a positive test result. What is the precision of the test (i.e the probability he is HIV positive)?', '32. You are running for office and your pollster polled hundred people. Sixty of them claimed they will vote for you. Can you relax?', '34. The homicide rate in Scotland fell last year to 99 from 115 the year before. Is this reported change really noteworthy?', '35. Consider influenza epidemics for two-parent heterosexual families. Suppose that the probability is 17% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12% while the probability that both the mother and father have contracted the disease is 6%. What is the probability that the mother has contracted influenza?', '36. Suppose that diastolic blood pressures (DBPs) for men aged 35–44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. About what is the probability that a random 35–44 year old has a DBP less than 70?', '37. In a population of interest, a sample of 9 men yielded a sample average brain volume of 1,100cc and a standard deviation of 30cc. What is a 95% Student’s T confidence interval for the mean brain volume in this new population?', '38. A diet pill is given to 9 subjects over six weeks. The average difference in weight (follow up — baseline) is -2 pounds. What would the standard deviation of the difference in weight have to be for the upper endpoint of the 95% T confidence interval to touch 0?', '39. In a study of emergency room waiting times, investigators consider a new and the standard triage systems. To test the systems, administrators selected 20 nights and randomly assigned the new triage system to be used on 10 nights and the standard system on the remaining 10 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 3 hours with a variance of 0.60 while the average MWT for the old system was 5 hours with a variance of 0.68. Consider the 95% confidence interval estimate for the differences of the mean MWT associated with the new system. Assume a constant variance. What is the interval? Subtract in this order (New System — Old System).', '40. To further test the hospital triage system, administrators selected 200 nights and randomly assigned a new triage system to be used on 100 nights and a standard system on the remaining 100 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 4 hours with a standard deviation of 0.5 hours while the average MWT for the old system was 6 hours with a standard deviation of 2 hours. Consider the hypothesis of a decrease in the mean MWT associated with the new treatment. What does the 95% independent group confidence interval with unequal variances suggest vis a vis this hypothesis? (Because there’s so many observations per group, just use the Z quantile instead of the T.)', 'References', 'Thanks for Reading!', 'More Relevant Articles']",110,"['If you enjoy this, sign up for my email list here!', 'Given the popularity of my articles, Google’s Data Science Interview Brain Teasers, Amazon’s Data Scientist Interview Practice Problems, Microsoft Data Science Interview Questions and Answers, and 5 Common SQL Interview Problems for Data Scientists, I collected a number of statistics data science interview questions on the web and answered them to the best of my ability.', 'The more that I’ve learned about data science, the more I’ve realized that fundamental statistics knowledge is essential to be successful. So, I crawled the web and found forty statistics interview questions for data scientists that I will be answering. Here we go!', 'You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null — in other words, the result is statistically significant.', 'Example of a long tail distribution', 'A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.', '3 practical examples include the power law, the Pareto principle (more commonly known as the 80–20 rule), and product sales (i.e. best selling products vs others).', 'It’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.', 'Statistics How To provides the best definition of CLT, which is:', '“The central limit theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.” [1]', 'The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.', '‘Statistical power’ refers to the power of a binary hypothesis, which is the probability that the test rejects the null hypothesis given that the alternative hypothesis is true. [2]', 'Selection bias is the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.', 'Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.', 'Types of selection bias include:', 'Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that you’re assuming that the data is not as spread out as it might actually be.', 'Observational data comes from observational studies which are when you observe certain variables and try to determine if there is any correlation.', 'Experimental data comes from experimental studies which are when you control certain variables and hold them constant to determine if there is any causality.', 'An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep.', 'Mean imputation is the practice of replacing null values in a data set with the mean of the data.', 'Mean imputation is generally bad practice because it doesn’t take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.', 'Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.', 'An outlier is a data point that differs significantly from other observations.', 'Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it’s important to remove them from the dataset. There are a couple of ways to identify outliers:', 'Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it’s equal to +/- 3, then it’s an outlier.Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score.', 'Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1–1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.', 'Photo from Michael Galarnyk', 'Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests.', 'An inlier is a data observation that lies within the rest of the dataset and is unusual or an error. Since it lies in the dataset, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the dataset to address them.', 'There are several ways to handle missing data:', 'The best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there’s a lot of data to start with and the percentage of missing values is low.', 'First I would conduct EDA — Exploratory Data Analysis to clean, explore, and understand my data. See my article on EDA here. As part of my EDA, I could compose a histogram of the duration of calls to see the underlying distribution.', 'My guess is that the duration of calls would follow a lognormal distribution (see below). The reason that I believe it’s positively skewed is because the lower end is limited to 0 since a call can’t be negative seconds. However, on the upper end, it’s likely for there to be a small proportion of calls that are extremely long relatively.', 'Lognormal Distribution Example', 'You could use a QQ plot to confirm whether the duration of calls follows a lognormal distribution or not. See here to learn more about QQ plots.', 'Administrative datasets are typically datasets used by governments or other organizations for non-statistical reasons.', 'Administrative datasets are usually larger and more cost-efficient than experimental studies. They are also regularly updated assuming that the organization associated with the administrative dataset is active and functioning. At the same time, administrative datasets may not capture all of the data that one may want and may not be in the desired format either. It is also prone to quality issues and missing entries.', 'There are a number of potential reasons for a spike in photo uploads:', 'The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause.', 'You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, “What is the probability A is true given B is true?” Therefore we need to know the probability of it raining in London on a given day. Let’s assume it’s 25%.', 'P(A) = probability of it raining = 25%P(B) = probability of all 3 friends say that it’s rainingP(A|B) probability that it’s raining given they’re telling that it is rainingP(B|A) probability that all 3 friends say that it’s raining given it’s raining = (2/3)³ = 8/27', 'Step 1: Solve for P(B)P(A|B) = P(B|A) * P(A) / P(B), can be rewritten asP(B) = P(B|A) * P(A) + P(B|not A) * P(not A)P(B) = (2/3)³ * 0.25 + (1/3)³ * 0.75 = 0.25*8/27 + 0.75*1/27', 'Step 2: Solve for P(A|B)P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)P(A|B) = 8 / (8 + 3) = 8/11', 'Therefore, if all three friends say that it’s raining, then there’s an 8/11 chance that it’s actually raining.', 'The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Let’s walk through each step.', 'Let’s say the first card you draw from each deck is a red Ace.', 'This means that in the deck with 12 reds and 12 blacks, there’s now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.', 'In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.', 'Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards.', 'Lift: lift is a measure of the performance of a targeting model measured against a random choice targeting model; in other words, lift tells you how much better your model is at predicting things than if you had no model.', 'KPI: stands for Key Performance Indicator, which is a measurable metric used to determine how well a company is achieving its business objectives. Eg. error rate.', 'Robustness: generally robustness refers to a system’s ability to handle variability and remain effective.', 'Model fitting: refers to how well a model fits a set of observations.', 'Design of experiments: also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. [4] In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).', '80/20 rule: also known as the Pareto principle; states that 80% of the effects come from 20% of the causes. Eg. 80% of sales come from 20% of customers.', 'Quality assurance: an activity or set of activities focused on maintaining a desired level of quality by minimizing mistakes and defects.', 'Six sigma: a specific type of quality assurance methodology composed of a set of techniques and tools for process improvement. A six sigma process is one in which 99.99966% of all outcomes are free of defects.', 'Root cause analysis: a method of problem-solving used for identifying the root cause(s) of a problem [5]', 'Correlation measures the relationship between two variables, range from -1 to 1. Causation is when a first event appears to have caused a second event. Causation essentially looks at direct relationships while correlation can look at both direct and indirect relationships.', 'Example: a higher crime rate is associated with higher sales in ice cream in Canada, aka they are positively correlated. However, this doesn’t mean that one causes another. Instead, it’s because both occur more when it’s warmer outside.', 'You can test for causation using hypothesis testing or A/B testing.', 'When there are a number of outliers that positively or negatively skew the data.', 'There are 4 combinations of rolling a 4 (1+3, 3+1, 2+2):P(rolling a 4) = 3/36 = 1/12', 'There are combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):P(rolling an 8) = 5/36', 'The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.', 'Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times.', 'You can use the margin of error (ME) formula to determine the desired sample size.', 'Potential biases include the following:', 'There are many things that you can do to control and minimize bias. Two common things include randomization, where participants are assigned by chance, and random sampling, sampling in which each member has an equal probability of being chosen.', 'A confounding variable, or a confounder, is a variable that influences both the dependent variable and the independent variable, causing a spurious association, a mathematical relationship in which two or more variables are associated but not causally related.', 'A/B testing is a form of hypothesis testing and two-sample hypothesis testing to compare two versions, the control and variant, of a single variable. It is commonly used to improve and optimize user experience and marketing.', 'Since we looking at the number of events (# of infections) occurring within a given timeframe, this is a Poisson distribution question.', 'Null (H0): 1 infection per person-daysAlternative (H1): >1 infection per person-days', 'k (actual) = 10 infectionslambda (theoretical) = (1/100)*1787p = 0.032372 or 3.2372% calculated using .poisson() in excel or ppois in R', 'Since p-value < alpha (assuming 5% level of significance), we reject the null and conclude that the hospital is below the standard.', 'Use the General Binomial Probability formula to answer this question:', 'p = 0.8n = 5k = 3,4,5', 'P(3 or more heads) = P(3 heads) + P(4 heads) + P(5 heads) = 0.94 or 94%', 'Using Excel…p =1-norm.dist(1200, 1020, 50, true)p= 0.000159', 'x = 3mean = 2.5*4 = 10', 'using Excel…', 'p = poisson.dist(3,10,true)p = 0.010336', 'Precision = Positive Predictive Value = PVPV = (0.001*0.997)/[(0.001*0.997)+((1–0.001)*(1–0.985))]PV = 0.0624 or 6.24%', 'See more about this equation here.', 'p-hat = 60/100 = 0.6z* = 1.96n = 100This gives us a confidence interval of [50.4,69.6]. Therefore, given a confidence interval of 95%, if you are okay with the worst scenario of tying then you can relax. Otherwise, you cannot relax until you got 61 out of 100 to claim yes.', 'Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy.', 'Using the General Addition Rule in probability:P(mother or father) = P(mother) + P(father) — P(mother and father)P(mother) = P(mother or father) + P(mother and father) — P(father)P(mother) = 0.17 + 0.06–0.12P(mother) = 0.11', 'Since 70 is one standard deviation below the mean, take the area of the Gaussian distribution to the left of one standard deviation.', '= 2.3 + 13.6 = 15.9%', 'Given a confidence level of 95% and degrees of freedom equal to 8, the t-score = 2.306', 'Confidence interval = 1100 +/- 2.306*(30/3)Confidence interval = [1076.94, 1123.06]', 'Upper bound = mean + t-score*(standard deviation/sqrt(sample size))0 = -2 + 2.306*(s/3)2 = 2.306 * s / 3s = 2.601903Therefore the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95% T confidence interval to touch 0.', 'See here for full tutorial on finding the Confidence Interval for Two Independent Samples.', 'Confidence Interval = mean +/- t-score * standard error (see above)', 'mean = new mean — old mean = 3–5 = -2', 't-score = 2.101 given df=18 (20–2) and confidence interval of 95%', 'standard error = sqrt((0.⁶²*9+0.⁶⁸²*9)/(10+10–2)) * sqrt(1/10+1/10)standard error = 0.352', 'confidence interval = [-2.75, -1.25]', 'Assuming we subtract in this order (New System — Old System):', 'confidence interval formula for two independent samples', 'mean = new mean — old mean = 4–6 = -2', 'z-score = 1.96 confidence interval of 95%', 'st. error = sqrt((0.⁵²*99+²²*99)/(100+100–2)) * sqrt(1/100+1/100)standard error = 0.205061lower bound = -2–1.96*0.205061 = -2.40192upper bound = -2+1.96*0.205061 = -1.59808', 'confidence interval = [-2.40192, -1.59808]', '[1] Central Limit Theorem, Definition and Examples in Easy Steps, Statistics How To', '[2] Power, Statistics, Wikipedia', '[3] Anthropic principle, Wikipedia', '[4] Design of experiments, Wikipedia', '[5] Root cause analysis, Wikipedia', 'If you like my work and want to support me…']"
02/2020,I built a DIY license plate reader with a Raspberry Pi and machine learning,Machine learning is…,3.6K,14,https://towardsdatascience.com/@robert.lucian.chiriac,https://towardsdatascience.com/i-built-a-diy-license-plate-reader-with-a-raspberry-pi-and-machine-learning-7e428d3c7401?source=collection_archive---------5-----------------------,17,12,"['I built a DIY license plate reader with a Raspberry Pi and machine learning', 'Step 1. Scoping the project', 'Step 2. Selecting the right models', 'Step 3. Designing the hardware', 'Step 4. Training the models', 'YOLOv3', 'CRAFT & CRNN', 'Step 5. Deploying my license plate detector models', 'Step 6. Developing the client', 'Results', 'Conclusions (and how 5G fits into all of this)', 'Resources']",62,"['A few months ago, I started entertaining the idea of giving my car the ability to detect and recognize objects. I mostly fancied this idea because I’ve seen what Teslas are capable of, and while I didn’t want to buy a Tesla right away (Model 3 is looking juicier with each passing day I gotta say), I thought I’d try meeting my dream halfway.', 'So, I did it.', 'Below, I’ve documented each step in the project. If you just want to see a video of the detector in action/the GitHub link, skip to the bottom.', 'I started by thinking of what such a system should be capable of. If there’s anything I learned during my life is starting small is always the best strategy: baby steps. So, apart from the obvious lane keeping task (which everyone has already done), I thought of just plainly identifying license plates as I was driving the car. This identifying process includes 2 steps:', 'I thought that if I could do this, then moving on to other tasks should be rather easy (things like determining collision risks, distances, etc). Maybe even create a vector space representation of the environment — that would be dope.', 'Before worrying too much about specifics, I know that I need:', 'To start, I tackled building the right model for object detection.', 'After careful research, I decided to go with the following machine-learning models:', 'How would this trio of models work together? Well, here’s the flow of operations:', 'With my basic model architecture sketched out, I could move onto the hardware.', 'Knowing that I need something low-powered made me think of my old love: the Raspberry Pi. It has enough compute power for preprocessing frames at a respectable framerate and it has the Pi Camera. The Pi Camera is the de-facto camera system for the Raspberry Pi. It has a really great library and it’s very mature.', 'As for internet access, I could just throw in an EC25-E for 4G access which also has a GPS module embedded in from one of my previous projects. Here’s the article that talks about this shield.', 'I decided to start with the enclosure. Hanging it on the car’s rear-view mirror should work well, so I ended up designing a two-component support structure:', 'These supports/enclosures would be printed with my trusty Prusa i3 MK3S 3D printer.', 'Figure 1 and Figure 2 show what the structures look like when they are rendered. Note that the C-Style holders are pluggable, so the enclosure for the Raspberry Pi and the suppport for the Pi Camera don’t come with the holders already printed. They have a socket into which the holders are plugged in. This is very useful if one of my readers decides to replicate the project. They only have to adapt the holders to work on their car’s rear-view mirror. Currently, the holders work well on my car: it’s a Land Rover Freelander.', 'Obviously, these took some time to be modeled — I needed a couple of iterations to get the structure sturdy. I used PETG material at a layer height of 200 microns. The PETG works well into the 80-90s (Celsius degrees) and is quite strong against UV radiation - not as good as ASA though, but strong.', 'This has been designed in SolidWorks and so all of my SLDPRT/SLDASM files alongside all STLs and gcodes can be found here. Use them to print your version as well.', 'Once I had the hardware, I moved on to training the models.', 'As expected, it’s better to not reinvent the wheel and reuse other people’s work as much as possible. That’s what transfer learning is all about — leveraging insights from other very large data sets. One very pertinent example of transfer learning is the one I read about in this article a couple of days ago. Somewhere in the middle it talks about a team affiliated with Harvard Medical School which was able to fine-tune a model to predict the “long-term mortality, including noncancer death, from chest radiographs”. They only had a small dataset of just 50,000 labeled images, but the pretrained model (Inception-v4) they used was trained on about 14 million images. It took them less than a fraction of what the original model took to train (both in time and money) and the accuracy they’ve achieved was pretty high nevertheless.', 'That’s what I also intended on doing.', 'I looked over the web for pretrained license plate models and there weren’t as many as I have initially expected, but I found one trained on about 3600 images with license plates. It’s not much, but it’s also more than nothing and in addition to that, it was also trained on Darknet’s pre-trained model. I could use that. Here’s the guy’s model.', 'And since I already had a hardware system that could record, I decided to use mine to drive around the town for a few hours and collect frames to finetune the above guy’s model.', 'I used VOTT to annotate the collected frames (with license plates of course). I ended up creating a small dataset of 534 images with labeled bounding boxes for license plates. Here’s the dataset.', 'I then found this Keras implementation of the YOLOv3 net. I used it to train my dataset and then PRed my model to this repo so that others can use it as well. The mAP I got on the test set is 90%, which is really well given how small my dataset is.', 'After countless tries to find a good kind of net to recognize text, I stumbled upon keras-ocr, which is a packaged and flexible version of CRAFT and CRNN. And it also comes with their pre-trained models. That’s fantastic. I decided to not fine-tune the models and leave them as is.', ""And above all, predicting text with keras-ocr is very simple. It's basically just a few lines of code. Checkout their homepage to see how it's done."", 'There are two big approaches to model deployment I can go with:', 'Both approaches come with their challenges. The first one implies having a big “brain” computer system and that is complex and expensive. The second presents challenges around latency and infrastructure, in particular, using GPUs for inference.', 'In my research, I stumbled upon an open source project called cortex. It’s quite new to the game, but it certainly does make sense to be the next step in the evolution of development tools for AI.', ""Basically, cortex is a platform for deploying machine learning models as production web services at the flick of a switch. What this means is that I can focus on my application and leave the rest to cortex to manage. In this case it does all the provisioning on AWS and the only thing I'd have to do is use template models to write my predictors. What is even more awesome is that I only have to write a few dozen lines for every model."", ""Here’s cortex in action in the terminal taken from their GitHub repo. If that ain't beautiful and simple, then I don't know what to call it then:"", ""Since this computer vision system isn’t used by an autopilot, latency isn’t as important for me, and I can go with cortex for this. If it were to be a part of an autopilot system, then using services provisioned through a cloud provider wouldn't have been such a good idea, at least not today."", 'Deploying the ML models with cortex is only a matter of:', 'Without getting into the gritty details of how I’ve done it (and to keep the article to a respectable length), here’s an example of a predictor for the classical iris dataset. The link to cortex’s implementation of these 2 APIs can be found on their repository here — all other resources for this project are present at the end of this article.', 'And then to make a prediction, you just use curl like this', 'And a prediction response would look like this ""setosa"". Very simple!', 'With cortex handling my deployments, I could move onto designing the client—which was the tricky part.', 'I thought of the following architecture:', 'Here’s a flowchart of the client in relation to the cloud APIs on AWS.', 'The client in our case is the Raspberry Pi and the cloud APIs to which the inference requests are sent to is provisioned by cortex on AWS (Amazon Web Services).', 'The source code for the client can also be inspected on its GitHub repository.', ""One particular challenge which I had to overcome was the bandwidth on 4G. It’s best to reduce the required bandwidth for this application to mitigate possible hangups or excessive use of the available data. I decided to go with a very low resolution on the Pi Camera: 480x270 (we can go with a small resolution because the field of view of the Pi Camera is very narrow so we can still easily identify license plates). Still, even at this resolution, the JPEG size of a frame is of about 100KB at 10MBits. Multiplying that by 30 frames per second gets us 3000KB which is about 24 Mb/s and that's without the HTTP overhead - that's a lot."", 'Instead, I did the following tricks:', 'The resulting frame has a size of about 7–10KB, which is exceptionally good. This translates to 2.8Mb/s. But with all the overhead, it’s about 3.5Mb/s (including the response as well).', ""For the crnn API, the cropped license plates don't take much at all, even without applying the compression trick. They are sitting at around 2-3KB a piece."", 'All in all, to run this at 30FPS, the required bandwidth for the inference APIs is around 6Mb/s. That’s a number I can live with.', 'It works!', 'The one above is a real-time example of running the inference through cortex. I needed about 20 GPU-equipped instances to be able to run this smoothly. Depending on the latency with the cluster, you may need more or fewer instances. The average latency between the moment a frame is captured and then broadcasted to a browser window is of about 0.9 seconds, which is so fantastic considering that the inference takes place some place far away - this still amazes me.', 'The text recognition part may not be the best, but it proves the point at least — it could be much more accurate by increasing the resolution of the video or by reducing the field of view of the camera or by fine-tuning it.', 'As for the high GPU count, this can be reduced with optimizations. For example, converting the models to use mixed/full half precision (FP16/BFP16). Making the models use mixed precision will have a minimal impact on the accuracy, generally speaking, so it’s not like we’re trading off much.', 'T4 and V100 GPUs have special tensor cores which are designed to be super fast on matrix multiplications on half precision types. The speedup of half precision over single precision operations on T4 is about 8x and on V100 is of 10x. That’s an order of magnitude difference. This means that a model that has been converted to use single/mixed precision can take up to 8 times less time to do an inference, and respectively a tenth of a time on a V100.', 'I haven’t converted the models to use single/mixed precision because this is out of the scope of this project. As far as I’m concerned, this is only an optimization problem. I’ll most likely do it the moment version 0.14 of cortex releases (with true multi-process support and queue-based autoscaling), so I can take advantage of the multi-processed web server as well.', 'All in all, if all optimizations are brought into place, reducing the cluster’s size from 20 GPU-equipped instances down to just one is actually feasible. If properly optimized, not even maxing out a single GPU-equipped instance gets possible.', 'To make costs even more acceptable, using Elastic Inference on AWS could reduce these by up to 75%, which is a lot! You could have a pipeline for processing a stream in real time for a dime, figuratively speaking. Unfortunately, at the moment, Elastic Inference isn’t supported on cortex, but I can see it being supported in the near future as it has already landed on their radar. See ticket cortexlabs/cortex/issues/618.', 'Note: YOLOv3 and CRNN models and can be improved a lot by fine-tuning them on much larger datasets (of around 50–100k samples). At that point even the frames’ size can be further reduced to reduce the data usage without loosing to much accuracy: “compensate somewhere to be able to take from some other place”. This, in conjunction with the conversion of all these models to use half-precision types (and possibly Elastic Inference as well) could make for a very efficient/cost-effective inference machine.', 'With version 0.14 of cortex that has support for multi-process workers for the web server, I was able to reduce the number of GPU instances for the yolov3 API from 8 down to 2 and for the crnn API (which runs the inference on CRNN and CRAFT model) from 12 down to 10. This effectively means the total number of instances has been reduced by 40%, which is a very nice gain. All these instances are equipped with single T4 GPUs and 4 vCPUs each.', 'I have found out the most compute-intensive model is the CRAFT model, which is built on top of the VGG-16 model that has about 138M weights. Keep in mind that multiple inferences are generally required for each frame because there can be multiple detected license plates in one shot. This dramatically increases the compute requirements for it. Theoretically, the CRAFT model should be eliminated and instead improve (fine-tune) the CRNN model to much better recognize license plates. This way, the crnn API could be scaled down a lot — down to just 1 or 2 instances.', 'A couple of other things that could significantly improve the computational costs are:', 'These improvements were discussed here, here, here or here.', 'As for the AWS cluster costs incurred, here are my thoughts:', 'I see devices starting to depend on cloud computing more and more, especially for edge devices which have limited computing power. And as 5G is being deployed at the moment, it should theoretically bring the cloud closer to these compute-restrained devices. Therefore, the cloud’s influence should grow with it. The more reliable and ever-more present the 5G networks are, the more there’s gonna be confidence in off-loading computation to the cloud for so called mission-critical tasks — think self-driving cars.', 'One other thing I’ve been learning with this project is how easy things have become with the advent of streamlined platforms for deploying machine learning models in the cloud. 5 years ago, this would have been quite a big challenge: but now, a single man can do so much in a relatively short period of time.']"
02/2020,Why The Coronavirus Mortality Rate is Misleading,"Either the number of infected people is incorrect, or…",2.9K,46,https://towardsdatascience.com/@charitn,https://towardsdatascience.com/why-the-coronavirus-mortality-rate-is-misleading-cc63f571b6a6?source=collection_archive---------6-----------------------,9,1,['Why The Coronavirus Mortality Rate is Misleading'],25,"['As you probably know, the coronavirus, or COVID-19, is a new virus that causes respiratory ailments. Its origins can be traced back to Wuhan, China, in late 2019. It is comparable to other animal-based coronaviruses such as SARS and MERS. Although most cases have been confined to the Hubei Province, which Wuhan is part of, it has spread to HK, Taiwan, Japan, Germany, the US, and possibly even North Korea. Although new details about the virus’s nature and its growth are still emerging, we know that it has a 2–14 day incubation period and the average #/transmissions by each person who has it is roughly in the range of 2–2.5. As of Feb 14, over 64,000 have been suspected/confirmed to be infected, with almost 1400 deaths.', 'Despite the seemingly endless media coverage of the virus, many questions remain unanswered. Are there truly only 60k+ confirmed cases, or is this figure deflated? Is the number of fatalities under-reported as well? How fast is the virus spreading every day? How dangerous is it? In order for us to accurately gauge the danger it poses, it’s necessary to calculate the case fatality rate (CFR). Even if the virus is spreading quickly, it is not a large threat if people don’t die after contracting it.', 'Currently, the case fatality rate calculations are incorrect and misleading. As of now, it is calculated by dividing the number of deaths by the number of infected. The figure below shows the “misleading” CFR for Hubei Province since Jan 22. Based on these calculations, the case fatality rate is determined to be around 2.5% currently. However, it is incorrect because it does not take one hugely important factor into account: the lag effect.', 'Consider a group of 100 people that feel sick on Jan 10, 2020. Let’s track them over time. Some of them fall seriously ill over a period of a few days. Some recover, while others continue to be sick. If we could fast forward a year from now, we would know exactly the number of people that would die from this cohort. Most of the people that were sick would have recovered, and others would have died. At this point, you can calculate the death rate for this cohort to be the number of dead to the number that were initially infected. For example, we could use this method to calculate the case fatality rate of the 2002 SARS outbreak in China, since everyone who contracted SARS has either recovered or died by now. This is not true of the COVID-19 because it is still ongoing.', 'In the midst of an infection, we have different cohorts at different stages. I.e, a few thousand people that just became sick yesterday would not die today; but people that die today would be from a much earlier cohort. As a result, it is completely incorrect to calculate the CFR to divide the number of dead by the number of infected. The correct way to do this would be to follow each cohort separately and determine the death rate from each. Of course, when the disease has completely stopped, both the methods would converge. The coronavirus epidemic is ongoing and has not ceased and this method will not work. In contrast, this would work with calculating the SARS case fatality rate now.', 'Here is the graphical way of depicting it.', 'On the x-axis is the number of days after the members of the cohort have been infected (they were all infected on the same day) On the y-axis is the cumulative death rate (number of deaths/number infected) within the cohort.', 'Reports say it takes anywhere between 2–14 days to even show symptoms and possibly an additional few weeks or so before you could potentially die. Therefore, the number of deaths will likely be zero for the first few days regardless of the nature of the cohort members.', 'For the sake of illustration, we will look at two different groups. Say one group is 25-year-old men from Wuhan (modeled by a CFR of 2 % and reaching its peak in 14 days), and another represents a cohort of 80-year old men (modeled by a CFR of 5% reaching a peak in 7 days). We can assume the latter will rise faster, reach a greater height, and plateau sooner than the former. This means that more 80-year olds will die, and more quickly, due to their generally lower immunity and weaker overall health. The same can be said for people with health conditions such as heart disease and diabetes, as well as chronic smokers. As well as these factors (the very old and very young have demonstrated more vulnerability), the cohort curves will vary with country, wealth, etc.', 'This idea sounds quite simple in theory: find the number of infected each day and track them at the cohort level and we would have the case fatality rate. In practice, this is a bit difficult. Here are some challenges.', 'We need to have much more granular data on the cohort level that can help us model the CFR. Unfortunately, that data does not appear to be available.', 'Triangle charts are the right way to calculate the death rate by cohort. We would need to construct these triangle charts for groups that are as homogeneous as possible to ensure that certain members do not skew the death rates. Age, high-risk medical groups, and countries are the right granular segments to look at. Using these, we can most accurately gauge the case fatality rate for each group.', 'Below is an example triangle chart for the death rate.', 'Here, the uppermost rows depict older cohorts, and newer cohorts are represented by the lower rows. Each column represents an additional day after they were infected, with the number in the box representing the cumulative number of deaths within the cohort. Scanning the boxes diagonally from right to left gives the numbers for each cohort on the same day.', 'In order to model the number of deaths, two assumptions are made.', '1) There is a 5% case fatality rate among all cohorts.', '2) If people died, they died within 7 days after contracting the virus. Additionally, the data is only from the Hubei Province, where Wuhan is situated.', 'We can immediately see the cool-to-warm gradient between the topmost and bottom-most rows. This means that the older cohorts were smaller in size to start with and the newer cohorts are larger, indicating the spread of the COVID-19. (see the above figure)', 'Although the information can paint a general picture of what’s happening, shedding light on the basic nature of COVID-19’s spread, the model has several shortfalls. Firstly, not all information is available, hence the entire rows without data. There may be deaths not accounted for, especially in Wuhan, the epicenter. We are likely vastly undercounting the number of infected. We also do not understand how long it takes for people to die once infected. We do not know the mix of old and young; healthy and sick; etc. Despite its deficiencies, the model is still illustrative.', 'Okay, here is some preliminary analysis that shows that the data is seriously incorrect', 'Next, we use the model — there is a case fatality rate of 5% and the number of days until death is 7— to compare the predicted number of death versus actual in Hubei province.', 'This figure compares the model of the number of cumulative deaths with the assumed case fatality rate and time to mortality to the numbers reported by Beijing. Our model, with the current parameters, is strikingly close. However, given the huge amount of factors other than time to mortality and the case fatality rate, we can never be completely sure what the real numbers are. Here are some key insights.', 'It should be noted that there are multiple ways to get to the same number of deaths by altering some of these parameters. Regardless, it is very likely that the data is seriously incorrect (many more are infected) or we have a serious crisis (the CFR is > 5%).', 'Here is an update that shows the range of possibilities of the case fatality rate. Another update that shows that the warm weather will not curb the growth. One more update where I reveal how many deaths the US could experience', 'Note from the editors: towardsdatascience.com is a Medium publication primarily based on the study of data science and machine learning. We aren’t health professionals or epidemiologists. To learn more about the coronavirus pandemic, you can click here.']"
02/2020,How bad will the Coronavirus Outbreak get? — Predicting the outbreak figures,A Data Analyst’s…,1.1K,13,https://towardsdatascience.com/@angeleastbengal,https://towardsdatascience.com/how-bad-will-the-coronavirus-outbreak-get-predicting-the-outbreak-figures-f0b8e8b61991?source=collection_archive---------7-----------------------,6,5,"['How bad will the Coronavirus Outbreak get? — Predicting the outbreak figures', 'Where do we stand today?', 'How Contagious is the Virus?', 'How bad is the Outbreak in China?', 'A look into the future — Number of Confirmed Cases will rise to over 38,000 by the end of next week!']",13,"['Note from the editors: towardsdatascience.com is a Medium publication primarily based on the study of data science and machine learning. We aren’t health professionals or epidemiologists. To learn more about the coronavirus pandemic, you can click here.', '“Everyone in China and elsewhere who are suffering, may God strengthen the victims and their families and guide them through this tough time. Please follow the advice outlined in the WHO website to stay safe and protected”', 'The outbreak continues to spread — As of Feb 1st, the WHO has confirmed around 12,000 cases, and more than 2,000 alone was confirmed on Saturday (That’s roughly 18% of the total reported cases). A total of 48 provinces across 28 countries have been affected. However, only a little more than 2% of these cases were found outside China. Although this number might seem small, Wuhan, the city at the center of the outbreak, is a major transportation hub of the country. Inter-city travels increasing because of the Lunar New year has aided the outbreak to some extent. During Oct-Nov last year, close to 2 million people flew out of Wuhan and roughly 120 thousand people flew outside the country as well. Hence the outbreak of Coronavirus remains a major concern across the globe.', 'The pace and ease at which any outbreak spreads, determine its scale. The Report published by the Imperial College of London suggests that a person suffering from Coronavirus can affect 1.5 to 3.5 healthy people. According to the New York Times, if 5 people with new coronavirus can impact 2.6 others, then 5 people could be sick after 1 Cycle, 18 people after 2 Cycles, 52 people after 3 Cycles and so on.', 'I worked on creating a QlikSense base dashboard to monitor the current scenario and track the outbreak. You can find more about it below.', 'As of Saturday, the WHO has confirmed 11,821 cases in China. However, figures collated by the Johns Hopkins University suggest a much higher number. A probability that some of the cases are FALSE POSITIVES exists, i.e. someone was incorrectly suspected of having the virus but were cleared off when tested. The average growth in the number of cases reported in China remains as high as 16.3% (higher than SARS cases in 2003).', 'It is difficult to predict the outbreak using a time series model, given we don’t have enough data points. Let’s take a look at the growth rate in confirmed cases over time. Three key parameters need to be considered when looking into a time-dependent variable.', 'Assumptions and Insights', 'Forecast', 'Using total confirmed cases from the previous day and applying an alpha (-2.9%) of recent growth rate, I predicted the outbreak for the next week. The forecast was validated using WHO Situation Report 12 & Report 13, and a MAPE of 1% is observed. Taking a look at the Graph suggests that I ended up over predicting the figures.', 'A time series is a sequence where a metric is recorded over regular time intervals. Depending on the frequency, a time series can be of yearly, monthly, weekly or even daily. Since the number of confirmed cases is a time-dependent variable technique like Single or Double Exponential Smoothing or an ARIMA can yield a better forecast. However, I didn’t try it owing to fewer data points. Any time series model requires enough data points to understand the interaction between the current and previous set of values. However, I used the partial autocorrelation (PACF) plot to validate if my assumptions were correct or not.', 'I am going to write more about this in my next article, however, if you wish to learn more, visit this site. Using PACF I figured out the following:', 'About the Author: Advanced analytics professional and management consultant helping companies find solutions for diverse problems through a mix of business, technology, and math on organisational data. A Data Science enthusiast, here to share, learn and contribute; You can connect with me on Linked;']"
02/2020,Deep learning isn’t hard anymore,"At least, building software with deep learning isn’t",5.2K,8,https://towardsdatascience.com/@calebkaiser,https://towardsdatascience.com/deep-learning-isnt-hard-anymore-26db0d4749d7?source=collection_archive---------8-----------------------,6,4,"['Deep learning isn’t hard anymore', 'What is transfer learning?', 'Why transfer learning is the key to the next generation of ML-powered software', 'Machine learning engineering is becoming a real ecosystem']",44,"['In the not-so-distant past, a data science team would need a few things to effectively use deep learning:', 'This had the effect of bottlenecking deep learning, limiting it to the few projects that met those conditions.', 'Over the last couple years, however, things have changed.', 'At Cortex, we are seeing users launch a new generation of products built on deep learning—and unlike before, these products aren’t all being built using one-of-a-kind model architectures.', 'The driver behind this growth is transfer learning.', 'Transfer learning, broadly, is the idea that the knowledge accumulated in a model trained for a specific task—say, identifying flowers in a photo—can be transferred to another model to assist in making predictions for a different, related task—like identifying melanomas on someone’s skin.', 'Note: If you want a more technical dive into transfer learning, Sebastian Ruder has written a fantastic primer.', 'There are a variety of approaches to transfer learning, but one approach in particular—finetuning—is finding widespread adoption.', 'In this approach, a team takes a pre-trained model and removes/retrains the last layers of the model to focus on a new, related task. For example, AI Dungeon is an open world text adventure game that went viral for how convincing its AI-generated stories are:', 'What’s remarkable is that AI Dungeon wasn’t developed in one of Google’s research labs—it was a hackathon project built by a single engineer.', 'Nick Walton, the creator of AI Dungeon, built it not by designing a model from scratch, but by taking a state-of-the-art NLP model—OpenAI’s GPT-2—and finetuning it on choose your own adventure texts.', 'The reason this works at all is that in a neural network, the initial layers focus on simple, general features, while the final layers focus on more task-specific classification/regression. Andrew Ng visualizes the layers and their relative levels of specificity by imagining an image recognition model:', 'The general knowledge of the base layers, it turns out, often translates well to other tasks. In the AI Dungeon example, GPT-2 had a state of the art understanding of general English, it just needed a little retraining in its final layers to perform well within the choose your own adventure genre.', 'Through this process, a single engineer can deploy a model that achieves state of the art results in a new domain in a matter of days.', 'Earlier, I mentioned the favorable conditions that needed to be present for machine learning—and deep learning, in particular—to be used effectively. You needed access to a large, clean dataset, you needed to be able to design an effective model, and you needed the means to train it.', 'This meant that by default, projects in certain domains or without certain resources weren’t feasible.', 'Now, with transfer learning, these bottlenecks are being removed:', 'Deep learning typically requires large amounts of labeled data, and in many domains, this data simply doesn’t exist. Transfer learning can fix this.', 'For example, a team affiliated with Harvard Medical School recently deployed a model that could “predict long-term mortality, including noncancer death, from chest radiographs.”', 'With a dataset of ~50,000 labeled images, they did not have the data necessary to train their CNN (convolutional neural network) from scratch. Instead, they took a pre-trained Inception-v4 model (which is trained on the ImageNet dataset of over 14 million images) and used transfer learning and slight architecture modifications to adapt the model to their dataset.', 'In the end, their CNN was successful in using just one chest image per patient to generate risk scores that were correlated with the patients’ actual mortality.', 'Training a model on huge amounts of data isn’t just a question of acquiring a large dataset, it is also a question of resources and time.', 'For example, when Google was developing their state of the art image classification model Xception, they trained two versions: one on the ImageNet dataset (14 million images), and the other on the JFT dataset (350 million images).', 'Training on 60 NVIDIA K80 GPUs—with various optimizations—it took 3 days for a single ImageNet experiment to run. The JFT experiments took over a month.', 'Now that the pre-trained Xception model is released, however, teams can finetune their own version much faster.', 'For example, a team at the University of Illinois and Argonne National Laboratory recently trained a model to classify images of galaxies as spiral or elliptical:', 'Despite only having a dataset of 35,000 labeled images, they were able to finetune Xception in just 8 minutes using NVIDIA GPUs.', 'The resulting model is able to classify galaxies with a 99.8% success rate at a superhuman speed of over 20,000 galaxies per minute, when it is served on GPUs.', 'Google probably didn’t care much about cost when they trained their Xception model on 60 GPUs for months at a time. However, for any team that doesn’t have a Google-sized budget, the price of model training is a real concern.', 'For example, when OpenAI first publicized the results of GPT-2, they released the model architecture but not the full pre-trained model, due to concerns about misuse.', 'In response, a team at Brown replicated GPT-2 by following the architecture and training procedure laid out in the paper, calling their model OpenGPT-2. It cost them ~$50,000 to train, and it didn’t perform as well as GPT-2.', '$50,000 for a model that performs below the state of the art is a big risk for any team building real, production software without a massive bankroll.', 'Fortunately, transfer learning lowers this cost dramatically.', 'When Nick Walton built AI Dungeon, he did it by finetuning GPT-2. OpenAI had already put roughly 27,118,520 pages of text and thousands of dollars into training the model, and Walton didn’t need to recreate any of that.', 'Instead, he used a much smaller set of text scraped from chooseyourstory.com, and finetuned the model in Google Colab—which is entirely free.', 'Looking at software engineering as a parallel, we typically see ecosystems “mature” in pretty standard patterns.', 'A new programming language will emerge with some exciting features, and people will use it for specialized use-cases, research projects, and toys. At this stage, anyone who uses it has to build all their basic utilities from scratch.', 'Down the line, people within the community develop libraries and projects that abstract common utilities away, until the tooling is capable and stable enough to be used in production.', 'At this stage, the engineers using it to build software are not concerned about sending HTTP requests or connecting to databases—all of that is abstracted away—and are solely focused on building their product.', 'In other words, Facebook builds React, Google builds Angular, and engineers use them to build products. With transfer learning, machine learning engineering is taking that step forward.', 'As companies like OpenAI, Google, Facebook, and the rest of the tech giants release powerful open source models, the “tooling” for machine learning engineers gets more capable and stable.', 'Instead of spending time building a model from scratch with PyTorch or TensorFlow, machine learning engineers are using open source models and transfer learning to build products, meaning a whole new generation of ML-powered software is on its way.', 'Now, machine learning engineers just have to worry about how to put these models into production.', 'Note: If you’re interested in building software with finetuned models, check out Cortex, an open source platform for deploying models. Full disclosure, I’m a contributor.']"
02/2020,Build a web data dashboard in just minutes with Python,Exponentially increase power & accessibility by…,1.1K,6,https://towardsdatascience.com/@_jphwang,https://towardsdatascience.com/build-a-web-data-dashboard-in-just-minutes-with-python-d722076aee2b?source=collection_archive---------9-----------------------,7,4,"['Build a web data dashboard in just minutes with Python', 'Before we get started', 'Previously, on Python…', 'Into the World Wide Web']",50,"['I don’t know about you, but I occasionally find it a little bit intimidating to have to code something. This is doubly so when I’m building something akin to web development rather than doing some local data analysis and visualisation. I’m a competent Python coder, but I wouldn’t call myself a web developer at all, even after having more than dabbled with Django and Flask.', 'Still, converting your data outputs to a web app leads to a few non-trivial improvements for your project.', 'It is just much easier to build in true, powerful interactivity into a web app. It also means that you can control exactly how the data is presented, as the web app can become the de facto report as well as the access point to your data. Lastly, and most importantly, you can exponentially scale the accessibility to your outputs; making them available anywhere, any time. There is always a web browser at a user’s fingertips.', 'So, I bit the bullet and started to do just this with some of my data projects recently, with surprisingly fast speed and efficiency. I converted one of my outputs from this article to a web app here in just a couple of hours.', 'I thought this was rather cool, and wanted to share how this came together in just a few lines of code.', 'As always, I include everything you need to replicate my steps (data & code), and the article is not really about basketball. So do not worry if you are unfamiliar with it, and let’s get going.', 'I include the code and data in my GitLab repo here (dash_simple_nba directory). So please feel free to play with it / improve upon it.', 'I assume you’re familiar with python. Even if you’re relatively new, this tutorial shouldn’t be too tricky, though.', 'You’ll need pandas, plotly and dash. Install each (in your virtual environment) with a simple pip install [PACKAGE_NAME].', 'For this tutorial, I am simply going to skip *most* of the steps taken to create the local version of our visualisation. If you’re interested in what is going on, take a look at this article:', 'We will have a recap session, though, so you can see what is happening between plotting the chart locally with Plotly, and how to port that to a web app with Plotly Dash.', 'I have pre-processed the data, and saved it as a CSV file. It is a collection of player data for the current NBA season (as of 26/Feb/2020), which shows:', 'For this portion, follow along by opening local_plot.py in my repo.', 'Load the data with:', 'Inspect the data with all_teams_df.head(), and you should see:', 'Each player’s data has been compiled for each minute of the game (excluding overtime), with stats pl_acc and pl_pps being the only exception, as they have been compiled per quarter of the game (for each 12 minute period).', 'The dataframe contains all NBA players, so let’s break it down to a manageable size, by filtering for a team. For instance, the New Orleans Pelicans’ players can be chosen with:', 'Then, our data can be visualised in Plotly, as below:', 'At the risk of doing this:', 'I do add a few small details to my chart, to produce this version of the same graph.', 'This is the code that I used to do it.', 'Now, while it’s a lot of formatting code, I thought it useful to show you how I did it, because we are going to be re-using these functions in our Dash version of the code.', 'Now, let’s get to the main event — how to create a web app out of these plots.', 'You can read more about Plotly Dash here, but for now all you need to know that it is an open-source software package developed to abstract away the difficulties in putting your visualisations on the web.', 'It works with Flask under the hood, and you can happily reuse most of the code that you used to develop plots in plotly.py.', 'This is the simple version that I put together:', 'Try it out! It should open this plot on your browser.', 'What’s the big deal? Well, for one — it is a live web app, in under 25 lines of code. And notice the drop-down menu on the top left? Try changing the values on it, and watch the graph change *magically*.', 'Go on, I’ll wait.', 'Okay? Done.', 'Let’s briefly go through the code.', 'At a high level, what I’m doing here is to:', 'If you take a look at the code, so many of what is probably trivial for web devs but annoying for me is abstracted away.', 'dcc.Graph wraps the figure object from plotly.py into my web app and HTML components like divs can be called and set up conveniently with html.Div objects.', 'Most gratifying for me personally is that Input objects and callbacks from those inputs are declaratively set up, and I can avoid having to deal with things like HTML forms or JavaScript.', 'And the resulting app still works beautifully. The graph is updated the moment that the pulldown menu is used to select another value.', 'And we did all that in fewer than 25 lines of code.', 'At this point, you might be asking — why Dash? We can do all this with a JS framework front end, and Flask, or any one of myriad other combinations.', 'To someone like me who prefers the comfort of Python than natively dealing with HTML and CSS, using Dash abstracts away a lot of stuff that doesn’t add a lot of value to the end product.', 'Take, for instance, a version of this app that includes further formatting and notes for the audience:', '(It is simple_dash_w_format.py in the git repo)', 'Most of the changes are cosmetic, but I will note that here, I just write the body text in Markdown, and simply carry over my formatting functions from Plotly to be used in the formatting the graphs in Dash.', 'This saves me a tremendous amount of time between doing data analysis and visualisation to deployment to clients’ views.', 'All in all, from starting with my initial graph, I think it probably took less than an hour to deploy it to Heroku. Which is pretty amazing.', 'I will get into more advanced features of Dash, and actually doing some cool things with it functionality-wise, but I was very happy with this outcome in terms of ease and speed.', 'Try it out yourself — I think that you’d be very impressed. Next time, I plan to write about some really cool things you can do with Dash, and building truly interactive dashboards.', 'Edit: I’m starting a Substack for all things Data and Visualisation — it is going to be a way for me to engage directly with you. I would love for you to join me here.', 'I also wrote a little about why I’m starting a Substack.', 'If you liked this, say 👋 / follow on twitter, or follow for updates. This is the article that the data viz is based on:', 'Also, here is another article I wrote about improving data-based storytelling through better visualisation of time series data:']"
03/2020,Why Python is not the programming language of the future,Opinion,15K,162,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/why-python-is-not-the-programming-language-of-the-future-30ddc5339b66?source=collection_archive---------0-----------------------,7,4,"['Why Python is not the programming language of the future', 'What makes Python popular right now', 'Downsides of Python — and whether they’ll be fatal', 'What could replace Python in the future — and when']",38,"['It took the programming community a couple of decades to appreciate Python. But since the early 2010’s, it has been booming — and eventually surpassing C, C#, Java and JavaScript in popularity.', 'But until when will that trend continue? When will Python eventually be replaced by other languages, and why?', 'Putting an exact expiry date on Python would be so much speculation, it might as well pass as Science-Fiction. Instead, I will assess the virtues that are boosting Python’s popularity right now, and the weak points that will break it in the future.', 'Python’s success is reflected in the Stack Overflow trends, which measure the count of tags in posts on the platform. Given the size of StackOverflow, this is quite a good indicator for language popularity.', 'While R has been plateauing over the last few years, and many other languages are on a steady decline, Python’s growth seems unstoppable. Almost 14% of all StackOverflow questions are tagged “python”, and the trend is going up. And there are several reasons for that.', 'Python has been around since the nineties. That doesn’t only mean that it has had plenty of time to grow. It has also acquired a large and supportive community.', 'So if you have any issue while you’re coding in Python, the odds are high that you’ll be able to solve it with a single Google search. Simply because somebody will have already encountered your problem and written something helpful about it.', 'It’s not only the fact that it has been around for decades, giving programmers the time to make brilliant tutorials. More than that, the syntax of Python is very human-readable.', 'For a start, there’s no need to specify the data type. You just declare a variable; Python will understand from the context whether it’s an integer, a float value, a boolean or something else. This is a huge edge for beginners. If you’ve ever had to program in C++, you know how frustrating it is your program won’t compile because you swapped a float for an integer.', 'And if you’ve ever had to read Python and C++ code side-by-side, you’ll know how understandable Python is. Even though C++ was designed with English in mind, it’s a rather bumpy read compared to Python code.', 'Since Python has been around for so long, developers have made a package for every purpose. These days, you can find a package for almost everything.', 'Want to crunch numbers, vectors and matrices? NumPy is your guy. Want to do calculations for tech and engineering? Use SciPy. Want to go big in data manipulation and analysis? Give Pandas a go.Want to start out with Artificial Intelligence? Why not use Scikit-Learn.', 'Whichever computational task you’re trying to manage, chances are that there is a Python package for it out there. This makes Python stay on top of recent developments, can be seen from the surge in Machine Learning over the past few years.', 'Based on the previous elaborations, you could imagine that Python will stay on top of sh*t for ages to come. But like every technology, Python has its weaknesses. I will go through the most important flaws, one by one, and assess whether these are fatal or not.', 'Python is slow. Like, really slow. On average, you’ll need about 2–10 times longer to complete a task with Python than with any other language.', 'There are various reasons for that. One of them is that it’s dynamically typed — remember that you don’t need to specify data types like in other languages. This means that a lot of memory needs to be used, because the program needs to reserve enough space for each variable that it works in any case. And lots of memory usage translates to lots of computing time.', 'Another reason is that Python can only execute one task at a time. This is a consequence of flexible datatypes — Python needs to make sure each variable has only one datatype, and parallel processes could mess that up.', 'In comparison, your average web browser can run a dozen different threads at once. And there are some other theories around, too.', 'But at the end of the day, none of the speed issues matter. Computers and servers have gotten so cheap that we’re talking about fractions of seconds. And the end user doesn’t really care whether their app loads in 0.001 or 0.01 seconds.', 'Originally, Python was dynamically scoped. This basically means that, to evaluate an expression, a compiler first searches the current block and then successively all the calling functions.', 'The problem with dynamic scoping is that every expression needs to be tested in every possible context — which is tedious. That’s why most modern programming languages use static scoping.', 'Python tried to transition to static scoping, but messed it up. Usually, inner scopes — for example functions within functions — would be able to see and change outer scopes. In Python, inner scopes can only see outer scopes, but not change them. This leads to a lot of confusion.', 'Despite all of the flexibility within Python, the usage of Lambdas is rather restrictive. Lambdas can only be expressions in Python, and not be statements.', 'On the other hand, variable declarations and statements are always statements. This means that Lambdas cannot be used for them.', 'This distinction between expressions and statements is rather arbitrary, and doesn’t occur in other languages.', 'In Python, you use whitespaces and indentations to indicate different levels of code. This makes it optically appealing and intuitive to understand.', 'Other languages, for example C++, rely more on braces and semicolons. While this might not be visually appealing and beginner-friendly, it makes the code a lot more maintainable. For bigger projects, this is a lot more useful.', 'Newer languages like Haskell solve this problem: They rely on whitespaces, but offer an alternative syntax for those who wish to go without.', 'As we’re witnessing the shift from desktop to smartphone, it’s clear that we need robust languages to build mobile software.', 'But not many mobile apps are being developed with Python. That doesn’t mean that it can’t be done — there is a Python package called Kivy for this purpose.', 'But Python wasn’t made with mobile in mind. So even though it might produce passable results for basic tasks, your best bet is to use a language that was created for mobile app development. Some widely used programming frameworks for mobile include React Native, Flutter, Iconic, and Cordova.', 'To be clear, laptops and desktop computers should be around for many years to come. But since mobile has long surpassed desktop traffic, it’s safe to say that learning Python is not enough to become a seasoned all-round developer.', 'A Python script isn’t compiled first and then executed. Instead, it compiles every time you execute it, so any coding error manifests itself at runtime. This leads to poor performance, time consumption, and the need for a lot of tests. Like, a lot of tests.', 'This is great for beginners since testing teaches them a lot. But for seasoned developers, having to debug a complex program in Python makes them go awry. This lack of performance is the biggest factor that sets a timestamp on Python.', 'There are a few new competitors on the market of programming languages:', 'While there are other languages on the market, Rust, Go, and Julia are the ones that fix weak patches of Python. All of these languages excel in yet-to-come technologies, most notably in Artificial Intelligence. While their market share is still small, as reflected in the number of StackOverflow tags, the trend for all of them is clear: upwards.', 'Given the ubiquitous popularity of Python at the moment, it will surely take half a decade, maybe even a whole, for any of these new languages to replace it.', 'Which of the languages it will be — Rust, Go, Julia, or a new language of the future — is hard to say at this point. But given the performance issues that are fundamental in the architecture of Python, one will inevitably take its spot.']"
03/2020,I Thought I Was Mastering Python Until I Discovered These Tricks,Python best practices and tips that…,5.5K,22,https://towardsdatascience.com/@chouhbik,https://towardsdatascience.com/i-thought-i-was-mastering-python-until-i-discovered-these-tricks-e40d9c71f4e2?source=collection_archive---------1-----------------------,9,12,"['I Thought I Was Mastering Python Until I Discovered These Tricks', '1. Readability is important', '2. Avoid unuseful conditions', '3. Adequate use of Whitespace', '4. Docstrings and Comments', '5. Variables and Assignment', '6. List Concatenation & Join', '7. Test true conditions', '8. Use enumerate when it’s possible', '9. List Comprehension', '10. Generator Expressions', 'Conclusion']",59,"['Python is one of the most popular programming languages \u200b\u200bfor beginners, making it the most widely taught language in schools around the world.', 'However, learning Python is not an easy thing. To get started, you first need to find the best online way to get there, which is difficult in itself. There are thousands of different Python courses and tutorials, all claiming to be the best.', 'True, practice alone is not perfect, but perfect practice is. This means that you need to make sure that you are always following the best coding practices (commenting on your code, using correct syntax, etc.), otherwise you will likely end up adopting bad habits that could harm your future lines of code.', '“A universal convention supplies all of maintainability, clarity, consistency, and a foundation for good programming habits too. What it doesn’t do is insist that you follow it against your will. That’s Python!”', '— Tim Peters on comp.lang.python, 2001–06–16', 'In this article, I’m going to give my top 10 tips to help you code in Python quickly and efficiently.', 'Programs must be written for people to read, and only incidentally for machines to execute.', 'Hal Abelson', 'First of all, try to make your programs easy to read by following some programming conventions. A programming convention is one that a seasoned programmer follows when writing his or her code. There’s no quicker way to reveal that you’re a “newbie” than by ignoring conventions. Some of these conventions are specific to Python; others are used by computer programmers in all languages.', 'Essentially, readability is the characteristic which specifies how easy another person can understand some parts of your code (and not you!).', 'As an example, I was not used to write with vertical alignment and to align the function’s parameters with opening delimiter.', 'Look at other examples in the Style Guide for Python Code and decide what looks best.', 'Another important thing that we do very often is resembling programs that we have seen or written before, which is why our exposure to readable programs is important in learning programming.', 'Often, a long if & elif & …. & else conditions is the sign of code that needs refactoring, these conditions make your code lengthy and really hard to interpret. Sometimes they can easily be replaced, for example, I used to do the following:', 'This is just dumb! The function is returning a boolean, so why even use if blocks in the first place? The correct what of doing this would be:', 'In a Hackerrank challenge, you are given the year and you have to write a function to check if the year is leap or not. In the Gregorian calendar three criteria must be taken into account to identify leap years:', 'So in this challenge, forget about ifs and elses and just do the following:', 'The docstrings explain how to use the code :', 'The comments explain what are for the maintainers of your code. Examples including notes for yourself, such as:', '# !!! BUG: …', '# !!! FIX: This is a hack', '# ??? Why is this here?', 'It is on your responsibility to write good docstrings and good comments, so always keep them up to date! When making changes, make sure the comments and docstrings are consistent with the code.', 'You will find a detailed PEP dedicated for Doctsrings : “Docstring Conventions”', 'In other programming languages :', 'In Python, it’s better to use the assignment in one line code :', 'You may have already seen it but do you know how it works?', 'Other examples:', 'Useful in loops on structured data (the variable user above has been kept):', 'It is also possible to do the opposite way, just make sure you have the same structure on the right and on the left:', 'Let’s start with a list of strings:', 'We want to concatenate these chains together to create a long one. Particularly when the number of substrings is large, avoid doing this :', 'It is very slow. It uses a lot of memory and performance. The sum will add up, store, and then move on to each intermediate step.', 'Instead, do this:', 'The join () method makes the entire copy in one pass. When you only process a few strings, it makes no difference. But get into the habit of building your chains optimally, because with hundreds or thousands strings, it will truly make a difference.', 'Here are some techniques for using the join () method. If you want a space as a separator:', 'or a comma and a space:', 'To make a grammatically correct sentence, we want commas between each value except the last one, where we prefer an “or”. The syntax for splitting a list does the rest. The [: -1] returns everything except the last value, which we can concatenate with our commas.', 'It is elegant and quick to take advantage of Python with regard to Boolean values:', 'The enumerate function takes a list and returns pairs (index, item):', 'It is necessary to use a list to display the results because enumerate is a lazy function, generating one item (a pair) at a time, only when requested. A for loop requires such a mechanism. Print does not take one result at a time but must be in possession of the entire message to be displayed. We therefore automatically converted the generator to a list before using print.', 'So, using the loop below is much better:', 'The version with enumerate is shorter and simpler than the two other versions. An example showing that the enumerate function returns an iterator (a generator is a kind of iterator)', 'The traditional way with for and if:', 'Using a list comprehension:', 'The listcomps are clear and direct. You can have several for loops and if conditions within the same listcomp, but beyond two or three, or if the conditions are complex, I suggest you use the usual for loop.', 'For example, the list of squares from 0 to 9:', 'The list of odd numbers within the previous list:', 'Another example:', 'Let’s sum the squares of numbers less than 100:', 'We can also use the sum function which does the job faster for us by building the right sequence.', 'The generator expressions are like list comprehensions, except in their calculation, they are lazy. Listcomps calculate the entire result in a single pass, to store it in a list. Generator expressions calculate one value at a time, when necessary. This is particularly useful when the sequence is very long and the generated list is only an intermediate step and not the final result.', 'For example if we have to sum the squares of several billion integers, we will reach a saturation of the memory with a list comprehension, but the generator expressions will not have any problem. Well it will take a while though!', 'The difference in syntax is that listcomps have square brackets, while generator expressions do not. Generator expressions sometimes require parentheses, so you should always use them.', 'In short :', 'In this article, I have presented some of my best tips for learning to program in Python. If you really want to become a programmer or add a coding skill to your skills, learning Python is a great place to start. Look for high quality Python training online and start to find out how to program in Python. I recommend that you learn the basics with an interactive course before moving on to more difficult concepts.', 'You should not speed up the learning process too much, or you may miss important information. Take notes and make sure to review them regularly and try to practice writing code as often as possible.', 'Connect with colleagues who are learning like you and don’t be afraid to ask questions when you have them. Helping others when they have problems can be a great review, and working with someone else’s code is a great way to learn new things.', 'If you do all of this, nothing can stop you! So what are you waiting for? Start programming in Python now!']"
03/2020,OVER 100 Data Scientist Interview Questions and Answers!,"Interview Questions from Amazon, Google…",2.3K,5,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/over-100-data-scientist-interview-questions-and-answers-c5a66186769a?source=collection_archive---------2-----------------------,45,8,"['OVER 100 Data Scientist Interview Questions and Answers!', 'Table of Content', 'Machine Learning Fundamentals', 'Statistics, Probability, and Mathematics', 'SQL Practice Problems', 'Miscellaneous', 'References', 'Thanks for Reading!']",236,"['I know this is long…', 'Really long. But don’t be intimidated by the length — I have broken this down into four sections (machine learning, stats, SQL, miscellaneous) so that you can go through this bit by bit.', 'Think of this as a workbook or a crash course filled with hundreds of data science interview questions that you can use to hone your knowledge and to identify gaps that you can then fill afterwards.', 'I hope you find this helpful and wish you the best of luck in your data science endeavors!', 'If you’re interested in learning how to solve data science interview questions, try Interview query and get an in-depth solution every week. Interview Query works on making you good at interviews as fast as possible.', 'There are many steps that can be taken when data wrangling and data cleaning. Some of the most common steps are listed below:', 'Read more on the Amazon machine learning interview and questions here.', 'There are a number of ways to handle unbalanced binary classification (assuming that you want to identify the minority class):', 'While boxplots and histograms are visualizations used to show the distribution of the data, they communicate information differently.', 'Histograms are bar charts that show the frequency of a numerical variable’s values and are used to approximate the probability distribution of the given variable. It allows you to quickly understand the shape of the distribution, the variation, and potential outliers.', 'Boxplots communicate different aspects of the distribution of data. While you can’t see the shape of the distribution through a box plot, you can gather other information like the quartiles, the range, and outliers. Boxplots are especially useful when you want to compare multiple charts at the same time because they take up less space than histograms.', 'Both L1 and L2 regularization are methods used to reduce the overfitting of training data. Least Squares minimizes the sum of the squared residuals, which can result in low bias but high variance.', 'L2 Regularization, also called ridge regression, minimizes the sum of the squared residuals plus lambda times the slope squared. This additional term is called the Ridge Regression Penalty. This increases the bias of the model, making the fit worse on the training data, but also decreases the variance.', 'If you take the ridge regression penalty and replace it with the absolute value of the slope, then you get Lasso regression or L1 regularization.', 'L2 is less robust but has a stable solution and always one solution. L1 is more robust but has an unstable solution and can possibly have multiple solutions.', 'StatQuest has an amazing video on Lasso and Ridge regression here.', 'A neural network is a multi-layered model inspired by the human brain. Like the neurons in our brain, the circles above represent a node. The blue circles represent the input layer, the black circles represent the hidden layers, and the green circles represent the output layer. Each node in the hidden layers represents a function that the inputs go through, ultimately leading to an output in the green circles. The formal term for these functions is called the sigmoid activation function.', 'If you want a step by step example of creating a neural network, check out Victor Zhou’s article here.', 'If you’re a visual/audio learner, 3Blue1Brown has an amazing series on neural networks and deep learning on YouTube here.', 'Cross-validation is essentially a technique used to assess how well a model performs on a new independent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model.', 'There isn’t a one-size-fits-all metric. The metric(s) chosen to evaluate a machine learning model depends on various factors:', 'There are a number of metrics that can be used, including adjusted r-squared, MAE, MSE, accuracy, recall, precision, f1 score, and the list goes on.', 'Recall attempts to answer “What proportion of actual positives was identified correctly?”', 'Precision attempts to answer “What proportion of positive identifications was actually correct?”', 'A false positive is an incorrect identification of the presence of a condition when it’s absent.', 'A false negative is an incorrect identification of the absence of a condition when it’s actually present.', 'An example of when false negatives are more important than false positives is when screening for cancer. It’s much worse to say that someone doesn’t have cancer when they do, instead of saying that someone does and later realizing that they don’t.', 'This is a subjective argument, but false positives can be worse than false negatives from a psychological point of view. For example, a false positive for winning the lottery could be a worse outcome than a false negative because people normally don’t expect to win the lottery anyways.', 'Supervised learning involves learning a function that maps an input to an output based on example input-output pairs [1].', 'For example, if I had a dataset with two variables, age (input) and height (output), I could implement a supervised learning model to predict the height of a person based on their age.', 'Unlike supervised learning, unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes. A common use of unsupervised learning is grouping customers by purchasing behavior to find target markets.', 'Check out my article ‘All Machine Learning Models Explained in Six Minutes’ if you’d like to learn more about this!', 'There are two main ways that you can do this:', 'A) Adjusted R-squared.', 'R Squared is a measurement that tells you to what extent the proportion of variance in the dependent variable is explained by the variance in the independent variables. In simpler terms, while the coefficients estimate trends, R-squared represents the scatter around the line of best fit.', 'However, every additional independent variable added to a model always increases the R-squared value — therefore, a model with several independent variables may seem to be a better fit even if it isn’t. This is where adjusted R² comes in. The adjusted R² compensates for each additional independent variable and only increases if each given variable improves the model above what is possible by probability. This is important since we are creating a multiple regression model.', 'B) Cross-Validation', 'A method common to most people is cross-validation, splitting the data into two sets: training and testing data. See the answer to the first question for more on this.', 'NLP stands for Natural Language Processing. It is a branch of artificial intelligence that gives machines the ability to read and understand human languages.', 'There are a couple of reasons why a random forest is a better choice of model than a support vector machine:', 'Dimensionality reduction is the process of reducing the number of features in a dataset. This is important mainly in the case when you want to reduce variance in your model (overfitting).', 'Wikipedia states four advantages of dimensionality reduction (see here):', 'In its simplest sense, PCA involves project higher dimensional data (eg. 3 dimensions) to a smaller space (eg. 2 dimensions). This results in a lower dimension of data, (2 dimensions instead of 3 dimensions) while keeping all original variables in the model.', 'PCA is commonly used for compression purposes, to reduce required memory and to speed up the algorithm, as well as for visualization purposes, making it easier to summarize data.', 'One major drawback of Naive Bayes is that it holds a strong assumption in that the features are assumed to be uncorrelated with one another, which typically is never the case.', 'One way to improve such an algorithm that uses Naive Bayes is by decorrelating the features so that the assumption holds true.', 'There are a couple of drawbacks of a linear model:', 'Another way of asking this question is “Is a random forest a better model than a decision tree?” And the answer is yes because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust, and less prone to overfitting.', 'Mean Squared Error (MSE) gives a relatively high weight to large errors — therefore, MSE tends to put too much emphasis on large deviations. A more robust alternative is MAE (mean absolute deviation).', 'The assumptions are as follows:', 'Extreme violations of these assumptions will make the results redundant. Small violations of these assumptions will result in a greater bias or variance of the estimate.', 'Multicollinearity exists when an independent variable is highly correlated with another independent variable in a multiple regression equation. This can be problematic because it undermines the statistical significance of an independent variable.', 'You could use the Variance Inflation Factors (VIF) to determine if there is any multicollinearity between independent variables — a standard benchmark is that if the VIF is greater than 5 then multicollinearity exists.', 'There are a couple of metrics that you can use:', 'R-squared/Adjusted R-squared: Relative measure of fit. This was explained in a previous answer', 'F1 Score: Evaluates the null hypothesis that all regression coefficients are equal to zero vs the alternative hypothesis that at least one doesn’t equal zero', 'RMSE: Absolute measure of fit.', 'Decision trees are a popular model, used in operations research, strategic planning, and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy.', 'Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. By relying on a “majority wins” model, it reduces the risk of error from an individual tree.', 'For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all 4 decision trees, the predicted value would be 1. This is the power of random forests.', 'Random forests offer several other benefits including strong performance, can model non-linear boundaries, no cross-validation needed, and gives feature importance.', 'A kernel is a way of computing the dot product of two vectors 𝐱x and 𝐲y in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called “generalized dot product” [2]', 'The kernel trick is a method of using a linear classifier to solve a non-linear problem by transforming linearly inseparable data to linearly separable ones in a higher dimension.', 'When the number of features is greater than the number of observations, then performing dimensionality reduction will generally improve the SVM.', 'Overfitting is an error where the model ‘fits’ the data too well, resulting in a model with high variance and low bias. As a consequence, an overfit model will inaccurately predict new data points even though it has a high accuracy on the training data.', 'Boosting is an ensemble method to improve a model by reducing its bias and variance, ultimately converting weak learners to strong learners. The general idea is to train a weak learner and sequentially iterate and improve the model by learning from the previous learner. You can learn more about it here.', 'We need to make some assumptions about this question before we can answer it. Let’s assume that there are two possible places to purchase a particular item on Amazon and the probability of finding it at location A is 0.6 and B is 0.8. The probability of finding the item on Amazon can be explained as so:', 'We can reword the above as P(A) = 0.6 and P(B) = 0.8. Furthermore, let’s assume that these are independent events, meaning that the probability of one event is not impacted by the other. We can then use the formula…', 'P(A or B) = P(A) + P(B) — P(A and B)P(A or B) = 0.6 + 0.8 — (0.6*0.8)P(A or B) = 0.92', 'Check out the Amazon data scientist interview guide here.', 'This can be answered using the Bayes Theorem. The extended equation for the Bayes Theorem is the following:', 'Assume that the probability of picking the unfair coin is denoted as P(A) and the probability of flipping 10 heads in a row is denoted as P(B). Then P(B|A) is equal to 1, P(B∣¬A) is equal to 0.⁵¹⁰, and P(¬A) is equal to 0.99.', 'If you fill in the equation, then P(A|B) = 0.9118 or 91.18%.', 'A convex function is one where a line drawn between any two points on the graph lies on or above the graph. It has one minimum.', 'A non-convex function is one where a line drawn between any two points on the graph may intersect other points on the graph. It characterized as “wavy”.', 'When a cost function is non-convex, it means that there’s a likelihood that the function may find local minima instead of the global minimum, which is typically undesired in machine learning models from an optimization perspective.', 'For this, I’m going to look at the eight rules of probability laid out here and the four different counting methods (see more here).', 'Eight rules of probability', 'Counting Methods', 'Factorial Formula: n! = n x (n -1) x (n — 2) x … x 2 x 1Use when the number of items is equal to the number of places available.Eg. Find the total number of ways 5 people can sit in 5 empty seats.= 5 x 4 x 3 x 2 x 1 = 120', 'Fundamental Counting Principle (multiplication)This method should be used when repetitions are allowed and the number of ways to fill an open place is not affected by previous fills.Eg. There are 3 types of breakfasts, 4 types of lunches, and 5 types of desserts. The total number of combinations is = 5 x 4 x 3 = 60', 'Permutations: P(n,r)= n! / (n−r)!This method is used when replacements are not allowed and order of item ranking matters.Eg. A code has 4 digits in a particular order and the digits range from 0 to 9. How many permutations are there if one digit can only be used once?P(n,r) = 10!/(10–4)! = (10x9x8x7x6x5x4x3x2x1)/(6x5x4x3x2x1) = 5040', 'Combinations Formula: C(n,r)=(n!)/[(n−r)!r!]This is used when replacements are not allowed and the order in which items are ranked does not mater.Eg. To win the lottery, you must select the 5 correct numbers in any order from 1 to 52. What is the number of possible combinations?C(n,r) = 52! / (52–5)!5! = 2,598,960', 'Brilliant provides a great definition of Markov chains (here):', '“A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed.”', 'The actual math behind Markov chains requires knowledge on linear algebra and matrices, so I’ll leave some links below in case you want to explore this topic further on your own.', 'See more here or here.', 'The box with 24 red cards and 24 black cards has a higher probability of getting two cards of the same color. Let’s walk through each step.', 'Let’s say the first card you draw from each deck is a red Ace.', 'This means that in the deck with 12 reds and 12 blacks, there’s now 11 reds and 12 blacks. Therefore your odds of drawing another red are equal to 11/(11+12) or 11/23.', 'In the deck with 24 reds and 24 blacks, there would then be 23 reds and 24 blacks. Therefore your odds of drawing another red are equal to 23/(23+24) or 23/47.', 'Since 23/47 > 11/23, the second deck with more cards has a higher probability of getting the same two cards.', 'Edit: Thank you guys for commenting and pointing out that it should be -$35!', 'This isn’t a trick question. The answer is simply to perform a hypothesis test:', 'Learn more about hypothesis testing here.', 'Since a coin flip is a binary outcome, you can make an unfair coin fair by flipping it twice. If you flip it twice, there are two outcomes that you can bet on: heads followed by tails or tails followed by heads.', 'P(heads) * P(tails) = P(tails) * P(heads)', 'This makes sense since each coin toss is an independent event. This means that if you get heads → heads or tails → tails, you would need to reflip the coin.', 'You can tell that this question is related to Bayesian theory because of the last statement which essentially follows the structure, “What is the probability A is true given B is true?” Therefore we need to know the probability of it raining in London on a given day. Let’s assume it’s 25%.', 'P(A) = probability of it raining = 25%P(B) = probability of all 3 friends say that it’s rainingP(A|B) probability that it’s raining given they’re telling that it is rainingP(B|A) probability that all 3 friends say that it’s raining given it’s raining = (2/3)³ = 8/27', 'Step 1: Solve for P(B)P(A|B) = P(B|A) * P(A) / P(B), can be rewritten asP(B) = P(B|A) * P(A) + P(B|not A) * P(not A)P(B) = (2/3)³ * 0.25 + (1/3)³ * 0.75 = 0.25*8/27 + 0.75*1/27', 'Step 2: Solve for P(A|B)P(A|B) = 0.25 * (8/27) / ( 0.25*8/27 + 0.75*1/27)P(A|B) = 8 / (8 + 3) = 8/11', 'Therefore, if all three friends say that it’s raining, then there’s an 8/11 chance that it’s actually raining.', 'Since these events are not independent, we can use the rule:P(A and B) = P(A) * P(B|A) ,which is also equal toP(not A and not B) = P(not A) * P(not B | not A)', 'For example:', 'P(not 4 and not yellow) = P(not 4) * P(not yellow | not 4)P(not 4 and not yellow) = (36/39) * (27/36)P(not 4 and not yellow) = 0.692', 'Therefore, the probability that the cards picked are not the same number and the same color is 69.2%.', 'You would perform hypothesis testing to determine statistical significance. First, you would state the null hypothesis and alternative hypothesis. Second, you would calculate the p-value, the probability of obtaining the observed results of a test assuming that the null hypothesis is true. Last, you would set the level of the significance (alpha) and if the p-value is less than the alpha, you would reject the null — in other words, the result is statistically significant.', 'A long-tailed distribution is a type of heavy-tailed distribution that has a tail (or tails) that drop off gradually and asymptotically.', '3 practical examples include the power law, the Pareto principle (more commonly known as the 80–20 rule), and product sales (i.e. best selling products vs others).', 'It’s important to be mindful of long-tailed distributions in classification and regression problems because the least frequently occurring values make up the majority of the population. This can ultimately change the way that you deal with outliers, and it also conflicts with some machine learning techniques with the assumption that the data is normally distributed.', 'Statistics How To provides the best definition of CLT, which is:', '“The central limit theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.” [1]', 'The central limit theorem is important because it is used in hypothesis testing and also to calculate confidence intervals.', '‘Statistical power’ refers to the power of a binary hypothesis, which is the probability that the test rejects the null hypothesis given that the alternative hypothesis is true. [2]', 'Selection bias is the phenomenon of selecting individuals, groups or data for analysis in such a way that proper randomization is not achieved, ultimately resulting in a sample that is not representative of the population.', 'Understanding and identifying selection bias is important because it can significantly skew results and provide false insights about a particular population group.', 'Types of selection bias include:', 'Handling missing data can make selection bias worse because different methods impact the data in different ways. For example, if you replace null values with the mean of the data, you adding bias in the sense that you’re assuming that the data is not as spread out as it might actually be.', 'Observational data comes from observational studies which are when you observe certain variables and try to determine if there is any correlation.', 'Experimental data comes from experimental studies which are when you control certain variables and hold them constant to determine if there is any causality.', 'An example of experimental design is the following: split a group up into two. The control group lives their lives normally. The test group is told to drink a glass of wine every night for 30 days. Then research can be conducted to see how wine affects sleep.', 'Mean imputation is the practice of replacing null values in a data set with the mean of the data.', 'Mean imputation is generally bad practice because it doesn’t take into account feature correlation. For example, imagine we have a table showing age and fitness score and imagine that an eighty-year-old has a missing fitness score. If we took the average fitness score from an age range of 15 to 80, then the eighty-year-old will appear to have a much higher fitness score that he actually should.', 'Second, mean imputation reduces the variance of the data and increases bias in our data. This leads to a less accurate model and a narrower confidence interval due to a smaller variance.', 'An outlier is a data point that differs significantly from other observations.', 'Depending on the cause of the outlier, they can be bad from a machine learning perspective because they can worsen the accuracy of a model. If the outlier is caused by a measurement error, it’s important to remove them from the dataset. There are a couple of ways to identify outliers:', 'Z-score/standard deviations: if we know that 99.7% of data in a data set lie within three standard deviations, then we can calculate the size of one standard deviation, multiply it by 3, and identify the data points that are outside of this range. Likewise, we can calculate the z-score of a given point, and if it’s equal to +/- 3, then it’s an outlier.Note: that there are a few contingencies that need to be considered when using this method; the data must be normally distributed, this is not applicable for small data sets, and the presence of too many outliers can throw off z-score.', 'Interquartile Range (IQR): IQR, the concept used to build boxplots, can also be used to identify outliers. The IQR is equal to the difference between the 3rd quartile and the 1st quartile. You can then identify if a point is an outlier if it is less than Q1–1.5*IRQ or greater than Q3 + 1.5*IQR. This comes to approximately 2.698 standard deviations.', 'Other methods include DBScan clustering, Isolation Forests, and Robust Random Cut Forests.', 'An inlier is a data observation that lies within the rest of the dataset and is unusual or an error. Since it lies in the dataset, it is typically harder to identify than an outlier and requires external data to identify them. Should you identify any inliers, you can simply remove them from the dataset to address them.', 'There are several ways to handle missing data:', 'The best method is to delete rows with missing data as it ensures that no bias or variance is added or removed, and ultimately results in a robust and accurate model. However, this is only recommended if there’s a lot of data to start with and the percentage of missing values is low.', 'First I would conduct EDA — Exploratory Data Analysis to clean, explore, and understand my data. See my article on EDA here. As part of my EDA, I could compose a histogram of the duration of calls to see the underlying distribution.', 'My guess is that the duration of calls would follow a lognormal distribution (see below). The reason that I believe it’s positively skewed is because the lower end is limited to 0 since a call can’t be negative seconds. However, on the upper end, it’s likely for there to be a small proportion of calls that are extremely long relatively.', 'You could use a QQ plot to confirm whether the duration of calls follows a lognormal distribution or not. See here to learn more about QQ plots.', 'Administrative datasets are typically datasets used by governments or other organizations for non-statistical reasons.', 'Administrative datasets are usually larger and more cost-efficient than experimental studies. They are also regularly updated assuming that the organization associated with the administrative dataset is active and functioning. At the same time, administrative datasets may not capture all of the data that one may want and may not be in the desired format either. It is also prone to quality issues and missing entries.', 'There are a number of potential reasons for a spike in photo uploads:', 'The method of testing depends on the cause of the spike, but you would conduct hypothesis testing to determine if the inferred cause is the actual cause.', 'Root cause analysis: a method of problem-solving used for identifying the root cause(s) of a problem [5]', 'Correlation measures the relationship between two variables, range from -1 to 1. Causation is when a first event appears to have caused a second event. Causation essentially looks at direct relationships while correlation can look at both direct and indirect relationships.', 'Example: a higher crime rate is associated with higher sales in ice cream in Canada, aka they are positively correlated. However, this doesn’t mean that one causes another. Instead, it’s because both occur more when it’s warmer outside.', 'You can test for causation using hypothesis testing or A/B testing.', 'When there are a number of outliers that positively or negatively skew the data.', 'There are 4 combinations of rolling a 4 (1+3, 3+1, 2+2):P(rolling a 4) = 3/36 = 1/12', 'There are combinations of rolling an 8 (2+6, 6+2, 3+5, 5+3, 4+4):P(rolling an 8) = 5/36', 'The Law of Large Numbers is a theory that states that as the number of trials increases, the average of the result will become closer to the expected value.', 'Eg. flipping heads from fair coin 100,000 times should be closer to 0.5 than 100 times.', 'You can use the margin of error (ME) formula to determine the desired sample size.', 'Potential biases include the following:', 'There are many things that you can do to control and minimize bias. Two common things include randomization, where participants are assigned by chance, and random sampling, sampling in which each member has an equal probability of being chosen.', 'A confounding variable, or a confounder, is a variable that influences both the dependent variable and the independent variable, causing a spurious association, a mathematical relationship in which two or more variables are associated but not causally related.', 'A/B testing is a form of hypothesis testing and two-sample hypothesis testing to compare two versions, the control and variant, of a single variable. It is commonly used to improve and optimize user experience and marketing.', 'Check out my article, A Simple Guide to A/B Testing for Data Science.', 'You can use hypothesis testing to prove that males are taller on average than females.', 'The null hypothesis would state that males and females are the same height on average, while the alternative hypothesis would state that the average height of males is greater than the average height of females.', 'Then you would collect a random sample of heights of males and females and use a t-test to determine if you reject the null or not.', 'Since we looking at the number of events (# of infections) occurring within a given timeframe, this is a Poisson distribution question.', 'Null (H0): 1 infection per person-daysAlternative (H1): >1 infection per person-days', 'k (actual) = 10 infectionslambda (theoretical) = (1/100)*1787p = 0.032372 or 3.2372% calculated using .poisson() in excel or ppois in R', 'Since p-value < alpha (assuming 5% level of significance), we reject the null and conclude that the hospital is below the standard.', 'Use the General Binomial Probability formula to answer this question:', 'p = 0.8n = 5k = 3,4,5', 'P(3 or more heads) = P(3 heads) + P(4 heads) + P(5 heads) = 0.94 or 94%', 'Using Excel…p =1-norm.dist(1200, 1020, 50, true)p= 0.000159', 'x = 3mean = 2.5*4 = 10', 'using Excel…', 'p = poisson.dist(3,10,true)p = 0.010336', 'Precision = Positive Predictive Value = PVPV = (0.001*0.997)/[(0.001*0.997)+((1–0.001)*(1–0.985))]PV = 0.0624 or 6.24%', 'See more about this equation here.', 'p-hat = 60/100 = 0.6z* = 1.96n = 100This gives us a confidence interval of [50.4,69.6]. Therefore, given a confidence interval of 95%, if you are okay with the worst scenario of tying then you can relax. Otherwise, you cannot relax until you got 61 out of 100 to claim yes.', 'Therefore the confidence interval = 100 +/- 19.6 = [964.8, 1435.2]', 'Therefore the confidence interval = 115+/- 21.45 = [93.55, 136.45]. Since 99 is within this confidence interval, we can assume that this change is not very noteworthy.', 'Using the General Addition Rule in probability:P(mother or father) = P(mother) + P(father) — P(mother and father)P(mother) = P(mother or father) + P(mother and father) — P(father)P(mother) = 0.17 + 0.06–0.12P(mother) = 0.11', 'Since 70 is one standard deviation below the mean, take the area of the Gaussian distribution to the left of one standard deviation.', '= 2.3 + 13.6 = 15.9%', 'Given a confidence level of 95% and degrees of freedom equal to 8, the t-score = 2.306', 'Confidence interval = 1100 +/- 2.306*(30/3)Confidence interval = [1076.94, 1123.06]', 'Upper bound = mean + t-score*(standard deviation/sqrt(sample size))0 = -2 + 2.306*(s/3)2 = 2.306 * s / 3s = 2.601903Therefore the standard deviation would have to be at least approximately 2.60 for the upper bound of the 95% T confidence interval to touch 0.', 'See here for full tutorial on finding the Confidence Interval for Two Independent Samples.', 'Confidence Interval = mean +/- t-score * standard error (see above)', 'mean = new mean — old mean = 3–5 = -2', 't-score = 2.101 given df=18 (20–2) and confidence interval of 95%', 'standard error = sqrt((0.⁶²*9+0.⁶⁸²*9)/(10+10–2)) * sqrt(1/10+1/10)standard error = 0.352', 'confidence interval = [-2.75, -1.25]', 'Assuming we subtract in this order (New System — Old System):', 'confidence interval formula for two independent samples', 'mean = new mean — old mean = 4–6 = -2', 'z-score = 1.96 confidence interval of 95%', 'st. error = sqrt((0.⁵²*99+²²*99)/(100+100–2)) * sqrt(1/100+1/100)standard error = 0.205061lower bound = -2–1.96*0.205061 = -2.40192upper bound = -2+1.96*0.205061 = -1.59808', 'confidence interval = [-2.40192, -1.59808]', 'Write a SQL query to get the second highest salary from the Employee table. For example, given the Employee table below, the query should return 200 as the second highest salary. If there is no second highest salary, then the query should return null.', 'This query says to choose the MAX salary that isn’t equal to the MAX salary, which is equivalent to saying to choose the second-highest salary!', 'Here are three SQL concepts to review before your next interview!', 'Write a SQL query to find all duplicate emails in a table named Person.', 'First, a subquery is created to show the count of the frequency of each email. Then the subquery is filtered WHERE the count is greater than 1.', ""Given a Weather table, write a SQL query to find all dates' Ids with higher temperature compared to its previous (yesterday's) dates."", 'In plain English, the query is saying, Select the Ids where the temperature on a given day is greater than the temperature yesterday.', 'The Employee table holds all employees. Every employee has an Id, a salary, and there is also a column for the department Id.', 'The Department table holds all departments of the company.', 'Write a SQL query to find employees who have the highest salary in each of the departments. For the above tables, your SQL query should return the following rows (order of rows does not matter).', ""Mary is a teacher in a middle school and she has a table seat storing students' names and their corresponding seat ids. The column id is a continuous increment. Mary wants to change seats for the adjacent students."", 'Can you write a SQL query to output the result for Mary?', 'For the sample input, the output is:', 'Note:If the number of students is odd, there is no need to change the last one’s seat.', 'Two weighings would be required (see part A and B above):', 'I’m not 100% sure about the answer to this question but will give my best shot!', 'Let’s take the instance where there’s an increase in the prime membership fee — there are two parties involved, the buyers and the sellers.', 'For the buyers, the impact of an increase in a prime membership fee ultimately depends on the price elasticity of demand for the buyers. If the price elasticity is high, then a given increase in price will result in a large drop in demand and vice versa. Buyers that continue to purchase a membership fee are likely Amazon’s most loyal and active customers — they are also likely to place a higher emphasis on products with prime.', 'Sellers will take a hit, as there is now a higher cost of purchasing Amazon’s basket of products. That being said, some products will take a harder hit while others may not be impacted. It is likely that premium products that Amazon’s most loyal customers purchase would not be affected as much, like electronics.', 'There are a number of possible variables that can cause such a discrepancy that I would check to see:', 'Check out more Facebook data science interview questions here', 'Generally, you would want to probe the interviewer for more information but let’s assume that this is the only information that he/she is willing to give.', 'Focusing on likes per user, there are two reasons why this would have gone up. The first reason is that the engagement of users has generally increased on average over time — this makes sense because as time passes, active users are more likely to be loyal users as using the platform becomes a habitual practice. The other reason why likes per user would increase is that the denominator, the total number of users, is decreasing. Assuming that users that stop using the platform are inactive users, aka users with little engagement and fewer likes than average, this would increase the average number of likes per user.', 'The explanation above can also be applied to minutes spent on the platform. Active users are becoming more engaged over time, while users with little usage are becoming inactive. Overall the increase in engagement outweighs the users with little engagement.', 'To take it a step further, it’s possible that the ‘users with little engagement’ are bots that Facebook has been able to detect. But over time, Facebook has been able to develop algorithms to spot and remove bots. If were a significant number of bots before, this can potentially be the root cause of this phenomenon.', 'The total number of likes in a given year is a function of the total number of users and the average number of likes per user (which I’ll refer to as engagement).', 'Some potential reasons for an increase in the total number of users are the following: users acquired due to international expansion and younger age groups signing up for Facebook as they get older.', 'Some potential reasons for an increase in engagement are an increase in usage of the app from users that are becoming more and more loyal, new features and functionality, and an improved user experience.', 'The metrics that determine a product’s success are dependent on the business model and what the business is trying to achieve through the product. The book Lean analytics lays out a great framework that one can use to determine what metrics to use in a given scenario:', 'You can perform an A/B test by splitting the users into two groups: a control group with the normal number of ads and a test group with double the number of ads. Then you would choose the metric to define what a “good idea” is. For example, we can say that the null hypothesis is that doubling the number of ads will reduce the time spent on Facebook and the alternative hypothesis is that doubling the number of ads won’t have any impact on the time spent on Facebook. However, you can choose a different metric like the number of active users or the churn rate. Then you would conduct the test and determine the statistical significance of the test to reject or not reject the null.', 'Lift: lift is a measure of the performance of a targeting model measured against a random choice targeting model; in other words, lift tells you how much better your model is at predicting things than if you had no model.', 'KPI: stands for Key Performance Indicator, which is a measurable metric used to determine how well a company is achieving its business objectives. Eg. error rate.', 'Robustness: generally robustness refers to a system’s ability to handle variability and remain effective.', 'Model fitting: refers to how well a model fits a set of observations.', 'Design of experiments: also known as DOE, it is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variable. [4] In essence, an experiment aims to predict an outcome based on a change in one or more inputs (independent variables).', '80/20 rule: also known as the Pareto principle; states that 80% of the effects come from 20% of the causes. Eg. 80% of sales come from 20% of customers.', 'Quality assurance: an activity or set of activities focused on maintaining a desired level of quality by minimizing mistakes and defects.', 'Six sigma: a specific type of quality assurance methodology composed of a set of techniques and tools for process improvement. A six sigma process is one in which 99.99966% of all outcomes are free of defects.', '[1] Central Limit Theorem, Definition and Examples in Easy Steps, Statistics How To', '[2] Power, Statistics, Wikipedia', '[3] Anthropic principle, Wikipedia', '[4] Design of experiments, Wikipedia', '[5] Root cause analysis, Wikipedia', 'Check out the Interview Query Youtube channel for more data science mock interviews and example solutions.']"
03/2020,from sklearn import *,and other dead-giveaways that you’re a fake data scientist,5K,24,https://towardsdatascience.com/@conor.lazarou,https://towardsdatascience.com/from-sklearn-import-478c711dafa1?source=collection_archive---------3-----------------------,9,5,"['from sklearn import *', '1. You Don’t Bother with Data Exploration', '2: You Fail to Choose an Appropriate Model Type', '3: You Don’t Use Effective Metrics and Controls', '…to import tensorflow as tf']",25,"['These days it seems like everyone and their dog are marketing themselves as data scientists—and you can hardly blame them, with “data scientist” being declared the Sexiest Job of the Century and carrying the salary to boot. Still, blame them we will, since many of these posers grift their way from company to company despite having little or no practical experience and even less of a theoretical foundation. In my experiences interviewing and collaborating with current and prospective data scientists, I’ve found a handful of tells that separate the posers from the genuine articles. I don’t mean to belittle self-taught and aspiring data scientists — in fact, I think this field is especially appropriate for passionate self-learners —but I definitely mean to belittle the sort of person who takes a single online course and ever after styles themselves an expert, despite having no knowledge of (or interest in) the fundamental theory of the field. I’ve compiled this list of tells so that, if you’re a hiring manager and don’t know what you’re looking for in a data scientist, you can filter out the slag, and if you’re an aspiring data scientist and any of these resonate with you, you can fix them before you turn into a poser yourself. Here are three broad domains of data science faux pas with specific examples that will land your resume in the bin.', 'Data exploration is the first step in any machine learning project. If you don’t take the time to familiarize yourself with your data and get well-acquainted with its features and quirks, you’re going to waste a significant amount of time barking up the wrong decision tree before arriving at a usable product — if you even make it there.', 'Exploratory data visualization is the best way to start any data-related project. If you’re applying machine learning, it’s likely that you’re working with a high volume of high-dimensional data; perusing a .csv in Excel or running a df.describe() is not a suitable alternative to proper data visualization. Francis Anscombe illustrated the importance of data visualization with his famous quartet:', 'The datasets in each panel all have essentially identical summary statistics: the x and y means, x and y sample variances, correlation coefficients, R-squared values, and lines of best fit are all (nearly) identical. If you don’t visualize your data and rely on summary stats, you might think these four datasets have the same distribution, when a cursory glance shows that this is obviously not the case.', 'Data visualization allows you to identify trends, artifacts, outliers, and distributions in your data; if you skip this step, you might as well do the rest of the project blindfolded, too.', 'Data is messy: values get entered wrong; conversions run awry; sensors go spastic. It’s important that you resolve these issues before you waste months and months on a dead-end project, and it’s mission critical that you resolve them before pushing your models to production. Remember: garbage in ⇒ garbage out.', 'There are lots of good ways to identify problems with your data and no good ways to identify them all. Data visualization is a good first step (have I mentioned this?), and although it can be a tedious and manual process it pays for itself many times over. Other methods include automatic outlier detection and conditional summary stats.', 'For an example, consider this histogram of human heights:', 'Training a model with this data would doubtless lead to poor results. But, by inspecting the data, we find that the 100 “outliers” in fact had their height entered in metres rather than centimetres. This can be correcting by multiplying these values by 100. Properly cleaning the data not only prevents the model from being trained on bad data, but, in this case, let us salvage 100 data points that might otherwise have been thrown out. If you don’t clean your data properly, you’re leaving money on the table at best and building a defective model at worst.', 'One of the cool things about neural networks is that you can often throw all your raw data at it and it will learn some approximation of your target function. Sorry, typo, I meant one of the worst things. It’s convenient, sure, but inefficient and brittle. Worst of all, it makes beginner data scientists reliant on deep learning when it’s often the case that a more traditional machine learning approach would be appropriate, sending them on a slow descent to poserdom. There’s no “right” way to do feature selection and engineering, but there are a few key outcomes to strive for:', 'Most laypeople think that machine learning is all about black boxes that magically churn out results from raw data; please don’t contribute to this misconception.', 'Machine learning is a broad field with a rich history, and for much of that history it went by the name “statistical learning”. With the advent of easy-to-use open source machine learning tools like Scikit-Learn and TensorFlow, combined with the deluge of data we now collect and a ubiquity of fast computers, it’s never been easier to experiment with different ML model types. However, it’s not a coincidence that removing the requirement that ML practitioners actually understand how different model types work has led to many ML practitioners not understanding how different model types work.', 'The github repos of aspiring data scientists are littered with Kaggle projects and online course assignments-come-portfolios that look like this:', 'This is an obvious giveaway that you don’t understand what you’re doing, and it’s a damned shame that so many online courses recommend this course of action. It’s a waste of time and easily leads to inappropriate model types being selected because they happened to work well on the validation data (you remembered to hold out a validation set, right? Right?). The type of model used should be selected based on the underlying data and the needs of the application, and the data should be engineered to match the chosen model. Selecting a model type is an important part of the data science process, and direct comparison between a handful of appropriate models may be warranted, but blindly applying every tool you can in order to find the one with “the best number” is a major red flag. In particular, this belies an underlying problem which is that…', 'Why might a KNN classifier not work so well if your inputs are “car age in years” and “kilometres traveled”? What’s the problem with applying linear regression to predict global population growth? Why isn’t my random forest classifier working on my dataset with a 1000-category one-hot-encoded variable? If you can’t answer those questions, that’s okay! There are lots of great resources to learn how each of these techniques work; just be sure to read and understand them before you apply for a job in the field.', 'The bigger problem here isn’t that people don’t know how different ML models work, it’s that they don’t care and aren’t interested in the underlying math. If you like machine learning but don’t like math, you don’t really like machine learning; you have a crush on what you think it is. If you don’t care to learn how models work or are fit to data, then you’ll have no hope of troubleshooting them when they inevitably go awry. The problem is only exacerbated when…', 'All model types have their pros and cons. An important trade-off in machine learning is that between accuracy and interpretability. You can have a model that does a poor job of making predictions but is easy to understand and effectively explains the process, you can have a black box which is very accurate but whose inner workings are an enigma, or you can land somewhere in the middle.', 'Which type of model you choose should be informed by which of these two traits is more important for your application. If the intent is to model the data and gain actionable insights, then an interpretable model, such as a decision tree or linear regression, is the obvious choice. If the application is production-level prediction such as image annotation, then interpretability takes a backseat to accuracy and a random forest or neural network is likely more appropriate.', 'In my experience, data scientists who don’t understand this trade-off and those who beeline for accuracy without even considering why interpretability matters are not the sort you want training models for you.', 'Despite making up 50% of the words and 64% of the letters, the “science” component of data science is often ignored. It’s not uncommon for poser data scientists to blindly apply a single metric in a vacuum as their model evaluation. Unwitting stakeholders are easily wowed by bold claims like “90% accuracy” which are technically correct but wildly inappropriate for the task at hand.', 'I have a test for pancreatic cancer which is over 99% accurate. Incredible, right? Well, it’s true, and you can try it for yourself by clicking this link.', 'If you saw a red circle with a line through it, you’ve tested negative. If you saw a green check mark, you’re lying. The point is, 99% of people don’t have pancreatic cancer (more, actually, but let’s just assume it’s 99% for the sake of this example), so my silly little “test” is accurate 99% of the time. Therefore, if accuracy is what we care about, any machine learning model used for diagnosing pancreatic cancer should perform at least as well as this uninformative, baseline model. If the hotshot you’ve hired fresh out of college claims he’s developed a tool with 95% accuracy, compare those results to a baseline model and make sure his model is performing better than chance.', 'Continuing the diagnostic example above, it’s important to make sure you’re using the right metric. For cancer diagnosis, accuracy is actually a bad metric; it’s often preferable to decrease your accuracy if it means an increase in sensitivity. What’s the cost associated with a false positive? Patient stress, as well as wasted time and resources. What’s the cost of a false negative? Death. An understanding of the real-world implications of your model and an appreciation of how those implications govern metric selection clearly delineate real data scientists from their script-kiddie lookalikes.', 'This is a big one, and it’s far too common. Properly testing a model is absolutely essential to the data science process. There are many ways this can go awry: not understanding the difference between validation and test data, performing data augmentation before splitting, not plugging data leaks, ignoring data splitting altogether… There’s not much to say about this other than that if you don’t know or care how to create a proper holdout set, all your work has been a waste of time.', 'These are only a handful of tells that give up the game. With enough experience, they’re easy to spot, but if you’re just starting out in the field it can be hard to separate the Siraj Ravals of the world from the Andrew Ngs. Now, I don’t mean to gatekeep the field to aspiring data scientists; if you feel attacked by any of the above examples, I’m glad to hear it because it means you care about getting things right. Keep studying, keep climbing so that you too can be endlessly irked by the sea of posers.']"
03/2020,Why jK8v!ge4D isn’t a good password,There’s a fundamental issue with password validation,11.4K,136,https://towardsdatascience.com/@jacobbergdahl_47336,https://towardsdatascience.com/why-password-validation-is-garbage-56e0d766c12e?source=collection_archive---------4-----------------------,6,4,"['Why jK8v!ge4D isn’t a good password', 'Forcing your passwords', 'How long does it take to crack?', 'Now that’s a good password']",28,"['Take a look at these two passwords:', 'Which password do you think takes the longest for a computer to crack? And which password do you think is the easiest to remember? The answer to both of these questions is password number 2. Yet people are encouraged to create passwords that look like number 1. People have been taught to write passwords that are difficult for humans to remember, for no real reason.', 'Let’s talk about that.', 'There are many bizarre things in internet standards. Validation is one of them. As a front-end developer, I am expected to validate the input that users enter into so-called input fields. These are the fields that you use when you enter your username, your email, your home address, your postal number, and so on. It is the duty of a front-end developer to ensure that the user doesn’t enter anything malicious or anything improperly formatted into these fields.', 'For example, a field that requests a postal number generally only allows for spaces and numbers, and if we know what country the user lives in, we can also limit it to a certain number of characters. Phone numbers can often include numbers, a plus sign (only at the beginning) and a dash, maybe some parenthesis too if we’re feeling liberal. E-mail addresses are difficult to validate, however a common practice is that they must include an at-sign (@) followed by a period, even though a perfectly valid e-mail address could actually have none of those attributes. Some websites try to validate names by forcing people to keep their names within a certain length or by forcing people to only use certain characters, though such validations never really work as people’s names can be just about anything.', 'Validation is implemented for several reasons. One is security concerns. Validation prevents users from entering scary code into the fields that could alter the database or perform other malicious actions. Another is to force a certain data type. If a field is only supposed to consist of numbers, the database engineer might have set up a database column that only allows for numbers, which means that a symbol that isn’t a number would cause an error to occur.', 'But the primary reason, really, is to help the user avoid making mistakes.', 'For some reason, front-end developers are expected to babysit users into entering what is traditionally conceived to be a good password. It should be at least eight characters long, include uppercase and lowercase characters, a number, and if we’re feeling really obnoxious, it should even include a special character, like an exclamation mark.', 'Here is an example of what is generally considered to be a solid password: jK8v!ge4D. Considering the fact that you are often asked to enter a password like this, it’d be fair of you to assume that we’d consider this a good password.', 'It’s not. It’s stupid. It’s a bad password.', 'First of all, how is anyone supposed to remember that? What ends up happening is that users can’t remember it so they write the passwords down somewhere. Like on a post-it note. And then they end up getting “hacked”.', 'Secondly, users end up using the same password for different services, because it’s obnoxious to keep track of a whole bunch of these complex passwords. When you create an account for a decent website, some magical code behind-the-scenes transforms your password into a hash (commonly and incorrectly referred to as encryption). Your password ends up looking something like this in the database: k5ka2p8bFuSmoVT1tzOyyuaREkkKBcCNqoDKzYiJL9RaE8yMnPgh2XzzF0NDrUhgrcLwg78xs1w5pJiypEdFX. Even if the database is hacked, the hacker cannot really do anything with this information. It is possible to figure out the original password if the password is common enough and the algorithm is simple enough, though with a somewhat decent password that’s been properly hashed, it’s generally quite safe.', 'The issue is that not all services hash their users’ passwords. If you use the same password for many services, you might end up using a poorly programmed service that actually does save your password in plain text in their database. If they end up getting compromised, the hacker suddenly has your password to all of your accounts where you use the same password. This is scary, and it happens a lot more often than one might think.', 'This is why you MUST use different passwords for different websites. However today’s users have accounts on tons and tons of websites. How are they supposed to remember all of their passwords? Power users may use a password tool, but you can’t expect the average user to do that.', 'Well, there’s a better way.', 'Take a look at this string of characters: gtypohgt. It’s eight random characters, all lowercase. It takes no more than a mere couple of minutes for a modern computer to brute force its way through it. Replace some characters with a few numbers, and you’re looking at a password which will take up to an hour to crack (g9ypo3gt). Make a few characters uppercase, and the password will take days to crack (g9YPo3gT). Throw a special sign in there, and it could take up to a month (g9Y!o3gT).', 'g9Y!o3gT is technically a decent password. No person will be able to guess it, it’s not on any short-list of common passwords, and computers will take a reasonable amount of time to crack it. The issue is that this password is hard for a human to remember — for no real reason.', 'Now take a look at this beauty: greenelephantswithtophats (green elephants with top hats). That’s 24 characters, all lowercase. No numbers, no random characters, no shenanigans. Yet this password will take a computer thousands upon thousands of years to crack. See, for every character you add, the time it takes for a computer to crack greatly increases. greenelephantswithtophats is not on any short-list of common passwords, and no human will be able to guess it either.', 'Make a password that tells a story. Need a Facebook password? How about afaceforabookbutapizzaforahorse (a face for a book but a pizza for a horse)? Visualize it. Our spacial memory is our strongest memory. Suddenly, you have an immensely powerful password that’s easy to remember and unique to one particular website. The password must be something that even people who know you very well cannot guess. You don’t talk about turtles often, do you? Have you ever seen a purple turtle? No? Visualize it. You have, now. It’s okay to lie in your passwords: ioncesawapurpleturtleiswear (I once saw a purple turtle I swear). That one will take millions of years for a modern computer to crack, and not even your sister will be able to guess it.', 'These passwords are easy to imagine. flyingcarsthatcannotflyarenotflying. applesmaybegreatbutpearsarelikeheaven. goatswithshoesenjoytrainsonrainydays. No one will guess these.', 'Yet some websites also will not allow these passwords. They will complain that you didn’t add a number or an uppercase character or that it’s too long, or some other nonsensical non-technical reason.', 'So you could cheat the system just a little bit. Add A1! to the end of any of the above passwords, and they’ll be accepted by any system that doesn’t call them too long. You now have an uppercase character, a number, and a special sign. Even if those three are the same on all of your passwords, the rest of the password will make up for it. ioncesawapurpleturtleiswear and ioncesawapurpleturtleiswearA1! are both impossible for a computer to crack, meaning it’s nothing but an inconvenience that you are required to enter those characters at the end.', 'The intention of developers is benign. People enter bad passwords. Website managers don’t want any scandals, so they try to force users to enter decent passwords, however cumbersome that ends up being.', 'Remember this technique next time you need to create a password. Make one that’s hard for computers to crack and easy for you to remember — not the other way around.', 'Oh, but even if you don’t, just promise to stay away from any variation of 123456, password123, and qwerty. Those are actually bad passwords.', 'Well, I guess that’s why we force you to write something like this jK8v!ge4D.', 'Full circle.', 'Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.']"
03/2020,9 Reasons why you’ll never become a Data Scientist,You need to change your mindset,4.3K,38,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/9-reasons-why-youll-never-become-a-data-scientist-c8c5b75503cf?source=collection_archive---------5-----------------------,7,1,['9 Reasons why you’ll never become a Data Scientist'],46,"['Disclaimer: This story is not meant to discourage you. Rather, it should serve as a long hard look in the mirror.', 'So you’re enthusiastic about Data Science, you’ve read a couple dozen blog posts and completed a few online classes. Now you’re dreaming of making this your career. After all, it’s the sexiest job of the 21st century, according to Harvard Business Review.', 'But despite your enthusiasm, Data Science might not be for you. At this moment in time, you’re holding too many illusions and false stereotypes.', 'Now, your task is simple: Remove the things that hold you back! And you’ll be surprised at how fast you move forward.', 'You have a master’s degree in a quantitative field, or maybe even a PhD. Now you want a head start in Data Science.', 'But have you ever used a shell before? Have you felt the intimidation that can come from command-line interfaces when you stumble upon errors? Have you ever worked with big databases — on the scale of Terabytes?', 'If you answer one of these questions with no, you’re not ready yet. You need some real-world experience and build some real projects. Only then will you encounter the type of problems that you’ll face every day as a Data Scientist. And only then will you develop the skills to solve them.', 'Congratulations on your degree. Now get cracking on the hard work.', 'Have you ever invested an entire weekend in a geeky project? Have you ever spent your nights browsing GitHub while your friends were out to party? Have you ever said no to doing your favorite hobby because you’d rather code?', 'If you could answer none of the above with yes, you’re not passionate enough. Data Science is about facing really hard problems and sticking at them until you found a solution. If you’re not passionate enough, you’ll shy away at the sight of the first difficulty.', 'Think about what attracts you to becoming a Data Scientist. Is it the glamorous job title? Or is it the prospect of ploughing through tons of data on the search for insights? If it is the latter, you’re heading in the right direction.', 'Only crazy ideas are good ideas. And as a Data Scientist, you’ll need plenty of those. Not only will you need to be open to unexpected results — they occur a lot!', 'But you’ll also have to develop solutions to really hard problems. This requires a level of extraordinary that you can’t accomplish with normal ideas.', 'If people constantly tell you that you’re off your rocker, you’re heading in the right direction. If not, you’ll need to work on your craziness.', 'This, of course, requires some boldness. Once you let out your eccentricity, some people will scratch their heads and turn their back on you.', 'But it’s worth it. Because you’re being true to yourself. And you’re igniting the spark of awesomeness that you need as a Data Scientist.', 'Don’t get me wrong. Textbooks and online classes are a great way to get started. But only to get started!', 'You need to work on real projects as soon as possible. Of course, there is no point in building a Python project without being able to code a single line in Python. But as soon as you’ve built a modest foundation, get active.', 'Learning by doing is key.', 'Start building your GitHub portfolio. Take part in some Hackathons and Kaggle competitions. And blog about your experiences.', 'Everybody can do textbooks. To be a Data Scientist, you must do more.', 'You’ve subscribed to a couple of online courses on Data Science and are reading a few textbooks. Now you think that once you’ve mastered those, you have learnt enough to break through in Data Science.', 'Wrong. This is yet the beginning. If you think you’re learning a lot now, think about how much you’ll be learning in three years.', 'If you end up as a Data Scientist, you’ll be learning ten times more than you are now. It’s an ever-changing field where new technologies are constantly needed. If you stop learning once you’ve landed your job, your trajectory is going to go from a beginner in Data Science to a Data Scientist that sucks.', 'If you want to excel in Data Science (and if you’re reading this, you do), you need to face the fact that your learning curve will get steeper over time. If you don’t enjoy learning bigly, stop dreaming about being a Data Scientist.', 'So you know a thing or two about Computer Science, and your math skills ain’t that bad. Will you be able to land a job in Data Science?', 'No, you won’t. Your skills in IT and math are essential, but not enough to set yourself aside from all the other Data Science enthusiasts.', 'Data Scientists work in all kinds of companies and all kinds of industries. To deliver key insights for your clients, you need knowledge about their domain.', 'For example, Kate Marie Lewis from the story below landed a position in Data Science in six months. But what made the difference was that as a neuroscientist, she had domain knowledge in healthcare.', 'Which domain are you good at? In which fields do you have experience?', 'Try to position yourself as a specialist in your domain, and less like a general Data Scientist. This is how you really land a job.', 'So you’re more the analytical type. You love numbers and quantitative analyses, and you hate soft skills and human interaction.', 'This doesn’t make you a good Data Scientist, my friend. Soft skills are important even in a quantitative job. Soft skills are what ultimately makes you rock that job interview.', 'Of all the soft skills that you could acquire, it’s your business skills that need a boost. Remember that your clients are business leaders. And as such, they need people who understand business. Only this way can you generate insights that add value to your client.', 'You want to land a job in the field but you don’t know any fellow Data Scientists? It’s time to get cracking, my friend.', 'Go to meetups. Join the relevant groups on LinkedIn. Get to know people on Hackathons. Follow the right people on Twitter. Meet your fellow contributors on that GitHub project. Do something exciting!', 'Like with any job search, 90% of your success isn’t determined by how vast your skills are. It’s determined by who can provide references for you, and who can give you an introduction.', 'If your LinkedIn connections are limited to your mom and your co-workers in that dead-end job, it’s about time to pimp your profile. If your follower count on Twitter is a single handful, get tweeting. If your blog has no readers, try SEOing and cross-platform marketing.', 'The connections will come. But you need to get cracking first.', 'You’ve heard all the buzz about Machine Learning and Artificial Intelligence. You think that Data Science could open the door to working with cutting-edge technologies.', 'Maybe you will. But I guarantee you that you won’t do it more than 5% of your time.', 'Once you’ve landed your dream job, you’ll spend the largest part of your time cleaning data. Congratulations, you just found a new job as a janitor!', 'If you don’t love that, go home — you shouldn’t be reading this post. If you still want to be a Data Scientist after reading all this, it’s about time that you fall in love with the dirty work.', 'Data Scientists are highly sought-after individuals — which makes a lot of people dabble with it. But to get a position in the field, dabbling is not enough. You need to put in the hard work.', 'If you’re still convinced about becoming a Data Scientist after reading this story, congratulations. You might be on a very good path.', 'If at this point you’re unsure about becoming a Data Scientist, identify the biggest reasons for your doubts. Then start working on those points. You can do this!']"
03/2020,Top 3 Python Functions You Don’t Know About (Probably),Cleaner Code and Fewer Loops? Count me in.,4.6K,24,https://towardsdatascience.com/@radecicdario,https://towardsdatascience.com/top-3-python-functions-you-dont-know-about-probably-978f4be1e6d?source=collection_archive---------6-----------------------,4,5,"['Top 3 Python Functions You Don’t Know About (Probably)', 'map()', 'filter()', 'reduce()', 'Before you go']",24,"['As one of the most popular languages of the 21st century, Python certainly has a lot of interesting functions worth exploring and studying in-depth. Three of those will be covered today, each theoretically and through practical examples.', 'The main reason why I want to cover these functions is that they help you avoid writing loops. Loops can be expensive to run in some cases, and alongside that, these functions will help with speed improvement.', 'Here are the functions that the article will cover:', 'Even if you’ve heard of those functions before there’s no harm in reinforcing your knowledge with a bit more theory and examples.', 'So without further ado, let’s get started!', 'The map() function takes in another function as a parameter, alongside an array of some sort. The idea is to apply a function (the one passed in as an argument) to every item in the array.', 'This comes in handy for two reasons:', 'Let’s see it in action. I will declare a function called num_func() that takes one number as a parameter. That number is squared and divided by 2 and returned as such. Note that the operations were chosen arbitrarily, you can do anything you want inside the function:', 'And now let’s declare an array of numbers on which we want to apply num_func(). Note that map() itself will return a map object so you’ll need to convert it to a list:', 'Seems like the process finished successfully. There’s nothing groundbreaking here, but it’s a good thing to avoid loops whenever possible.', 'Here’s another one decent function that will save you time — both on writing and on execution. As the name suggests the idea is to keep in array only the items that satisfy a certain condition.', 'Just like with map(), we can declare the function beforehand, and then pass it to filter() alongside the list of iterables.', 'Let’s see this in action. I’ve gone ahead and declared a function called more_than_15(), which, as the name suggests, will return True if item given as parameter is greater than 15:', 'Next, we declare an array of numbers and pass them as a second parameter in the filter() function:', 'As expected, only three values satisfy the given condition. Once again, nothing groundbreaking here, but looks a lot better than a loop.', 'Now reduce() is a bit different than the previous two. To start out, we have to import it from the functools module. The main idea behind this is that it will apply a given function to the array of items and will return a single value as a result.', 'The last part is crucial — reduce() won’t return an array of items, it always returns a single value. Let’s see a diagram to make this concept concrete.', 'Here’s the logic written out in case diagram isn’t 100% clear:', 'And 70 is the value that gets returned. To start out with the code implementation, let’s import reduce function from functools module and declare a function that returns a sum of two numbers:', 'Now we can revisit the diagram in code, and verify that everything works as it should:', 'Don’t jump into the comment section just yet — I’m perfectly aware that there are other ways to sum items of a list. This is just the most simple example to show how the function works.', 'I hope you can somehow utilize these three functions in your daily routine. The speed improvement might not be drastic — depends on the volume of data you are dealing with — but the code will generally look nicer with fewer loops.', 'If you have some other examples, don’t hesitate to share it in the comment section.', 'Thanks for reading.']"
03/2020,Covid-19 infection in Italy. Mathematical models and predictions,A comparison of logistic and…,519,31,https://towardsdatascience.com/@gianlucamalato,https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d?source=collection_archive---------7-----------------------,6,8,"['Covid-19 infection in Italy. Mathematical models and predictions', 'Data collection', 'Data preparation', 'The logistic model', 'Exponential model', 'Plots', 'Analysis of residuals', 'Which is the right model?']",42,"['The world is fighting against a new enemy in these days, which is the Covid-19 virus.', 'The virus has spread quickly in the world since its first appearance in China. Unfortunately, Italy is recording the highest number of Covid-19 infected people in Europe. We’ve been the first nation facing this new enemy in the Western World and we are all fighting every day against all the economical and social implications of this virus.', 'In this article, I’ll show you a simple mathematical analysis of the infection growth in Python and two models to better understand the evolution of the infection.', 'Every day, the Italian Civil Protection Department refreshes the cumulative data of infected people. This data is publicly available as open data on GitHub here: https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv', 'My goal is to create models of the time series of the total number of infected people to date (i.e. the actually infected people plus the people who have had been infected). These models have parameters, which will be estimated by curve fitting.', 'Let’s do it in Python.', 'First, let’s import some libraries.', 'Now, let’s take a look at the raw data.', 'The column we need is ‘totale_casi’ which contains the cumulative number of infected people to date.', 'This is the raw data everything starts from. Now, let’s prepare it for our analysis.', 'First, we need to change dates into numbers. We’ll take the days since January 1st.', 'We can now analyze the two models I’ll take into the exam, which are the logistic function and the exponential function.', 'Each model has three parameters, that will be estimated by a curve fitting calculation on the historical data.', 'The logistic model has been widely used to describe the growth of a population. An infection can be described as the growth of the population of a pathogen agent, so a logistic model seems reasonable.', 'This formula is very known among data scientists because it’s used in the logistic regression classifier and as an activation function of neural networks.', 'The most generic expression of a logistic function is:', 'In this formula, we have the variable x that is the time and three parameters: a,b,c.', 'At high time values, the number of infected people gets closer and closer to c and that’s the point at which we can say that the infection has ended. This function has also an inflection point at b, that is the point at which the first derivative starts to decrease (i.e. the peak after which the infection starts to become less aggressive and decreases).', 'Let’s define it in python.', 'We can use the curve_fit function of scipy library to estimate the parameter values and errors\xa0starting\xa0from\xa0the\xa0original\xa0data.', 'Here are the values:', 'The function returns the covariance matrix too, whose diagonal values are the variances of the parameters. Taking their square root we can calculate the standard errors.', 'These numbers give us many useful insights.', 'The expected number of infected people at infection end is 15968+/- 4174.', 'The infection peak is expected around 9 March 2020.', 'The expected infection end can be calculated as that particular day at which the cumulative infected people count is equal to the c parameter rounded to the nearest integer.', 'We can use the fsolve function of scipy to numerically find the root of the equation that defines the infection end day.', 'It’s on 15 April 2020.', 'While the logistic model describes ain infection growth that is going to stop in the future, The exponential model describes an unstoppable infection growth. For example, if a patient infects 2 patients per day, after 1 day we’ll have 2 infections, 4 after 2 days, 8 after 3 and so on.', 'The most generic exponential function is:', 'The variable x is the time and we still have the parameters a, b, c. The meaning, however, is different from the logistic function parameters’.', 'Let’s define the function in Python and let’s perform the same curve fitting procedure used for logistic growth.', 'Parameters and their standard errors are:', 'We have now all the necessary data to visualize our results.', 'Both theoretical curves seem to approximate the experimental trend quite well. Which one does it better? Let’s take a look at the residuals.', 'Residuals are the differences between each experimental point and the corresponding theoretical point. We can analyze the residuals of both models in order to verify the best fitting curve. In a first approximation, the lower Mean Squared Error between theoretical and experimental data, the better the fit.', 'Logistic model MSE: 8254.07', 'Exponential model MSE: 16219.82', 'Residuals analysis seems to point toward the logistic model. It’s very likely because the infection should end someday in the future; even if everybody will be infected, they’ll develop the proper immunity defense to avoid a second infection. That’s right as long as the virus doesn’t mutate too much (as, for example, influenza virus).', 'But there’s something that still worries me. I’ve been fitting the logistic curve every day since the beginning of the infection and every day I got different parameter values. The number of infected people at the end increases, the maximum infection day is often the current day or the next day (which is compatible with the standard error of 1 day on this parameter). That’s why I think that, although the logistic model seems to be the most reasonable one, the shape of the curve will probably change due to exogenous effects like new infection hotspots, government actions to bind the infection and so on.', 'That’s why I think that the predictions of this model will start to become useful only within a few weeks, reasonably after the infection peak.', 'Note from the editors: towardsdatascience.com is a Medium publication primarily based on the study of data science and machine learning. We aren’t health professionals or epidemiologists. To learn more about the coronavirus pandemic, you can click here.']"
03/2020,Top Programming Languages for AI Engineers in 2020,"From several programming languages, AI engineers…",460,7,https://towardsdatascience.com/@harish_6956,https://towardsdatascience.com/top-programming-languages-for-ai-engineers-in-2020-33a9f16a80b0?source=collection_archive---------8-----------------------,9,10,"['Top Programming Languages for AI Engineers in 2020', '● Python', '● Java', '● R', '● Prolog', '●Lisp', '● Haskell', '● Julia', 'Conclusion', 'Similar Articles —']",93,"['Artificial Intelligence has now become an integral part of our daily lives with all the benefits it provides over hundreds of unique use cases and situations, not to mention how simple and easy it has made things for us.', 'With the boost in recent years, AI has come a long way to help businesses grow and achieve their full potential. These advancements in AI would not have been possible without the core improvements in the underlying programming languages.', 'With the boom in AI, the need for efficient and skilled programmers and engineers skyrocketed along with improvements in programming languages. While there are plenty of programming languages to get you started with developing on AI, no single programming language is a one-stop-solution for AI programming as various objectives require a specific approach for every project.', 'We will discuss some of the most popular ones listed below and leave the decision making up to you —', 'Python is the most powerful language you can still read.- Pau Dubois', 'Developed in 1991, Python has been A poll that suggests over 57% of developers are more likely to pick Python over C++ as their programming language of choice for developing AI solutions. Being easy-to-learn, Python offers an easier entry into the world of AI development for programmers and data scientists alike.', 'Python is an experiment in how much freedom programmers need. Too much freedom and nobody can read another’s code; too little and expressiveness is endangered.', '- Guido van Rossum', 'With Python, you not only get excellent community support and an extensive set of libraries but also enjoy the flexibility provided by the programming language. Some of the features that you may benefit the most from Python are platform independence and extensive frameworks for Deep Learning and Machine Learning.', 'The joy of coding Python should be in seeing short, concise, readable classes that express a lot of action in a small amount of clear code — not in reams of trivial code that bores the reader to death.', '- Guido van Rossum', 'Python Code Snippet Example:', '● TensorFlow, for machine learning workloads and working with datasets', '● scikit-learn, for training machine learning models', '● PyTorch, for computer vision and natural language processing', '● Keras, as the code interface for highly complex mathematical calculations and operations', '● SparkMLlib, like Apache Spark’s Machine Learning library, making machine learning easy for everyone with tools like algorithms and utilities', '● MXNet, as another one of Apache’s library for easing deep learning workflows', '● Theano, as the library for defining, optimizing and evaluating mathematical expressions', '● Pybrain, for powerful machine learning algorithms', 'Also, Python has surpassed Java and became the 2nd most popular language according to GitHub repositories contributions. In fact, Stack Overflow calls it the “fastest growing” major programming language.”', 'Python Courses for Beginners —', 'Write once, run anywhere', 'Java is considered one of the best programming languages in the world and the last 20 years of its use is proof of that.', 'With its high user-friendliness, flexible nature and platform independence, Java has been used for developing for AI in various ways, read on to know about some of them:', '● TensorFlowTensorFlow’s list of supported programming languages also includes Java with an API. The support isn’t as feature-rich as other fully supported languages, but it’s there and is being improved at a rapid pace.', '● Deep Java Library Built by Amazon to create and deploy deep learning abilities using Java.', '● KubeflowKubeflow facilitates easy deployment and management of Machine Learning stacks on Kubernetes, providing ready to use ML solutions.', '● OpenNLP Apache’s OpenNLP is a machine learning tool for natural language processing.', '● Java Machine Learning Library Java-ML provides developers with several machine learning algorithms.', '● Neuroph Neuroph makes designing neural networks using the open-source framework of Java possible with the help of Neuroph GUI.', 'If Java had true garbage collection, most programs would delete themselves upon execution.- Robert Sewell', 'Java Code Snippet Example:', 'Java Courses for Beginners —', 'R was created by Ross Ihaka and Robert Gentleman with the first version being launched in 1995. Currently being maintained by the R Development Core Team, R is the implementation of S programming language and aids in developing statistical software and data analysis.', 'The qualities that are making R a good fit for AI programming among developers are:', '● The fundamental feature of R being good at crunching huge numbers puts it in a better position than Python with its comparatively unrefined NumPy package.', '● With R, you can work on various paradigms of programming such as functional programming, vectorial computation and object-oriented programming.', 'Some of the AI programming packages available for R are:', '● Gmodels provides a collection of several tools for model fitting', '● Tm, as a framework for text mining applications  ● RODBC as an ODBC interface for R', '● OneR, for implementing One Rule Machine Learning classification algorithm, useful for machine learning models', 'Used widely among Data Miners and Statisticians, features provided by R are:', '● Wide variety of libraries and packages to extend its functionalities', '● Active and supportive community', '● Able to work in tandem with C, C++ and Fortran', '● Several packages help extend the functionalities', '● Support for producing high-quality graphs', 'Something Interesting —Covid-19 Interactive Map made using R', 'Short for Logic Programming, Prolog first showed up in 1972. It makes for an exciting tool for developing Artificial Intelligence, specifically Natural Language Processing. Prolog works best for creating chatbots, ELIZA was the first-ever chatbot created with Prolog to have ever existed.', 'To understand Prolog, you must familiarize yourself with some of the fundamental terms of Prolog’s that guide it’s working, they are explained in brief below:', '● Facts define the true statements', '● Rules define the statement but with additional conditions', '● Goals define where the submitted statements stand according to the knowledgebase', '● Queries define the how of making your statement true and the final analysis of facts and rules', 'Prolog offers two approaches for implementing AI that has been in practice for a long time and is well-known among data scientists and researchers:', '● The Symbolic Approach includes rule-based expert systems, theorem provers, constraint-based approaches.', '● The Statistical approach includes neural nets, data mining, machine learning and several others.', 'Short for List Processing, it is the second oldest programming language next to Fortran. Called as one of the Founding Fathers of AI, Lisp was created by John McCarthy in 1958.', 'Lisp is a language for doing what you’ve been told is impossible.', '-Kent Pitman', 'Built as a practical mathematical notation for programs, Lisp soon became the choice of AI programming language for developers very quickly. Below are some of the Lisp features that make it one of the best options for AI projects on Machine Learning:', '● Rapid Prototyping', '● Dynamic Object Creation', '● Garbage Collection', '● Flexibility', 'With major improvements in other competing programming languages, several features specific to Lisp have made their way into other languages. Some of the notable projects that involved Lisp at some point in time are Reddit and HackerNews.', 'Take Lisp, you know its the most beautiful language in the world — at least up until Haskell came along.-Larry Wall', 'Defined in 1990 and named after the famous mathematician Haskell Brooks Curry, Haskell is a purely functional and statically typed programming language, paired with lazy evaluation and shorter code.', 'It is considered a very safe programming language as it tends to offer more flexibility in terms of handling errors as they happen so rarely in Haskell compared to other programming languages. Even if they do occur, a majority of the non-syntactical errors are caught at compile-time instead of runtime. Some of the features offered by Haskell are:', '● Strong abstraction capabilities', '● Built-in memory management', '● Code reusability', '● Easy to understand', 'SQL, Lisp , and Haskell are the only programming languages that I’ve seen where one spends more time thinking than typing.-Philip Greenspun', 'Its features help improve the productivity of the programmer. Haskell is a lot like the other programming languages, just used by a niche group of developers. Putting the challenges aside, Haskell can prove to be just as good as other competing languages for AI with increased adoption by the developer community.', 'Julia is a high-performance and general-purpose dynamic programming language tailored to create almost any application but is highly suited for numerical analysis and computational science. Various tools available for working with Julia are:', '● Popular editors such as Vim and Emacs', '● IDEs such as Juno and Visual Studio', 'Some of the several features offered by Julia that make it a noteworthy option for AI programming, Machine Learning, statistics, and data modeling are:', '● Dynamic type system', '● Built-in package manager', '● Able to work for parallel and distributed computing', '● Macros and metaprogramming abilities', '● Support for Multiple dispatches', '● Direct support for C functions', 'Built to eliminate the weaknesses of other programming languages, Julia can also be used for Machine Learning applications with integrations with tools such as TensorFlow.jl, MLBase.jl, MXNet.jl and many more that utilize the scalability provided by Julia.', 'Google Trend — Julia Interest Over Time', 'JuliaCon 2019 Highlights —', 'With several AI programming languages to choose from, AI engineers and scientists can pick the right one that suits the needs of their project. Every AI programming language comes with its fair share of pros and cons. With improvements made to these languages regularly, it won’t be long when developing for AI would become more comfortable than how it is today so that more people could join this wave of innovation. Outstanding community support has made things even better for new people, and the community contributions towards several packages and extensions make life easier for everyone.', 'I hope you’ve found this article useful! Below are additional resources if you’re interested in learning more: —', 'About Author', 'Claire D. is a Content Crafter and Marketer at Digitalogy — a tech sourcing and custom matchmaking marketplace that connects people with pre-screened & top-notch developers and designers based on their specific needs across the globe. Connect with Digitalogy on Linkedin, Twitter, Instagram.']"
03/2020,How to Export Pandas DataFrame to CSV,"In this post, we’ll go over how to write DataFrames to CSV…",193,3,https://towardsdatascience.com/@barney.h,https://towardsdatascience.com/how-to-export-pandas-dataframe-to-csv-2038e43d9c03?source=collection_archive---------9-----------------------,2,5,"['How to Export Pandas DataFrame to CSV', 'Short Answer', 'Recap on Pandas DataFrame', 'Exporting the DataFrame into a CSV file', 'Appendix']",18,"['The easiest way to do this :', 'If you want to export without the index, simply add index=False;', ""If you getUnicodeEncodeError , simply add encoding='utf-8' ;"", 'Pandas DataFrames create an excel data structure with labeled axes (rows and columns). To define one DataFrame, you need at least the rows of data and columns name (header).', 'Here is an example of pandas DataFrame:', 'Pandas DataFrame is Excel-like Data', 'Code to generate DataFrame:', 'Pandas DataFrame to_csv() function exports the DataFrame to CSV format. If a file argument is provided, the output will be the CSV file. Otherwise, the return value is a CSV format like string.', 'Here are some options:', 'path_or_buf: A string path to the file or a StringIO', 'sep: Specify a custom delimiter for the CSV output, the default is a comma.', 'na_rep: A string representation of a missing value like NaN. The default value is ‘’.', 'float_format: Format string for floating-point numbers.', 'header: Whether to export the column names. The default value is True.', 'columns: Columns to write. The default value is None, and every column will export to CSV format. If set, only columns will be exported.', 'index: Whether to write the row index number. The default value is True.', 'Common scenarios of writing to CSV files', 'The syntax of DataFrame to_csv() is:']"
04/2020,Springer has released 65 Machine Learning and Data books for free,-,8.1K,37,https://towardsdatascience.com/@urieli17,https://towardsdatascience.com/springer-has-released-65-machine-learning-and-data-books-for-free-961f8181f189?source=collection_archive---------0-----------------------,4,2,"['Springer has released 65 Machine Learning and Data books for free', 'The 65 books list:']",196,"['Hundreds of books are now free to download', 'Springer has released hundreds of free books on a wide range of topics to the general public. The list, which includes 408 books in total, covers a wide range of scientific and technological topics. In order to save you some time, I have created one list of all the books (65 in number) that are relevant to the data and Machine Learning field.', 'Among the books, you will find those dealing with the mathematical side of the domain (Algebra, Statistics, and more), along with more advanced books on Deep Learning and other advanced topics. You also could find some good books in various programming languages such as Python, R, and MATLAB, etc.', 'If you are looking for more recommended books about Machine Learning and data you can check my previous article about it.', 'The Elements of Statistical Learning', 'Trevor Hastie, Robert Tibshirani, Jerome Friedman', 'http://link.springer.com/openurl?genre=book&isbn=978-0-387-84858-7', 'Introductory Time Series with R', 'Paul S.P. Cowpertwait, Andrew V. Metcalfe', 'http://link.springer.com/openurl?genre=book&isbn=978-0-387-88698-5', 'A Beginner’s Guide to R', 'Alain Zuur, Elena N. Ieno, Erik Meesters', 'http://link.springer.com/openurl?genre=book&isbn=978-0-387-93837-0', 'Introduction to Evolutionary Computing', 'A.E. Eiben, J.E. Smith', 'http://link.springer.com/openurl?genre=book&isbn=978-3-662-44874-8', 'Data Analysis', 'Siegmund Brandt', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-03762-2', 'Linear and Nonlinear Programming', 'David G. Luenberger, Yinyu Ye', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-18842-3', 'Introduction to Partial Differential Equations', 'David Borthwick', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-48936-0', 'Fundamentals of Robotic Mechanical Systems', 'Jorge Angeles', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-01851-5', 'Data Structures and Algorithms with Python', 'Kent D. Lee, Steve Hubbard', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-13072-9', 'Introduction to Partial Differential Equations', 'Peter J. Olver', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-02099-0', 'Methods of Mathematical Modelling', 'Thomas Witelski, Mark Bowen', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-23042-9', 'LaTeX in 24 Hours', 'Dilip Datta', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-47831-9', 'Introduction to Statistics and Data Analysis', 'Christian Heumann, Michael Schomaker, Shalabh', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-46162-5', 'Principles of Data Mining', 'Max Bramer', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4471-7307-6', 'Computer Vision', 'Richard Szeliski', 'http://link.springer.com/openurl?genre=book&isbn=978-1-84882-935-0', 'Data Mining', 'Charu C. Aggarwal', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-14142-8', 'Computational Geometry', 'Mark de Berg, Otfried Cheong, Marc van Kreveld, Mark Overmars', 'http://link.springer.com/openurl?genre=book&isbn=978-3-540-77974-2', 'Robotics, Vision and Control', 'Peter Corke', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-54413-7', 'Statistical Analysis and Data Display', 'Richard M. Heiberger, Burt Holland', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2122-5', 'Statistics and Data Analysis for Financial Engineering', 'David Ruppert, David S. Matteson', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2614-5', 'Stochastic Processes and Calculus', 'Uwe Hassler', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-23428-1', 'Statistical Analysis of Clinical Data on a Pocket Calculator', 'Ton J. Cleophas, Aeilko H. Zwinderman', 'http://link.springer.com/openurl?genre=book&isbn=978-94-007-1211-9', 'Clinical Data Analysis on a Pocket Calculator', 'Ton J. Cleophas, Aeilko H. Zwinderman', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-27104-0', 'The Data Science Design Manual', 'Steven S. Skiena', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-55444-0', 'An Introduction to Machine Learning', 'Miroslav Kubat', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-63913-0', 'Guide to Discrete Mathematics', 'Gerard O’Regan', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-44561-8', 'Introduction to Time Series and Forecasting', 'Peter J. Brockwell, Richard A. Davis', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-29854-2', 'Multivariate Calculus and Geometry', 'Seán Dineen', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4471-6419-7', 'Statistics and Analysis of Scientific Data', 'Massimiliano Bonamente', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4939-6572-4', 'Modelling Computing Systems', 'Faron Moller, Georg Struth', 'http://link.springer.com/openurl?genre=book&isbn=978-1-84800-322-4', 'Search Methodologies', 'Edmund K. Burke, Graham Kendall', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6940-7', 'Linear Algebra Done Right', 'Sheldon Axler', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-11080-6', 'Linear Algebra', 'Jörg Liesen, Volker Mehrmann', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-24346-7', 'Algebra', 'Serge Lang', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4613-0041-0', 'Understanding Analysis', 'Stephen Abbott', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4939-2712-8', 'Linear Programming', 'Robert J Vanderbei', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-7630-6', 'Understanding Statistics Using R', 'Randall Schumacker, Sara Tomek', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6227-9', 'An Introduction to Statistical Learning', 'Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-7138-7', 'Statistical Learning from a Regression Perspective', 'Richard A. Berk', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-44048-4', 'Applied Partial Differential Equations', 'J. David Logan', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-12493-3', 'Robotics', 'Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani, Giuseppe Oriolo', 'http://link.springer.com/openurl?genre=book&isbn=978-1-84628-642-1', 'Regression Modeling Strategies', 'Frank E. Harrell , Jr.', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-19425-7', 'A Modern Introduction to Probability and Statistics', 'F.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, L.E. Meester', 'http://link.springer.com/openurl?genre=book&isbn=978-1-84628-168-6', 'The Python Workbook', 'Ben Stephenson', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-14240-1', 'Machine Learning in Medicine — a Complete Overview', 'Ton J. Cleophas, Aeilko H. Zwinderman', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-15195-3', 'Object-Oriented Analysis, Design and Implementation', 'Brahma Dathan, Sarnath Ramnath', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-24280-4', 'Introduction to Data Science', 'Laura Igual, Santi Seguí', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-50017-1', 'Applied Predictive Modeling', 'Max Kuhn, Kjell Johnson', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-6849-3', 'Python For ArcGIS', 'Laura Tateosian', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-18398-5', 'Concise Guide to Databases', 'Peter Lake, Paul Crowther', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4471-5601-7', 'Digital Image Processing', 'Wilhelm Burger, Mark J. Burge', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4471-6684-9', 'Bayesian Essentials with R', 'Jean-Michel Marin, Christian P. Robert', 'http://link.springer.com/openurl?genre=book&isbn=978-1-4614-8687-9', 'Robotics, Vision and Control', 'Peter Corke', 'http://link.springer.com/openurl?genre=book&isbn=978-3-642-20144-8', 'Foundations of Programming Languages', 'Kent D. Lee', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-70790-7', 'Introduction to Artificial Intelligence', 'Wolfgang Ertel', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-58487-4', 'Introduction to Deep Learning', 'Sandro Skansi', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-73004-2', 'Linear Algebra and Analytic Geometry for Physical Sciences', 'Giovanni Landi, Alessandro Zampini', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-78361-1', 'Applied Linear Algebra', 'Peter J. Olver, Chehrzad Shakiban', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-91041-3', 'Neural Networks and Deep Learning', 'Charu C. Aggarwal', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-94463-0', 'Data Science and Predictive Analytics', 'Ivo D. Dinov', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-72347-1', 'Analysis for Computer Scientists', 'Michael Oberguggenberger, Alexander Ostermann', 'http://link.springer.com/openurl?genre=book&isbn=978-3-319-91155-7', 'Excel Data Analysis', 'Hector Guerrero', 'http://link.springer.com/openurl?genre=book&isbn=978-3-030-01279-3', 'A Beginners Guide to Python 3 Programming', 'John Hunt', 'http://link.springer.com/openurl?genre=book&isbn=978-3-030-20290-3', 'Advanced Guide to Python 3 Programming', 'John Hunt', 'http://link.springer.com/openurl?genre=book&isbn=978-3-030-25943-3']"
04/2020,53 Python Interview Questions and Answers,Python questions for data scientist and software engineers,1.8K,26,https://towardsdatascience.com/@chris-writes-code,https://towardsdatascience.com/53-python-interview-questions-and-answers-91fa311eec3f?source=collection_archive---------1-----------------------,15,2,"['53 Python Interview Questions and Answers', 'Conclusion']",133,"['Not so long ago I started a new role as a “Data Scientist” which turned out to be “Python Engineer” in practice.', 'I would have been more prepared if I’d brushed up on Python’s thread lifecycle instead of recommender systems in advance.', 'In that spirit, here are my python interview/job preparation questions and answers. Most data scientists write a lot code so this applies to both scientists and engineers.', 'Whether you’re interviewing candidates, preparing to apply to jobs or just brushing up on Python, I think this list will be invaluable.', 'Questions are unordered. Let’s begin.', 'I’ve been asked this question in every python / data science interview I’ve ever had. Know the answer like the back of your hand.', 'Without importing the Template class, there are 3 ways to interpolate strings.', 'Early in my python career I assumed these were the same… hello bugs. So for the record, is checks identity and == checks equality.', 'We’ll walk through an example. Create some lists and assign them to names. Note that b points to the same object as a in below.', 'Check equality and note they are all equal.', 'But do they have the same identity? Nope.', 'We can verify this by printing their object id’s.', 'c has a different id than a and b.', 'Another questions I’ve been asked in every interview. It’s deserves a post itself, but you’re prepared if you can walk through writing your own example.', 'A decorator allows adding functionality to an existing function by passing that existing function to a decorator, which executes the existing function as well as additional code.', 'We’ll write a decorator that that logs when another function is called.', ""Write the decorator function. This takes a function, func, as an argument. It also defines a function, log_function_called, which calls func() and executes some code, print(f'{func} called.'). Then it return the function it defined"", 'Let’s write other functions that we’ll eventually add the decorator to (but not yet).', 'Now add the decorator to both.', 'See how we can now easily add logging to any function we write just by adding @logging above it.', 'Range generates a list of integers and there are 3 ways to use it.', 'The function takes 1 to 3 arguments. Note I’ve wrapped each usage in list comprehension so we can see the values generated.', 'range(stop) : generate integers from 0 to the “stop” integer.', 'range(start, stop) : generate integers from the “start” to the “stop” integer.', 'range(start, stop, step) : generate integers from “start” to “stop” at intervals of “step”.', 'Thanks Searge Boremchuq for suggesting a more pythonic way to do this!', 'Instance methods : accept self parameter and relate to a specific instance of the class.', 'Static methods : use @staticmethod decorator, are not related to a specific instance, and are self-contained (don’t modify class or instance attributes)', 'Class methods : accept cls parameter and can modify the class itself', 'We’re going to illustrate the difference around a fictional CoffeeShop class.', ""CoffeeShop class has an attribute, specialty, set to 'espresso' by default. Each instance of CoffeeShop is initialized with an attribute coffee_price . It also has 3 methods, an instance method, a static method and a class method."", 'Let’s initialize an instance of the coffee shop with a coffee_price of 5. Then call the instance method make_coffee.', 'Now call the static method. Static methods can’t modify class or instance state so they’re normally used for utility functions, for example, adding 2 numbers. We used ours to check the weather.Its sunny. Great!', 'Now let’s use the class method to modify the coffee shop’s specialty and then make_coffee.', 'Note how make_coffee used to make espresso but now makes drip coffee!', 'The purpose of this question is to see if you understand that all functions are also objects in python.', 'func is the object representing the function which can be assigned to a variable or passed to another function. func() with parentheses calls the function and returns what it outputs.', 'map returns a map object (an iterator) which can iterate over returned values from applying a function to every element in a sequence. The map object can also be converted to a list if required.', 'Above, I added 3 to every element in the list.', 'A reader suggested a more pythonic implementation. Thanks Chrisjan Wust !', 'Also, thanks Michael Graeme Short for the corrections!', 'This can be tricky to wrap your head around until you use it a few times.', 'reduce takes a function and a sequence and iterates over that sequence. On each iteration, both the current element and output from the previous element are passed to the function. In the end, a single value is returned.', '11 is returned which is the sum of 1+2+3+5.', 'Filter literally does what the name says. It filters elements in a sequence.', 'Each element is passed to a function which is returned in the outputted sequence if the function returns True and discarded if the function returns False.', 'Note how all elements not divisible by 2 have been removed.', 'Be prepared to go down a rabbit hole of semantics if you google this question and read the top few pages.', 'In a nutshell, all names call by reference, but some memory locations hold objects while others hold pointers to yet other memory locations.', 'Let’s see how this works with strings. We’ll instantiate a name and object, point other names to it. Then delete the first name.', 'What we see is that all these names point to the same object in memory, which wasn’t affected by del x.', 'Here’s another interesting example with a function.', 'Notice how adding an s to the string inside the function created a new name AND a new object. Even though the new name has the same “name” as the existing name.', 'Thanks Michael P. Reilly for the corrections!', 'Note how reverse() is called on the list and mutates it. It doesn’t return the mutated list itself.', 'Let’s see the results of multiplying the string ‘cat’ by 3.', 'The string is concatenated to itself 3 times.', 'Let’s see the result of multiplying a list, [1,2,3] by 2.', 'A list is outputted containing the contents of [1,2,3] repeated twice.', 'Self refers to the instance of the class itself. It’s how we give methods access to and the ability to update the object they belong to.', 'Below, passing self to __init__() gives us the ability to set the color of an instance on initialization.', 'Adding 2 lists together concatenates them. Note that arrays do not function the same way.', 'We’ll discuss this in the context of a mutable object, a list. For immutable objects, shallow vs deep isn’t as relevant.', 'We’ll walk through 3 scenarios.', 'i) Reference the original object. This points a new name, li2, to the same place in memory to which li1 points. So any change we make to li1 also occurs to li2.', 'ii) Create a shallow copy of the original. We can do this with the list() constructor, or the more pythonic mylist.copy() (thanks Chrisjan Wust !).', 'A shallow copy creates a new object, but fills it with references to the original. So adding a new object to the original collection, li3, doesn’t propagate to li4, but modifying one of the objects in li3 will propagate to li4.', 'iii) Create a deep copy. This is done with copy.deepcopy(). The 2 objects are now completely independent and changes to either have no affect on the other.', 'Note: Python’s standard library has an array object but here I’m specifically referring to the commonly used Numpy array.', 'I wrote another comprehensive post on arrays.', 'Remember, arrays are not lists. Arrays are from Numpy and arithmetic functions like linear algebra.', 'We need to use Numpy’s concatenate function to do it.', 'Note this is a very subjective question and you’ll want to modify your response based on what the role is looking for.', 'Python is very readable and there is a pythonic way to do just about everything, meaning a preferred way which is clear and concise.', 'I’d contrast this to Ruby where there are often many ways to do something without a guideline for which is preferred.', 'Also subjective, see question 21.', 'When working with a lot data, nothing is quite as helpful as pandas which makes manipulating and visualizing data a breeze.', 'Immutable means the state cannot be modified after creation. Examples are: int, float, bool, string and tuple.', 'Mutable means the state can be modified after creation. Examples are list, dict and set.', 'Use the round(value, decimal_places) function.', 'Slicing notation takes 3 arguments, list[start:stop:step], where step is the interval at which elements are returned.', 'Pickling is the go-to method of serializing and unserializing objects in Python.', 'In the example below, we serialize and unserialize a list of dictionaries.', 'Dict is python datatype, a collection of indexed but unordered keys and values.', 'JSON is just a string which follows a specified format and is intended for transferring data.', 'ORMs (object relational mapping) map data models (usually in an app) to database tables and simplifies database transactions.', 'SQLAlchemy is typically used in the context of Flask, and Django has it’s own ORM.', 'Any takes a sequence and returns true if any element in the sequence is true.', 'All returns true only if all elements in the sequence are true.', 'Looking up a value in a list takes O(n) time because the whole list needs to be iterated through until the value is found.', 'Looking up a key in a dictionary takes O(1) time because it’s a hash table.', 'This can make a huge time difference if there are a lot of values so dictionaries are generally recommended for speed. But they do have other limitations like needing unique keys.', 'A module is a file (or collection of files) that can be imported together.', 'A package is a directory of modules.', 'So packages are modules, but not all modules are packages.', 'Increments and decrements can be done with +- and -= .', 'Use the bin() function.', 'This can be done by converting the list to a set then back to a list.', 'Note that sets will not necessarily maintain the order of a list.', 'Use in.', 'append adds a value to a list while extend adds values in another list to a list.', 'This can be done with the abs() function.', 'You can use the zip function to combine lists into a list of tuples. This isn’t restricted to only using 2 lists. It can also be done with 3 or more.', 'You can’t “sort” a dictionary because dictionaries don’t have order but you can return a sorted list of tuples which has the keys and values that are in the dictionary.', 'In the below example, Audi, inherits from Car. And with that inheritance comes the instance methods of the parent class.', 'The easiest way is to split the string on whitespace and then rejoin without spaces.', '2 readers recommended a more pythonic way to handle this following the Python ethos that Explicit is better than Implicit. It’s also faster because python doesn’t create a new list object. Thanks Евгений Крамаров and Chrisjan Wust !', 'enumerate() allows tracking index when iterating over a sequence. It’s more pythonic than defining and incrementing an integer representing the index.', 'pass means do nothing. We typically use it because Python doesn’t allow creating a class, function or if-statement without code inside it.', 'In the example below, an error would be thrown without code inside the i > 3 so we use pass.', 'continue continues to the next element and halts execution for the current element. So print(i) is never reached for values where i < 3.', 'break breaks the loop and the sequence is not longer iterated over. So elements from 3 onward are not printed.', 'This for loop.', 'Becomes.', 'List comprehension is generally accepted as more pythonic where it’s still readable.', 'The ternary operator is a one-line if/else statement.', 'The syntax looks like a if condition else b.', 'You can use isnumeric().', 'You can use isalpha().', 'You can use isalnum().', 'This can be done by passing the dictionary to python’s list() constructor, list().', 'You can use the upper() and lower() string methods.', 'remove() remove the first matching value.', 'del removes an element by index.', 'pop() removes an element by index and returns that element.', 'Below we’ll create dictionary with letters of the alphabet as keys, and index in the alphabet as values.', 'Python provides 3 words to handle exceptions, try, except and finally.', 'The syntax looks like this.', 'In the simplistic example below, the try block fails because we cannot add integers with strings. The except block sets val = 10 and then the finally block prints complete.', 'You never know what questions will come up in interviews and the best way to prepare is to have a lot of experience writing code.', 'That said, this list should cover most anything you’ll be asked python-wise for a data scientist or junior/intermediate python developer roles.', 'I hope this was as helpful for you as writing it was for me.', 'Are there any great questions I missed?']"
04/2020,Dashboards are Dead,-,8.5K,87,https://towardsdatascience.com/@taylor_46803,https://towardsdatascience.com/dashboards-are-dead-b9f12eeb2ad2?source=collection_archive---------2-----------------------,7,4,"['Dashboards are Dead', 'Hello Dashboard, my old friend', 'Data’s going portrait mode', 'Resources']",22,"['Dashboards have been the primary weapon of choice for distributing data over the last few decades, but they aren’t the end of the story. To increasingly democratise access to data we need to think again, and the answer may be closer than you think…!', 'When I started my career, I was working in a large tech manufacturing company. The company had just purchased its first dashboarding tool and our team was responsible for the exciting transition from tired spreadsheets and SSRS reports to shiny, new dashboards.', 'The jump from spreadsheet to dashboard was a significant leap forward in analytical maturity for us. Dashboards’ thoughtful design and interactivity dramatically reduced the ‘cost of admission’ to data. Suddenly, you would walk around the office and see employees from any role and background fiddling with dashboards. This is a data-lover’s paradise, right?', 'Not quite. We soon found that dashboards brought with them a new set of problems:', 'To demonstrate this further, let’s consider a data dashboard that’s been widely popular during the current Coronavirus crisis: the Johns Hopkins Coronavirus Dashboard.', 'The JHU dashboard is visually appealing; the red and black evoking a sense of severity and immediacy that this moment deserves. As our eyes span across the page, we’re confronted with numbers, dots of various sizes, and graphs almost always increasingly headed up and to the right. We are left with a sense that things are bad, and seem to be getting worse. This dashboard was built with the purpose of getting data out there in an accessible and engaging way. It may have even been designed to answer a few key questions like “how many new cases were the today in my country? my county?” And to be clear, this is so much better than if they had just posted a table, or a download link.', 'But beyond those superficial findings, we cannot take action with this data. If we wanted to use this data for a specific purpose, we would be lacking the necessary context around these numbers to make them useful — and to trust them as our own (e.g. when did social distancing measures start in my country/county? How available are tests in my country?) And even if we somehow did manage to get the necessary context to trust these numbers, the dashboard itself lacks the power and flexibility to do the bespoke analysis we would need.', 'Much like in my experience at a certain unnamed company, this dashboard is succeeding in getting people to do something with data, but not necessarily something meaningful with data. At said unnamed company, we tried to solve this by adding more and more dashboards, then adding more and more filters to those dashboards, then killing those dashboards when they decidedly weren’t useful. This negative feedback loop contributed to a serious mistrust of data and inter-team schisms, many of which I believe are still around if passive-aggressive LinkedIn updates are to be believed.', 'Dashboards have done a huge amount for data empowerment (and my career!) but they are certainly not the optimal interface for data collaboration and reporting. Thankfully there’s a contender which you may already be using…', 'Data notebooks, like Jupyter, have become very popular over the last few years in the data science field. The process-oriented nature has proven superior to traditional scripting for doing data analysis and data science. Not only is this beneficial for the analyst doing the work, but it also helps the boss/coworker/reluctant friend that has to use it.', 'Fundamentally, notebooks offer the opportunity:', 'I am certainly not the first one to want to apply the power and flexibility of notebooks to the data analysis/business intelligence realm. We’ve talked to a number of companies who are using notebooks in favour of dashboards. Some only only use Jupyter notebooks for their reporting, others will cut and paste charts into text editors to achieve a similar effect. These are imperfect solutions, but a sign that companies are willing to move past highly crafted dashboards to realise the benefits of notebooks.', 'We just need a way to extend these principle beyond data science and enable the notebook to become as accessible as dashboards have been.', 'At Count, we’re so convinced of the fundamental benefits of notebooks that we’ve built a data analysis platform around them. No dashboards here people!', 'In order to use them beyond the walls of data science, we’ve had to craft our own version but the fundamental principles still apply, but with some added benefits…', 'Built for all levels of experience', 'Collaboration-enabled', 'By embracing notebooks at the core, Count provides the power, transparency and collaboration which teams need to not just give people numbers, but to give them the power to get the insights they need, and share them with rest of the company.', 'As we’ve been building Count we’ve been working with a number of organisations to see how notebooks change the way data is used amongst the team. Here’s what we’ve found:', 'Since everything is consumable by everyone, and in a single place, the trust issues start to improve (or, in reality, just become about something else). They aren’t creating dashboards for people who won’t use them, or thousands of filters to accommodate every need since people have more power to create the reports they really need. The scenes they describe prove that the small shift from dashboard to notebook can have a dramatic impact on the way your team utilises data.', 'If you want to learn more about how notebooks could help make your team more data-driven, drop us a line at hello@count.co, or you can read more here!', '[1] Coronavirus COVID-19 Global Cases by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU) (2020), John’s Hopkins University']"
04/2020,What’s happened to the data science job market in the past month,-,3.8K,28,https://towardsdatascience.com/@edouard_harris,https://towardsdatascience.com/whats-happened-to-the-data-science-job-market-in-the-past-month-88c748a4cd25?source=collection_archive---------3-----------------------,5,1,['What’s happened to the data science job market in the past month'],24,"['I work at a company that mentors data scientists for free until they’re hired. Because we only make money when our data scientists get hired, we get a very detailed view of the North American data science job market and how it’s evolving in real time. We know who is getting hired where, how much they’re being offered, the details of the offer negotiation process, and lots of other data.', 'In this post I’ll break down what we’ve been seeing in the data science job market over the past month. I originally released some of this information in a tweetstorm last week, but I’ll be going into greater detail in this post.', 'All the statistics below are based on the internal data from our mentorship program. We closely track which of our mentees are getting hired where, when, and by whom. The extrapolation to the broader data science job market won’t be perfect, but it should be quite close based on our scale and geographic reach.', 'There’s been a general hiring slowdown during March, by a factor of about 60–70% for data scientists. We actually expected hiring would go to zero temporarily, but it hasn’t — at least not yet.', 'The impact of this slowdown is very uneven. Companies that rely on foot traffic (e.g., fashion retailers) have been hit hard, are freezing hiring, and are doing layoffs or furloughs. Business-to-business software companies like Twilio and Airtable are largely hiring normally. Remote work enablers like Slack and Zoom are accelerating recruitment.', 'So far, we’ve seen more mentees get hired in April compared to the same period in March. But it’s still early in the month, and things are changing fast.', 'In March, we saw several finalized job offers get withdrawn without notice. That means actual, signed agreements — with commitments from companies like “Your pay is $X0,000 and you start Monday. Welcome aboard!” — were pulled back. This is unprecedented: In the 18 months before March, it happened zero times.', 'Some of the companies that withdrew offers were startups; others were big companies you’ve definitely heard of. It wasn’t possible for us to predict who would withdraw their offers and who wouldn’t. Watch out for this.', 'In April, we haven’t seen any more of these offer withdrawals so far. We suspect most companies have now adjusted their hiring plans, so this sort of thing will be happening less.', 'Layoffs haven’t hit too many data scientists just yet. So far only 3% of our alumni have been affected. That includes both temporary furloughs and salary reductions that fall short of layoffs.', 'When doing layoffs, most companies cut early, cut once, and cut deep. This tends to reduce uncertainty and risk for the employees that remain. If your company is using this strategy, and you survive the first round of layoffs, your job is more likely to be secure in the medium term.', 'If you’re worried about getting laid off, your first line of defense is to become indispensable. Find ways to save your company money and to add value, even if that’s beyond the scope of your job description — and do it without being asked. Most companies don’t want to do layoffs. The more value you bring, the more likely you’ll stick around.', '(Companies: If you need to cut payroll, please consider temporary furloughs instead of firing folks outright. That will give your employees the optics of still having a job, plus the flexibility of being able to look for a new one. It’s the right thing to do.)', 'Several job posts you’ll see right now are duds. A lot of companies have cancelled positions internally because of COVID, but haven’t yet deleted their job posts. Applying through job boards was never a great strategy, but it’s even harder now because of these dud job posts.', 'In-person networking is also off the table for obvious reasons.', 'What works better is cold emailing and LinkedIn outreach. There’s a lot of detail involved in doing these really well — but the quick version is: 1) Do your research on the company; 2) Send a thoughtful, well-crafted message; and 3) Talk about concrete things you’ve built or accomplished.', 'Interestingly, a big factor in whether a person will answer your cold email or LinkedIn message is whether or not that person has kids. People without kids are often bored at home, and more likely to answer; people with kids are busy taking care of them, because daycare is off the table these days too.', 'Online meetups might also be a good channel for networking. We aren’t sure about this yet — it’s too early to say. But some of our mentees are actively experimenting with it.', 'If you get an interview, you’ll probably be interviewed over Zoom. Zoom is a very stable video platform, but still try to make sure your Internet connection is solid. If you don’t have a great connection, it’s okay to mention that at the start of the interview. Most interviewers will understand and cut you some slack.', '(Zoom used to have a setting that told the meeting organizer what you were looking at on your screen, and this was briefly a factor during job interviews. Fortunately, they recently removed it.)', 'Here’s the good news: Pretty much every job is suddenly remote. Geographic barriers have disappeared overnight. You can now apply to a much wider range of companies than before. This is especially great for folks in rural areas, and it levels the playing field in a lot of ways.', 'That’s what we’ve been seeing so far, but we’re still in the early days of this pandemic. It’s hard to predict what the next few months will bring, but at the moment, getting a data science job is hard but not impossible. The trick is to find the companies that haven’t been hit too hard by lockdowns.', 'If you want advice on how to navigate the DS job market — particularly in North America — feel free to follow me on Twitter and I’ll try to help you.', 'Good luck out there everyone. ❤️']"
04/2020,Machine Learning Engineers Will Not Exist In 10 Years.,OPINION,4.5K,75,https://towardsdatascience.com/@posey,https://towardsdatascience.com/machine-learning-engineers-will-not-exist-in-10-years-c9cbbf4472f3?source=collection_archive---------4-----------------------,6,1,['Machine Learning Engineers Will Not Exist In 10 Years.'],26,"['Note: this is an opinion piece, feel free to share your own opinion so we can continue to move our field in the right direction.', 'Machine Learning will transition to a commonplace part of every Software Engineer’s toolkit.', 'In every field we get specialized roles in the early days, replaced by the commonplace role over time. It seems like this is another case of just that.', 'Let’s unpack.', 'Machine Learning Engineer as a role is a consequence of the massive hype fueling buzzwords like AI and Data Science in the enterprise. In the early days of Machine Learning, it was a very necessary role. And it commanded a nice little pay bump for many! But Machine Learning Engineer has taken on many different personalities depending on who you ask.', 'The purists among us say a Machine Learning Engineer is someone who takes models out of the lab and into production. They scale Machine Learning systems, turn reference implementations into production-ready software, and oftentimes cross over into Data Engineering. They’re typically strong programmers who also have some fundamental knowledge of the models they work with.', 'But this sounds a lot like a normal software engineer.', 'Ask some of the top tech companies what Machine Learning Engineer means to them and you might get 10 different answers from 10 survey participants. This should be unsurprising. This is a relatively young role and the folks posting these jobs are managers, oftentimes of many decades who don’t have the time (or will) to understand the space.', 'Here are a few requirements from job listings from some of the top tech companies, notice how vastly they differ:', 'This first one is spicy. Are you sure this isn’t a researcher? How is this a Machine Learning Engineer?', 'This next one’s more on-brand. And it comes from the top so it shouldn’t be a surprise.', 'And finally drilling down on your stereotypical ML Engineer posting.', 'Some companies have started a new approach and I think most will follow. The approach is to list a Software Engineering role with exposure to Machine Learning as a core requirement + a few years of experience as a preferred qualification. Employers will take a preference to engineers with experience building and scaling systems, regardless of whether it was based on Machine Learning or some other technology.', 'The Machine Learning Engineer is necessary as long as Machine Learning understanding is rare and has a high barrier to entry.', 'It’s my earnest belief that the role of Machine Learning Engineer will be taken over entirely by the common software engineer. It will transition to a standard engineering role where the engineer will get a spec or reference implementation from someone upstream, turn it into production code, and ship and scale applications.', 'For now, much of many Machine Learning roles exist in this weird space where we’re attacking problems with ML that just haven’t been attacked before. By consequence, ML Engineers are in many cases half researcher, half engineer. I’ve come across my fair share of Machine Learning Engineers who play across the entire stack. I’ve come across others who have a more narrow skillset but spend more time reading new research papers and turning them into usable code.', 'We’re at a weird crossroads where we’re defining where the members of our teams fit into the puzzle.', 'By consequence of the way we work, we tend to shove ourselves into discussions and sit in meetings regardless of whether it’s core to our expertise. We accept any and every meeting invite… It’s my opinion the Machine Learning Engineer belongs at the tail end of building a reference implementation and then owns turning any of that into production code.', 'Not long from now, most enterprises will have little need for research efforts to get their projects to the finish line. Only niche use-cases and deep technical efforts will require a special skillset. Engineers will consume APIs and the world will move on; Machine Learning becoming a commonplace tool in every new engineer’s toolkit. We’re already seeing this as more and more exposure to Machine Learning trickles into universities. Go to a Machine Learning course at a university and it’s packed to the brim. Almost every graduate will leave university with some exposure to the field.', 'We can draw an analogy to Blockchain where the Distributed Systems Engineer became hot. The vast majority of Blockchain projects since Nakamoto’s white paper have been spending their efforts on building the fundamental technology and infrastructure. To do so you had to have incredibly strong engineering skills, most-often described as a Distributed Systems Engineer. You’re finally seeing a shift where things are getting abstracted, enterprises are starting to find use-cases, and the everyday engineer can now build novel use-cases using blockchain. We’re seeing the same general shift in AI/ML.', 'One of my favorite responses to the article, from Varii on Twitter:', '“Like you said, it’s a title. Most employers expect you to have overlapping skillsets. I feel like in the end it’s not about who gets wiped out, it’s about who is versatile enough to constantly adapt to the ever changing industry.”', 'Tons of great input from the broader community that I’m learning from. But my opinion will never shift on one thing: if you’re passionate about something it doesn’t matter what happens to a title, a field, or a trend, there will always be a place for you to pursue your passion and build cool things.', 'Stay safe and build on!', 'I started a (free) analytics group called Dataset Daily where we share a dataset every Monday and code throughout the week.', 'Let’s continue the conversation on Twitter.']"
04/2020,RIP correlation. Introducing the Predictive Power Score,An open-source alternative that finds more…,6.5K,50,https://towardsdatascience.com/@florian.wetschoreck,https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598?source=collection_archive---------5-----------------------,13,8,"['RIP correlation. Introducing the Predictive Power Score', 'Too many problems with the correlation', 'Calculating the Predictive Power Score (PPS)', 'Comparing the PPS to correlation', 'Applications of the PPS and the PPS matrix', 'How to use the PPS in your own (Python) project', 'Limitations', 'Conclusion']",35,"['It is Friday afternoon and your boss tells you that the data delivery surprisingly arrived early — after only 4 weeks of back and forth. This was the missing piece for your predictive model. You’re excited but also a little bit anxious because you know what’s next: exploring the data. All. 45. Columns. This will take many hours but you know it’s worth it because without data understanding you are walking blind. One obvious step is to have a look at all the univariate column distributions. But this won’t be enough.', 'You ask yourself: what relationships exist between columns?', 'To answer this question, you just repeat the typical drill: calculate a correlation matrix and check for some surprising relationships. Whenever you are surprised, you take a moment to plot a scatterplot of the two columns at hand and see if you can make any sense of it. Hopefully you can but too often you cannot because you don’t even know what the columns mean in the first place. But this is a story for another day.', 'After inspecting the correlation matrix you move on and you don’t even know what you don’t know (scary).', 'Let’s take a moment to review the correlation. The score ranges from -1 to 1 and indicates if there is a strong linear relationship — either in a positive or negative direction. So far so good. However, there are many non-linear relationships that the score simply won’t detect. For example, a sinus wave, a quadratic curve or a mysterious step function. The score will just be 0, saying: “Nothing interesting here”. Also, correlation is only defined for numeric columns. So, let’s drop all the categoric columns. In my last project more than 60% of the columns were categoric, but hey. Never mind. And no, I won’t convert the columns because they are not ordinal and OneHotEncoding will create a matrix that has more values than there are atoms in the universe.', 'If you are a little bit too well educated you know that the correlation matrix is symmetric. So you basically can throw away one half of it. Great, we saved ourselves some work there! Or did we? Symmetry means that the correlation is the same whether you calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetric. More often, relationships are asymmetric. Here is an example: The last time I checked, my zip code of 60327 tells strangers quite reliably that I am living in Frankfurt, Germany. But when I only tell them my city, somehow they are never able to deduce the correct zip code. Pff … amateurs. Another example is this: a column with 3 unique values will never be able to perfectly predict another column with 100 unique values. But the opposite might be true. Clearly, asymmetry is important because it is so common in the real world.', 'Thinking about those shortcomings of correlation, I started to wonder: can we do better?', 'The requirements: One day last year, I was dreaming about a score that would tell me if there is any relationship between two columns — no matter if the relationship is linear, non-linear, gaussian or only known by aliens. Of course, the score should be asymmetric because I want to detect all the weird relationships between cities and zip codes. The score should be 0 if there is no relationship and the score should be 1 if there is a perfect relationship. And as the icing on the cake, the score should be able to handle categoric and numeric columns out of the box. Summing it up for all my academic friends: an asymmetric, data-type-agnostic score for predictive relationships between two columns that ranges from 0 to 1.', 'First of all, there is not the one and only way to calculate the predictive power score. In fact, there are many possible ways to calculate a score that satisfies the requirements mentioned before. So, let’s rather think of the predictive power score as a framework for a family of scores.', 'Let’s say we have two columns and want to calculate the predictive power score of A predicting B. In this case, we treat B as our target variable and A as our (only) feature. We can now calculate a cross-validated Decision Tree and calculate a suitable evaluation metric. When the target is numeric we can use a Decision Tree Regressor and calculate the Mean Absolute Error (MAE). When the target is categoric, we can use a Decision Tree Classifier and calculate the weighted F1. You might also use other scores like the ROC etc but let’s put those doubts aside for a second because we have another problem:', 'Most evaluation metrics are meaningless if you don’t compare them to a baseline', 'I guess you all know the situation: you tell your grandma that your new model has a F1 score of 0.9 and somehow she is not as excited as you are. In fact, this is very smart of her because she does not know if anyone can score 0.9 or if you are the first human being who ever scored higher than 0.5 after millions of awesome KAGGLErs tried. So, we need to “normalize” our evaluation score. And how do you normalize a score? You define a lower and an upper limit and put the score into perspective. So what should the lower and upper limit be? Let’s start with the upper limit because this is usually easier: a perfect F1 is 1. A perfect MAE is 0. Boom! Done. But what about the lower limit? Actually, we cannot answer this in absolute terms.', 'The lower limit depends on the evaluation metric and your data set. It is the value that a naive predictor achieves.', 'If you achieve a F1 score of 0.9 this might be super bad or really good. If your super fancy cancer detection model always predicts “benign” and it still scores 0.9 on that highly skewed dataset then 0.9 is obviously not so good. So, we need to calculate a score for a very naive model. But what is a naive model? For a classification problem, always predicting the most common class is pretty naive. For a regression problem, always predicting the median value is pretty naive.', 'Getting back to the example of the zip codes and the city name. Imagine both columns are categoric. First, we want to calculate the PPS of zip code to city. We use the weighted F1 score because city is categoric. Our cross-validated Decision Tree Classifier achieves a score of 0.95 F1. We calculate a baseline score via always predicting the most common city and achieve a score of 0.1 F1. If you normalize the score, you will get a final PPS of 0.94 after applying the following normalization formula: (0.95–0.1) / (1–0.1). As we can see, a PPS score of 0.94 is rather high, so the zip code seems to have a good predictive power towards the city. However, if we calculate the PPS in the opposite direction, we might achieve a PPS of close to 0 because the Decision Tree Classifier is not substantially better than just always predicting the most common zip code.', 'Please note: the normalization formula for the MAE is different from the F1. For MAE lower is better and the best value is 0.', 'In order to get a better feeling for the PPS and its differences to the correlation, let’s have a look at the following two examples:', 'Let’s use a typical quadratic relationship: the feature x is a uniform variable ranging from -2 to 2 and the target y is the square of x plus some error. In this case, x can predict y very well because there is a clear non-linear, quadratic relationship — after all that’s how we generated the data. However, this is not true in the other direction from y to x. For example, if y is 4, it is impossible to predict whether x was roughly 2 or -2. Thus, the predictive relationship is asymmetric and the scores should reflect this.', 'What are the values of the scores in this example? If you don’t already know what you are looking for, the correlation will leave you hanging because the correlation is 0. Both from x to y and from y to x because the correlation is symmetric. However, the PPS from x to y is 0.67, detecting the non-linear relationship and saving the day. Nevertheless, the PPS is not 1 because there exists some error in the relationship. In the other direction, the PPS from y to x is 0 because your prediction cannot be better than the naive baseline and thus the score is 0.', 'Let’s compare the correlation matrix to the PPS matrix on the Titanic dataset. “The Titanic dataset? Again??” I know, you probably think you already have seen everything about the Titanic dataset but maybe the PPS will give you some new insights.', 'After we learned about the advantages of the PPS, let’s see where we can use the PPS in the real life.', 'Disclaimer: There are use cases for both the PPS and the correlation. The PPS clearly has some advantages over correlation for finding predictive patterns in the data. However, once the patterns are found, the correlation is still a great way of communicating found linear relationships.', 'If you are still following along you are one of the rare human beings who still have an attention span — you crazy beast! If you can’t wait to see what the PPS will reveal on your own data, we have some good news for you: we open-sourced an implementation of the PPS as a Python library named ppscore.', 'Before using the Python library, please take a moment to read through the calculation details', 'Installing the package:', 'Calculating the PPS for a given pandas dataframe:', 'You can also calculate the whole PPS matrix:', 'Although the PPS has many advantages over the correlation, there is some drawback: it takes longer to calculate. But how bad is it? Does it take multiple weeks or are we done in a couple of minutes or even seconds? When calculating a single PPS using the Python library, the time should be no problem because it usually takes around 10–500ms. The calculation time mostly depends on the data types, the number of rows and the used implementation. However, when calculating the whole PPS matrix for 40 columns this results in 40*40=1600 individual calculations which might take 1–10 minutes. So you might want to start the calculation of the PPS matrix in the background and go on that summer vacation you always dreamed of! 🏖 ️For our projects and datasets the computational performance was always good enough but of course there is room for improvement. Fortunately, we see many ways how the calculation of the PPS can be improved to achieve speed gains of a factor of 10–100. For example, using intelligent sampling, heuristics or different implementations of the PPS. If you like the PPS and are in need of a faster calculation, please reach out to us.', 'We made it — you are excited and want to show the PPS to your colleagues. However, you know they are always so critical about new methods. That’s why you better be prepared to know the limitations of the PPS:', 'After years of using the correlation we were so bold (or crazy?) to suggest an alternative that can detect linear and non-linear relationships. The PPS can be applied to numeric and categoric columns and it is asymmetric. We proposed an implementation and open-sourced a Python package. In addition, we showed the differences to the correlation on some examples and discussed some new insights that we can derive from the PPS matrix.', 'Now it is up to you to decide what you think about the PPS and if you want to use it on your own projects. We have been using the PPS for over a year as part of the library bamboolib where the PPS is essential to add some advanced features and thus we wanted to share the PPS with the broader community. Therefore, we hope to receive your feedback about the concept and we would be thrilled if you try the PPS on your own data. In case that there might be a positive reception, we are happy to hear about your requests for adjustments or improvements to the implementation. As we mentioned before, there are many ways on how to improve the speed and on how to adjust the PPS for more specific use cases.', 'Github: https://github.com/8080labs/ppscore', 'Email: florian AT 8080labs.com', 'Newsletter: if you want to hear more about the PPS and our other upcoming Data Science projects and tools, you can subscribe here. We will not write about paid products, you can unsubscribe anytime and — sad that we even have to mention this — we will never give away your email.', 'Originally published at https://8080labs.com on April 23, 2020.']"
04/2020,3 Insane Secret Weapons for Python,I don’t know how I lived without them,5K,16,https://towardsdatascience.com/@pbadeer,https://towardsdatascience.com/the-3-secret-weapons-that-changed-my-python-editor-forever-c99f7b2e0084?source=collection_archive---------6-----------------------,7,5,"['3 Insane Secret Weapons for Python', 'Secret Weapon #1: Write Faster and Google Less with Kite', 'Secret Weapon #2: Stabilize Your Code with MyPy and Static Typing', 'Secret Weapon #3: Find Errors Faster and Write Simpler Functions with Sonarlint', 'Conclusion']",47,"['I’ve been writing Python for over 5 years and my toolset generally has been getting smaller, not bigger. A lot of tools just aren’t necessary or useful, and some of them you’ll simply outgrow.', 'These are three I’ve been stuck on for a long time, and unlike the rest, I just keep using them more and more.', 'Most code editors have an autocomplete feature that looks something like this:', '…which uses language (and sometimes library) documentation to suggest things like function names and parameters.', 'Sure that’s okay, but what if your editor could surf years of GitHub data and autocomplete not just function names, but entire lines of code.', 'This is just the first of three reasons you should use Kite.', 'Kite looks at your codebase and variables, frequently used parameter names online, documentation, and then makes super contextual recommendations like this:', 'The above example shows how Kite can predict which variables you’ll use where even they’re named generically (like b) or using common names (like x or y).', '…we’ve spent around 50 engineer-years semantically indexing all the code on Github, building statistical type inference, and rich statistical models that use this semantic information in a very deep way. — Adam Smith, Founder/CEO at Kite', 'Here’s a real-time demo video, or if you’d like to, go play in the sandbox.', 'If you’ve never been told to “RTFM” then you probably haven’t made the mistakes I have.', 'Regardless, you should always read the documentation before bugging a senior developer or even looking at Stack Overflow answers.', 'Kite Copilot makes documentation stupid-easy. It runs alongside your editor and in real-time shows the docs for any object/function/etc that you highlight with your cursor.', 'Dear senior developer at my first job: I’m sorry. Now I truly have no excuse not to look for answers in the docs first. 😉', 'On top of all that it’s built to run locally so you get incredibly fast recommendations, it works offline, and your code is never sent to the cloud.', 'This is incredibly important for people with poor internet connections and folks who work in closed-source codebases.', 'I’ve been using Kite for years, and it just keeps getting better. With over $17M in investment, this company isn’t going anywhere and the tool, for some stupid reason, i̶s̶ ̶c̶o̶m̶p̶l̶e̶t̶e̶l̶y̶ ̶f̶r̶e̶e̶. Edit: The first feature I mentioned is now a part of Kite Pro, the others are still free. There’s a great ROI calculator for convincing your boss to buy it for you.', 'All you have to do is download the Kite plugin for your editor, or download the copilot and it can install plugins for you. Go get it!', 'Python is dynamically typed, an oversimplified explanation would be that you can make any variable be any data type (string, integer, etc) at any time.', 'The opposite is languages that are statically typed, where variables must have one specific data type, and always adhere to it.', 'The advantage of dynamic typing is that you can be lazy when you’re writing and it can reduce code clutter.', 'The disadvantages are numerous and big though:', 'As of Python 3.5, you can use static typing natively! Additionally, mypy is a module that lets you quickly and easily scan your codebase for static type violations.', 'Here’s just one example of how it’s used:', 'Now you can scan by installing mypy with pip install mypy and scan your code by using the commandmypy [path to python file]. This will notify you of any places where a data type does not match the static type you specified.', 'With the static example, we’re specifying that the function returns an Iterator of Integers. This simple change makes the function more future-proof by enforcing a consistent output.', 'Other developers only have to look at the declaration to see what data type the output will be, and unlike just using documentation, your code will error if that declaration is disobeyed.', 'This is a super simple example taken from the examples here, go check them out if it’s still not making sense.', 'It’s hard to list all the ways that static typing can save you future pain, but the mypy docs have a great FAQ with more pros and cons.', 'If you’re working in a production codebase where stability is king, definitely give mypy and the typing module a try.', 'Nowadays every editor has some type of error checking or “linter” built-in. It looks at the code, typically without running it, and tries to guess what might go wrong. This is called Static Code Analysis.', 'Dynamic Code Analysis actually attempts to run/compile parts of your code to see if it’s working properly, but it does this in the background automatically. Instead of guessing, it actually knows if it will work and what the exact errors would be.', 'SonarLint is Dynamic Code Analysis at its best, plus more. These features are why I love it:', 'I’m guilty of leaving print statements, commented out code, and unused functions lying all over my codebase. This will warn me about it, making it hard to forget, and tell me where it is, making it easy to find.', 'A huge database of constantly updated security risks is thrown at your codebase in real-time, warning you of any known vulnerabilities that you’re exposed to.', 'Security risks are very niche and impossible to memorize, so everyone should use something to track these. SonarLint is a great place to start.', 'Slightly different from uncalled code, this will warn me if I’ve created any evaluations that have a result that is impossible to reach. These are tricky to spot and can lead to hours of debugging, so it’s one of my favorite warnings.', 'Here’s an example:', 'This is a fascinating topic that I could write an entire post about, in fact, there’s a whole whitepaper on it.', 'The simple explanation is that they’ve created a math formula that can score how difficult code is to read/understand.', 'It’s not only incredibly useful, but it’s also easy to follow. Every time SonarLint has asked me to “reduce cognitive complexity” it comes with a simple explanation of the rule I broke, like “too many nested if statements”.', 'I find this more useful than basic blocking and linting practices, and I’m convinced it’s making me write human-friendlier code. Which is Pythonic, by the way!', 'SonarLint is free, so there’s no reason not to grab it now and get it attached to your editor.', 'If you skipped here, just a quick warning that you might not be able to use these properly unless you have a basic understanding of the features.', 'Here’s an overview of the three secret weapons:', 'I hope these tools serve you well, I’ve become quite attached to them myself. I’m sure I’ve missed out on some other incredible resources though, so be sure to share what you can’t live without in the comments.', 'If you work in Data Science, you might find value from my current top post:']"
04/2020,"If I had to start learning Data Science again, how would I do it?",-,6.6K,27,https://towardsdatascience.com/@santiviquez,https://towardsdatascience.com/if-i-had-to-start-learning-data-science-again-how-would-i-do-it-78a72b80fd93?source=collection_archive---------7-----------------------,5,2,"['If I had to start learning Data Science again, how would I do it?', 'Kaggle micro-courses']",45,"['A couple of days ago I started thinking if I had to start learning machine learning and data science all over again where would I start? The funny thing was that the path that I imagined was completely different from that one that I actually did when I was starting.', 'I’m aware that we all learn in different ways. Some prefer videos, others are ok with just books and a lot of people need to pay for a course to feel more pressure. And that’s ok, the important thing is to learn and enjoy it.', 'So, talking from my own perspective and knowing how I learn better I designed this path if I had to start learning Data Science again.', 'As you will see, my favorite way to learn is going from simple to complex gradually. This means starting with practical examples and then move to more abstract concepts.', 'I know it may be weird to start here, many would prefer to start with the heaviest foundations and math videos to fully understand what is happening behind each ML model. But from my perspective starting with something practical and concrete helps to have a better view of the whole picture.', 'In addition, these micro-courses take around 4 hours/each to complete so meeting those little goals up front adds an extra motivational boost.', 'If you are familiar with Python you can skip this part. Here you’ll learn basic Python concepts that will help you start learning data science. There will be a lot of things about Python that are still going to be a mystery. But as we advance, you will learn it with practice.', 'Link: https://www.kaggle.com/learn/python', 'Price: Free', 'Pandas is going to give us the skills to start manipulating data in Python. I consider that a 4-hour micro-course and practical examples is enough to have a notion of the things that can be done.', 'Link: https://www.kaggle.com/learn/pandas', 'Price: Free', 'Data visualization is perhaps one of the most underrated skills but it is one of the most important to have. It will allow you to fully understand the data with which you will be working.', 'Link: https://www.kaggle.com/learn/data-visualization', 'Price: Free', 'This is where the exciting part starts. You are going to learn basic but very important concepts to start training machine learning models. Concepts that later will be essential to have them very clear.', 'Link: https://www.kaggle.com/learn/intro-to-machine-learning', 'Precio: Free', 'This is complementary to the previous one but here you are going to work with categorical variables for the first time and deal with null fields in your data.', 'Link: https://www.kaggle.com/learn/intermediate-machine-learning', 'Price: Free', 'Let’s stop here for a moment. It should be clear that these 5 micro-courses are not going to be a linear process, you are probably going to have to come and go between them to refresh concepts. When you are working in the Pandas one you may have to go back to the Python course to remember some of the things you learned or go to the pandas documentation to understand new functions that you saw in the Introduction to Machine Learning course. And all of this is fine, right here is where the real learning is going to happen.', 'Now, if you realize these first 5 courses will give you the necessary skills to do exploratory data analysis (EDA) and create baseline models that later you will be able to improve. So now is the right time to start with simple Kaggle competitions and put in practice what you’ve learned.', 'Here you’ll put into practice what you learned in the introductory courses. Maybe it will be a little intimidating at first, but it doesn’t matter it’s not about being first in the leaderboard, it’s about learning. In this competition, you will learn about classification and relevant metrics for these types of problems such as precision, recall and accuracy.', 'Link: https://www.kaggle.com/c/titanic', 'In this competition, you are going to apply regression models and learn about relevant metrics such as RMSE.', 'Link: https://www.kaggle.com/c/home-data-for-ml-course', 'By this point, you already have a lot of practical experience and you’ll feel that you can solve a lot of problems, buuut chances are that you don’t fully understand what is happening behind each classification and regression algorithms that you have used. So this is where we have to study the foundations of what we are learning.', 'Many courses start here, but at least I absorb this information better once I have worked on something practical before.', 'At this point we will momentarily separate ourselves from pandas, scikit-learn and other Python libraries to learn in a practical way what is happening “behind” these algorithms.', 'This book is quite friendly to read, it brings Python examples of each of the topics and it doesn’t have much heavy math, which is fundamental for this stage. We want to understand the principle of the algorithms but with a practical perspective, we don’t want to be demotivated by reading a lot of dense mathematical notation.', 'Link: Amazon', 'Price: $26 aprox', 'If you got this far I would say that you are quite capable of working in data science and understand the fundamental principles behind the solutions. So here I invite you to continue participating in more complex Kaggle competitions, engage in the forums and explore new methods that you find in other participants solutions.', 'Here we are going to see many of the things that we have already learned but we are going to watch it explained by one of the leaders in the field and his approach is going to be more mathematical so it will be an excellent way to understand our models even more.', 'Link: https://www.coursera.org/learn/machine-learning', 'Price: Free without the certificate — $79 with the certificate', 'Now the heavy math part starts. Imagine if we had started from here, it would have been an uphill road all along and we probably would have given up easier.', 'Link: Amazon', 'Price: $60, there is an official free version in the Stanford page.', 'By then you have probably already read about deep learning and play with some models. But here we are going to learn the foundations of what neural networks are, how they work and learn to implement and apply the different architectures that exist.', 'Link: https://www.deeplearning.ai/deep-learning-specialization/', 'Price: $49/month', 'At this point it depends a lot on your own interests, you can focus on regression and time series problems or maybe go more deep into deep learning.', 'I wanted to tell you that I launched a Data Science Trivia game with questions and answers that usually come out on interviews. To know more about this Follow me on twitter.']"
04/2020,Python’s Expiration Date,How long will Python be the “ big boy?” in scripting?,1.2K,14,https://towardsdatascience.com/@emmettgb,https://towardsdatascience.com/pythons-expiration-date-b1a55f368f1a?source=collection_archive---------8-----------------------,7,6,"['Python’s Expiration Date', 'What is Python used for?', 'Python’s Downsides', 'Benefits of Python', 'Is Python Going To Expire?', 'Conclusion']",31,"['In the past decade, one language has soared in popularity and surpassed all of its predecessors in popularity, and that language is of course Python. Python is an easy to use, easy to read, easy to mutate object-oriented programming language that is interpreted by C. There are many reasons why Python has recently been crowned the most popular programming language in the world, but there are also many reasons that it could potentially lose that title.', 'Python is an interpreted language, meaning there is no compiler or assembler that is able to put the language into machine code. Instead, another language, C in Python’s case, is used to interpret the language with the Python.h header. Typically, this would place Python into the scripting category, however, I think it’s important not to ignore Python’s position as a programming staple universally.', 'One enormous downside to Python is that Python code cannot be compiled into an executable. Any application or tool written in Python is going to require Python, as well as its dependencies, to be installed on the end-user’s system. Along with that are all of the correct versions of each package that correspond with the packages used to develop said application.', 'One thing Python has proven itself to be shockingly talented at is running the back-end for websites. This is also an advantage that Python has to many other languages. Python has plenty of incredible packages for deploying APIs and even designing fully featured web-apps.', 'Machine-learning is another prominent use for Python that has certainly helped to skyrocket its popularity in recent years. Machine-learning is at the forefront of technology, and Python, along with its close relationship with C is surprisingly effective and useful for machine-learning. While Python certainly does lack the performance of some similar languages like Nim, Julia, and Go, in a lot of ways it makes up for it by being quick, easy, concise, but perhaps more importantly,', 'universal.', 'It’s hard to talk about Python without talking about modern Unix-like systems. Python 2.7 has been deprecated for two months now and my desktop environment, as well as many more features in my operating-system still use Python 2.7. The combination of Bash and Python can make for some pretty useful scripts to run servers, populate data, complete requests, editing files, and certainly a lot more.', 'Though Python is certainly a great language, and has made an enormous impact on the entire world, every language has its downsides in one regard or another, and Python is no different. Firstly, and most notable:', 'Python is slow.', 'Though I certainly agree that using logs and the for each method and generally just writing better code can certainly speed Python up, there are definitely some situations where the code will just have to be slow. Machine-learning is a great example because training neural networks often requires a deadly use of recursion. I can’t tell you how many times I’ve written a script to pull data in Python, and then experienced timeouts and slowdowns inside of my Command Line Interface (CLI.)', 'There are efforts to mitigate this, with the ever impressive Cython, but often the transition isn’t quite as simple as apples and oranges when trying to use Cython. Though there are many scenarios where Python’s limitations can’t be felt, I would be lying if I said that I have never had to switch to Julia, Nim, or C to get something done.', 'Another significant downside to Python is dependencies and virtual environments. In regards to a scripting language in the realm of Python, I think Python does quite well with dependencies and virtual environments. I like to compare Python to the two languages I use to do similar things, Julia and Nim, and Julia’s world of dependencies is quite similar to that of Python’s. One advantage that Julia does have is in my subjective opinion, virtual environments are a lot better, and are easier to use.', 'However, with the inclusion of Nim, Python is left dead in the water. With Nim, you can create a compiled executable containing all of the dependencies required. Nim easily trumps both Python and Julia on how dependencies and packages are handled for end-users and deployment.', 'Despite my gripes with Python, there are a lot of things I do love about Python.', 'Python is a very commonly used programming language, which has a host of benefits like:', 'Not only that, but Python is proven to be venerable and reliable. This makes Python an excellent choice for beginners, and also those who want to build stable, long-lasting technology incredibly fast that can simultaneously be used incredibly easily.', 'A big difference between a language like Python and similar scripting languages is just how readable and easy to understand Python is. Often reading Python can be like reading a strange, abstract book about variables equal to numbers. This not only makes it easier for beginners, but also makes it easier to mutate, modify, and decipher, which are all very important, and especially so to large code-pools with thousands of developers working in them.', 'Let’s be honest, we’d all rather write Python than C.', 'Though to some it might not matter whether or not their language is free and open source, to me it is certainly important. The Python Foundation functions primarily off of donations and education certificates, meaning Python is an entirely free and open piece of software that someone coded for you to use, which is great.', 'A question I actually get asked a lot is “ How long do you think Python is going to be around?” Often this question arises in the machine-learning space, because Python has a whole host of issues associated with machine-learning. It’s funny to think that any language could possibly be static and remain as the most used programming language for a very long time.', 'Fortran was the big deal before C, C was the big deal before C++, C++ was the big deal before Java, and that list goes on and on, and always will. Computers are exciting because they are constantly evolving, and with them the technology that they work over. Just 30 years ago, the idea of 16GB of RAM was an entirely outlandish concept, so there is no telling at all what the future holds for programming languages.', 'Though it’s certainly true that eventually Python will most likely be overtaken by another programming language, I think it’s important to remember that people still write Fortran, C, Java, and C++; so Python itself will likely be with us and be used very commonly for an extended period of time, however unpopular it might become.', 'The biggest threat posed to Python’s popularity is probably other new programming languages. The languages I have discussed, Julia and Nim, are simply what I believe to be the two big competitors to Python at this moment in time. Julia could certainly change the way we do machine-learning, and Nim is definitely a big game-changer for high-level scripting.', 'These languages might not have gained all that much steam just yet, but they are showing one thing that I think is important:', 'Advancement is possible.', 'That is to say that Python is not a solid brick wall, and the advancement of programming languages, machine-learning, and scripting is certainly not going to stop with it. Part of the reason I think these languages have been unsuccessful is because they are both statistically-typed, and for the most part', 'functional languages,', 'and I understand that can be a very jarring concept for those who have stuck with and love Python.', 'Should you learn Python?', 'My answer to that question will always be yes. Python is a great language to get you started and familiar with how to work through problems and begin programming. Python also has an overwhelming amount of resources compared to other languages including Julia and Nim, making it far easier to learn.', 'One thing I aim to provide is a very objective view on programming concepts, and I don’t think Python is going away anytime soon. I do, however, think it’s important to constantly grow as a programmer and more importantly broaden your horizons. If you’ve been using Python for awhile, maybe pick up a second language. Try a language from a different paradigm, and understand how that paradigm and the generic programming methodology your languages of choice utilize those concepts. I think this can be helpful, because not only can you learn to use faster methods, but also you can learn about a host of things that you might not have otherwise even considered.']"
04/2020,Infectious Disease Modelling: Beyond the Basic SIR Model,-,478,8,https://towardsdatascience.com/@hf2000510,https://towardsdatascience.com/infectious-disease-modelling-beyond-the-basic-sir-model-216369c584c4?source=collection_archive---------9-----------------------,13,5,"['Infectious Disease Modelling: Beyond the Basic SIR Model', 'Models as State Transitions', 'Introducing new Compartments', 'Time-Dependent Variables', 'Recap']",65,"['My last article explains the background and provides an introduction to the topic of modelling infectious diseases. You might want to read that first to understand this one if you don’t already have a solid grasp of the SIR equations. This article is focused on more elaborate variants of the basic SIR model and will enable you to implement and code your own variants and ideas. The next article will be concerned with fitting a model to real-world data and includes Covid-19 as a case study.', 'You can find the python notebook for the whole article here.', 'First, we’ll quickly explore the SIR model from a slightly different — more visual — angle. Afterwards, we derive and implement the following extensions:', 'As a quick recap, take a look at the variables we defined:', 'And here are the basic equations again:', 'When deriving the equations, we already intuitively thought of them as “directions” that tell us what happens to the population the next day (for example, when 10 people are infected and recovery takes place at the rate 1/5 (that’s gamma), then the number of recovered individuals the next day should increase by 1/5 * 10 = 2). We now solidify this understanding of the equations as “directions” or “transitions” from one compartment S, I or R to another — this will greatly simplify things when we introduce more compartments later on and the equations get messy.', 'Here’s the notation we need:', 'Compartments are boxes (the “states”), like this:', 'Transitions from one compartment to another are represented by arrows, with the following labeling:', 'The rate describes how long the transition takes, population is the group of individuals that this transition applies to, and probability is the probability of the transition taking place for an individual.', 'As an example, let’s look at the transition from Susceptibles to Infected in our SIR equations, with beta=2, a total population of 100, 10 infected and 90 susceptible. The rate is 1, as the infections happen immediately; the population the transition applies to is 2 * 10 = 20 individuals, as the 10 infected each infect 2 people; the probability is 90%, as 90/100 people can still be infected. It corresponds to this intuitive notation:', 'And more generally, now for the whole model (for I → R, the rate is γ and the probability is 1 as everyone recovers):', 'As you can see, arrows pointing towards a compartment get added in the equation; arrows pointing away from a compartment get subtracted. That’s not too bad, is it? Take a moment to really understand the new notation and see how it is just another way of writing the equations.', 'Right, we now understand the SIR model and can code it in python, but is it already useful? Can it tell us anything about real-world infectious diseases? The answer is no. In its current state, the model is more of a toy than a useful tool. Let’s change that!', 'Many infectious diseases have an incubation period before being infectious during which the host cannot yet spread the disease. We’ll call such individuals — and the whole compartment — Exposed.', 'Intuitively, we’ll have transitions of the form S → E → I → R: Susceptible people can contract the virus and thus become exposed, then infected, then recovered. The new transition S → E will have the same arrow as the current S → I transition, as the probability is the same (all susceptibles can be exposed), the rate is the same (“exposition” happens immediately) and the population is the same (the infectious individuals can spread the disease and each exposes β new individuals per day). There’s also no reason for the transition from I to R to change. The only new transition is the one from E to I: the probability is 1 (everyone that’s exposed becomes infected), the population is E (all exposed will become infected), and the rate gets a new variable, δ (delta). We arrive at these transitions:', 'From these transitions, we can immediately derive these equations (again, compare the state transitions and the equations until it makes sense to you):', 'This should not be too hard, we just have to change a few lines of code from the last article (again, the full code is here to read along, I’m just showcasing the important bits here). We’ll model a highly infectious (R₀ =5.0) disease in a population of 1 million, with an incubation period of 5 days and a recovery taking 7 days.', 'The equations and initial values now look like this:', 'We calculate S, E, I, and R over time:', 'And (after plotting) get this:', 'We are now able to model real diseases a little more realistically, though one compartment is definitely still missing; we’ll add it now:', 'For very deadly diseases, this compartment is very important. For some other situations, you might want to add completely different compartments and dynamics (such as births and non-disease-related deaths when studying a disease over a long time); these models can get as complex as you want!', 'Let’s think about how we can take our current transitions and add a Dead state. When can people die from the disease? Only while they are infected! That means that we’ll have to add a transition I → D. Of course, people don’t die immediately; We define a new variable ρ (rho) for the rate at which people die (e.g. when it takes 6 days to die, ρ will be 1/6). There’s no reason for the rate of recovery, γ, to change. So our new model will look somehow like this:', 'The only thing that’s missing are the probabilities of going from infected to recovered and from infected to dead. That’ll be one more variable (the last one for now!), the death rate α. For example, if α=5%, ρ = 1 and γ = 1 (so people die or recover in 1 day, that makes for an easier example) and 100 people are infected, then 5% ⋅ 100 = 5 people will die. That leaves 95% ⋅ 100 = 95 people recovering. So all in all, the probability for I → D is α and thus the probability for I → R is 1-α. We finally arrive at this model:', 'Which naturally translates to these equations:', 'We only need to make some slight changes to the code (and we’ll set α to 20% and ρ to 1/9)…', '… and we arrive at this:', 'Note that I added a “total” that adds up S, E, I, R, and D for every time step as a “sanity check”: The compartments always have to sum up to N; this can give you a hint as to whether your equations are correct.', 'You should now know how you can add a new compartment to the model: Think about what transitions need to be added and changed; think about the probabilities, populations and rates of these new transitions; draw the diagram; and finally write down the equations. The coding is definitively not the hard part for these models!', 'For example, you might want to add a “ICU” compartment for infected individuals that need to go to an ICU (we’ll do that in the next article). Think about from which compartment people can go to the ICU, where they can go after the ICU, etc.', 'Here’s an updated list of the variables we currently use:', 'As you can see, only the compartments change over time (they are not constant). Of course, this is highly unrealistic! As an example, why should the R₀-value be constant? Surely, nationwide lockdowns reduce the number of people an infected person infects, that’s what they’re all about! Naturally, to get closer to modelling real-world developments, we have to make our variables change over time.', 'First, we implement a simple change: on day L, a strict “lockdown” is enforced, pushing R₀ to 0.9. In the equations, we use β and not R₀, but we know that R₀ = β / γ, so β = R₀ ⋅ γ. That means that we define a function', 'and another function for beta that calls this function:', 'Right, seems easy enough; We just change the code accordingly:', 'Let’s plot this for some different values of L:', 'A few days can make a huge difference in the overall spread of the disease!', 'In reality, R₀ probably never “jumps” from one value to another. Rather, it (more or less quickly) continuously changes (and might go up and down several times, e.g. if social distancing measures are loosened and then tightened again). You can choose any function you want for R₀, I just want to present one common choice to model the initial impact of social distancing: a logistic function.', 'The function (adopted for our purposes) looks like this:', 'And here’s what the parameters actually do:', 'These plots might help you understand the parameters:', 'Again, changing the code accordingly:', 'We let R₀ decline quickly from 5.0 to 0.5 around day 50 and can now really see the curves flattening after day 50:', 'Similarly to R₀, the fatality rate α is probably not constant for most real diseases. It might depend on a variety of things; We’ll focus on dependency on resources and age.', 'First, let’s look at resource dependency. We want the fatality rate to be higher when more people are infected. Think about how this could be put into a function: we probably need a “base” or “optimal” fatality rate for the case that only few people are infected (and thus receive optimal treatment) and some factor that takes into account what proportion of the population is currently infected. This is one example of a function that implements these ideas:', 'Here, s is some arbitrary but fixed (that means we choose it freely once for a model and then it stays constant over time) scaling factor that controls how big of an influence the proportion of infected should have; α_OPT is the optimal fatality rate. For example, if s=1 and half the population is infected on one day, then s ⋅ I(t) / N = 1/2, so the fatality rate α(t) on that day is 50% + α_OPT. Or maybe most people barely have any symptoms and thus many people being infected does not clog the hospitals. Then a scaling factor of 0.1 might be appropriate (in the same scenario, the fatality rate would only be 5% + α_OPT).', 'More elaborate models might make the fatality rate depend on the number of ICU beds or ventilators available, etc. We’ll do that in the next article when modelling Coronavirus.', 'Age dependency is a little more difficult. To fully implement it, we’d have to include separate compartments for every age group (e.g. an Infected-compartment for people aged 0–9, another one for people aged 10–19, …). That’s doable with a simple for-loop in python, but the equations get a little messy. A simpler approach that still is able to produce good results is the following:', 'We need 2 things for the simpler approach: Fatality rates by age group and proportion of the total population that is in that age group. For example, we might have the following fatality rates and number of individuals by age group (in Python dictionaries):', '(This would be an extremely old population with 40% being in the 60–89 range and 20% being in the 89+ range). Now we calculate the overall average fatality rate by adding up the age group fatality rate multiplied with the proportion of the population in that age group:', 'α = 0.01 ⋅ 0.1 + 0.05 ⋅ 0.3 + 0.2 ⋅ 0.4 + 0.3 ⋅ 0.2 = 15.6%. Or in code:', 'A rather young population with the following proportions …', '… would only have an average fatality rate of 7.4%!', 'If we want to use both our formulas for resource- and age-dependency, we could use the resource-formula we just used to calculate α_OPT and use that in our resource-dependent formula from above.', 'There are certainly more elaborate ways to implement fatality rates over time. For example, we’re not taking into account that only critical cases needing intensive care fill up the hospitals and might increase fatality rates; or that deaths change the population structure that we used to calculate the fatality rate in the first place; or that the impact of infected on fatality rates should take place several days later as people don’t usually die immediately, which would result in a Delay Differential Equation, and that’s annoying to deal with in Python! Again, get as creative as you want!', 'This is rather straightforward, we don’t even have to change our main equations (we define alpha inside the equations as we need access to the current value I(t)).', 'With the fatality rates by age group and the older population from above (and a scaling factor s of 1, so many people being infected has a high impact on fatality rates), we arrive at this plot:', 'For the younger population (around 80k instead of 150k deaths):', 'And now with a scaling factor of only 0.01:', 'For the older population (note how the fatality rate only rises very slightly over time) …', '… and the younger population:', 'That’s all! You should now be able to add your own compartments (maybe a compartment for individuals that can get infected again, or a compartment for a special risk group such as diabetics), first graphically through the state transition notation, then formally through the equations, and finally programmatically in Python! Also, you saw some examples of implementing time-dependent variables, making the models much more versatile.', 'All this should allow you to design SIR models that come quite close to the real world. Of course, many scientists are working on these models currently (this link can help you find some current articles— be warned, they can get quite complex, but it’s nonetheless a great way to get insights into the current state of the field). In the next article, we’ll focus on designing and fitting models to real-world data, with Coronavirus as a case-study.', 'Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.']"
05/2020,Bye-bye Python. Hello Julia!,OPINION,12.5K,120,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/bye-bye-python-hello-julia-9230bff0df62?source=collection_archive---------0-----------------------,8,5,"['Bye-bye Python. Hello Julia!', 'The Zen of Python versus the Greed of Julia', 'What Julia developers are loving', 'The data: Invest in things while they’re small', 'Bottom line: Do Julia and let it be your edge']",38,"['Don’t get me wrong. Python’s popularity is still backed by a rock-solid community of computer scientists, data scientists and AI specialists.', 'But if you’ve ever been at a dinner table with these people, you also know how much they rant about the weaknesses of Python. From being slow to requiring excessive testing, to producing runtime errors despite prior testing — there’s enough to be pissed off about.', 'Which is why more and more programmers are adopting other languages — the top players being Julia, Go, and Rust. Julia is great for mathematical and technical tasks, while Go is awesome for modular programs, and Rust is the top choice for systems programming.', 'Since data scientists and AI specialists deal with lots of mathematical problems, Julia is the winner for them. And even upon critical scrutiny, Julia has upsides that Python can’t beat.', 'When people create a new programming language, they do so because they want to keep the good features of old languages and fix the bad ones.', 'In this sense, Guido van Rossum created Python in the late 1980s to improve ABC. The latter was too perfect for a programming language — while its rigidity made it easy to teach, it was hard to use in real life.', 'In contrast, Python is quite pragmatic. You can see this in the Zen of Python, which reflects the intention that the creators have:', 'Python still kept the good features of ABC: Readability, simplicity, and beginner-friendliness for example. But Python is far more robust and adapted to real life than ABC ever was.', 'In the same sense, the creators of Julia want to keep the good parts of other languages and ditch the bad ones. But Julia is a lot more ambitious: instead of replacing one language, it wants to beat them all.', 'This is how Julia’s creators say it:', 'Julia wants to blend all upsides that currently exist, and not trade them off for the downsides in other languages. And even though Julia is a young language, it has already achieved a lot of the goals that the creators set.', 'Julia can be used for everything from simple machine learning applications to enormous supercomputer simulations. To some extent, Python can do this, too — but Python somehow grew into the job.', 'In contrast, Julia was built precisely for this stuff. From the bottom up.', 'Julia’s creators wanted to make a language that is as fast as C — but what they created is even faster. Even though Python has become easier to speed up in recent years, its performance is still a far cry from what Julia can do.', 'In 2017, Julia even joined the Petaflop Club — the small club of languages who can exceed speeds of one petaflop per second at peak performance. Apart from Julia, only C, C++ and Fortran are in the club right now.', 'With its more than 30 years of age, Python has an enormous and supportive community. There is hardly a Python-related question that you can’t get answered within one Google search.', 'In contrast, the Julia community is pretty tiny. While this means that you might need to dig a bit further to find an answer, you might link up with the same people again and again. And this can turn into programmer-relationships that are beyond value.', 'You don’t even need to know a single Julia-command to code in Julia. Not only can you use Python and C code within Julia. You can even use Julia within Python!', 'Needless to say, this makes it extremely easy to patch up the weaknesses of your Python code. Or to stay productive while you’re still getting to know Julia.', 'This is one of the strongest points of Python — its zillion well-maintained libraries. Julia doesn’t have many libraries, and users have complained that they’re not amazingly maintained (yet).', 'But when you consider that Julia is a very young language with a limited amount of resources, the number of libraries that they already have is pretty impressive. Apart from the fact that Julia’s amount of libraries is growing, it can also interface with libraries from C and Fortran to handle plots, for example.', 'Python is 100% dynamically typed. This means that the program decides at runtime whether a variable is a float or an integer, for example.', 'While this is extremely beginner-friendly, it also introduces a whole host of possible bugs. This means that you need to test Python code in all possible scenarios — which is quite a dumb task that takes a lot of time.', 'Since the Julia-creators also wanted it to be easy to learn, Julia fully supports dynamical typing. But in contrast to Python, you can introduce static types if you like — in the way they are present in C or Fortran, for example.', 'This can save you a ton of time: Instead of finding excuses for not testing your code, you can specify the type wherever it makes sense.', 'While all these things sound pretty great, it’s important to keep in mind that Julia is still tiny compared to Python.', 'One pretty good metric is the number of questions on StackOverflow: At this point in time, Python is tagged about twenty more often than Julia!', 'This doesn’t mean that Julia is unpopular — rather, it’s naturally taking some time to get adopted by programmers.', 'Think about it — would you really want to write your whole code in a different language? No, you’d rather try a new language in some future project. This creates a time lag that every programming language faces between its release and its adoption.', 'But if you adopt it now — which is easy because Julia allows an enormous amount of language conversion — you’re investing in the future. As more and more people adopt Julia, you’ll already have gained enough experience to answer their questions. Also, your code will be more durable as more and more Python code is replaced by Julia.', 'Forty years ago, artificial intelligence was nothing but a niche phenomenon. The industry and investors didn’t believe in it, and many technologies were clunky and hard to use. But those who learned it back then are the giants of today — those that are so high in demand that their salary matches that of an NFL player.', 'Similarly, Julia is still very niche now. But when it grows, the big winners will be those who adopted it early.', 'I’m not saying that you’re guaranteed to make a shitload of money in ten years if you adopt Julia now. But you’re increasing your chances.', 'Think about it: Most programmers out there have Python on their CV. And in the next few years, we’ll see even more Python programmers on the job market. But if the demand of enterprises for Python slows, the perspectives for Python programmers are going to go down. Slowly at first, but inevitably.', 'On the other hand, you have a real edge if you can put Julia on your CV. Because let’s be honest, what distinguishes you from any other Pythonista out there? Not much. But there won’t be that many Julia-programmers out there, even in three years’ time.', 'With Julia-skills, not only are you showing that you have interests beyond the job requirements. You’re also demonstrating that you’re eager to learn and that you have a broader sense of what it means to be a programmer. In other words, you’re fit for the job.', 'You — and the other Julia programmers — are future rockstars, and you know it. Or, as Julia’s creators said it in 2012:', 'Python is still insanely popular. But if you learn Julia now, that could be your golden ticket later on. In this sense: Bye-bye Python. Hello Julia!']"
05/2020,Don’t Become a Data Scientist,OPINION,10.1K,75,https://towardsdatascience.com/@chris-writes-code,https://towardsdatascience.com/dont-become-a-data-scientist-ee4769899025?source=collection_archive---------1-----------------------,6,11,"['Don’t Become a Data Scientist', '1. There Are More Software Engineering Jobs', '2. There’s No Consensus What “Data Science” Means', '3. Data Science Is Siloed', '4. Data Science Is Exploratory', '5. Companies Aren’t Ready for AI', '6. Software Engineering Teaches Generic Skills', '7. Software Engineering Is More Transferrable', '8. Machine Learning Will Become A Tool For Software Engineers', '9. AI Is Not Replacing Software Engineers', 'Conclusion']",50,"['This is an opinion piece. I’d love to hear your counter arguments below.', 'Everyone and their grandmother wants to be a data scientist. But while data science may be the sexiest job of the 21 century, that discounts another rewarding and highly paid profession, the software engineer.', 'I often get messages from new grads and career changers asking me for advice on getting into data science. I tell them to become a software engineer instead.', 'Having experience in both, I’ll try to convince you to become the latter.', 'There’s an order of magnitude more jobs in software engineering compared to data science.', 'Below are couple screenshots after googling “data scientist” and “software engineer” jobs.', 'Thats 7616 data science jobs compared with 53,893 software engineering jobs. This is just jobs in the US but other countries showed similar results.', 'According to Glassdoor, data scientists make more money, but my untested hypothesis is that data science jobs are also on average more senior.', 'That said, if you’re offered a $1 million salary at Open AI, I recommend you take it.', 'Management often doesn’t have a consensus on what “data science” means. It’s also possible that given business constraints, they don’t have the luxury of rigidly following a framework of roles.', 'This means the responsibilities of a “data scientist” varies a lot from company-to-company.', 'While an ideal spectrum of roles between software engineer to data scientist may exist, it‘s unlikely that it’s followed in reality. This particularly goes for scaling startups still building infrastructure.', 'Hired candidates end up working on the problems a company currently needs solved, rather than the “role” they may have been hired for.', 'Anecdotal evidence from colleagues in the field is that many data scientists find themselves writing the backend code like software engineers. I’ve known other “data scientists” who crunched financials in excel.', 'This is in stark contrast to what you’d expect if you grew up on Kaggle competitions.', 'Most companies don’t need as many data scientists as software engineers. Other companies are hiring their first data scientist right now.', 'For this reason, many data scientists end up working alone, even if they sit at the same table as developers.', 'This can make it difficult to get feedback and second opinions. Software engineers either don’t understand predictive modelling, or are too busy working on completely different problems.', 'In contrast, one of the perks of being on a software engineering team is being able to say to colleagues, “I think we should implement ABC in XYZ way. What do you think?”.', 'Be prepared to have that conversation with yourself… or a rubber duck.', 'Be prepared for awkward conversations with management regarding why something you spent 2 weeks on cannot be used.', 'Working on solved vs un-solved problems is one of the fundamental differences between software development and AI.', 'Bugs and constraints aside, you know whether most software engineering projects are possible before beginning any work. The same can’t be said about ML where you don’t know if a model will be effective until after you build it.', 'Even in an era when every company is an AI company, most don’t have the infrastructure to support it or even need it.', 'The Head of Data Science for a rapidly scaling startup recently shared some advice over coffee.', 'First you figure out the problem, then you build the infrastructure, then you bring in the data scientists. This is not a quick process. (I paraphrase)', 'Another 1st-data-science-hire at a well-known company recently vented to me. She was forced to train AI models on big data on her laptop rather than in the cloud.', 'If you’re brought in without specific problems to solve, or the company isn’t prepared for data science, you may find yourself struggling to add value.', 'Becoming a junior software engineer is like getting an MBA in technology. You learn a little bit of everything.', 'You’ll learn databases, cloud technology, deployment, security, and writing clean code.', 'You’ll learn to manage building software by watching your scrum leader, senior developer or PM.', 'You’ll get mentorship via code reviews.', 'If you land in a company with an established engineering team, it’s almost guaranteed that you’ll level up your skills quickly and build a generalist background.', 'By providing a more holistic experience with technology, software engineering provides better exit opportunities when you’ve decided it’s time for a change.', 'DevOps, Security, Front End, Back End, Distributed Systems, Business Intelligence, Data Engineering, Data Science…', 'I’ve know a number of developers who moved from software to data science. If you skim data science job descriptions you’ll immediately notice they’re littered with core software development skills.', 'If you can build end-to-end projects you can also do more than build a model for Kaggle. You can take that model, productionize it, setup authorization and Stripe, then start charging users for access. That’s your own startup.', 'I’d never argue that data science isn’t transferrable. Making decisions based on data is a killer skill. But it’s also something that will become more of every job as we become more data driven.', 'As AI becomes commoditized and easier to use, software engineers will begin using it to solve their problems.', 'I can teach a developer to build Sklearn classifiers in an afternoon. This doesn’t mean they can build the next AlphaGo, but it does give them an alternative to hard coded conditional logic based on user input.', 'Data scientists have specialized knowledge like statistics and an intuition for how models work. But DevOps and Security engineers have their own specialized knowledge as well.', 'I’d argue that these are more common than different. A seasoned software professional could move between specialties an order of magnitude faster than a new entrant could pick one up.', 'While I don’t think we’ll see a complete merger of data science into software engineering, it does feel like data science could become another software engineering specialty.', 'As silly as it sounds, I got into software engineering in 2014 because I worried AI would render every other job obsolete.', 'Yet since then, the dial barely moved outside specific environments. Technology adoption is slow and AI is narrower than the media would have you believe.', 'Compared to other professions, machine learning is even further from automating software engineering. While we have startups building cool products like AI enabled code completion, writing code isn’t the real job. The job is solving problems using technology.', 'Pre-singularity, that will remain a valuable and highly paid skill.', 'Firstly, this is anecdotal. Secondly, I realize I conflated data scientists, ML engineers and AI researchers. But I think these arguments are still worth considering, given this is your career.', 'Don’t take it too seriously. I’d prefer you research and make your own decision. That’s part of being a data scientist after all :).', 'At the end of the day, we’re paid to solve problems.']"
05/2020,"Sorry, Online Courses Won’t Make you a Data Scientist",The reason why you should really stop signing up…,5.3K,48,https://towardsdatascience.com/@yadramshankar,https://towardsdatascience.com/sorry-online-courses-wont-make-you-a-data-scientist-8639d5f00889?source=collection_archive---------2-----------------------,8,5,"['Sorry, Online Courses Won’t Make you a Data Scientist', 'Why work on projects?', 'Making that transition into project-based learning', 'But, why is project-based learning not easy?', 'The Final Point']",26,"['It was a weekend. I had just finished another online course on Data Science. I felt accomplished. Well, after “successfully completing” 5 different courses and receiving “certificates” for each of them, anybody would tend to be of the confidence that they are now real data scientists. I too was no different.', 'But online courses can be a funny thing. Most of them have glittering descriptions, a huge list of topics that they would cover, promises to make you adept at one or more skills and if we are lucky, we can also see a bunch of testimonies from other participants that are usually about how the course saved a participant from perpetual doom and made him or her an absolute champion in the field. But, what really most of us look for are those final certificates that we receive. Those online, colourful documents bearing our names make the biggest difference to most of us. But one day, we sit in a room across our potential employer and see that most of them seem less impressed than we expected with our certificates. Some don’t even extend the courtesy of acknowledging the time spent on getting those certificates. They just get straight to the point and say, “These certificates are useless if you have not worked on any projects”. Now, it’s hard when somebody tells you that. Even more because our hopes on doing well at that interview and getting selected was primarily pinned on the online courses we had taken. It was our investment of money, time and effort. And to have our primary weapon disqualified even before the contest would weaken anybody’s confidence.', 'Trust me. I have been there as a second year undergraduate, one who carried 2 copies of a minimalistic resume, 5 different certificates and a huge bundle of hope into 7 different rooms in one day, each with a different company at the college’s annual internship fest. I sang the same song of “I have certificates from these courses…” in the first 6. In the 6th company, I received the knockout punch that came in the verbal form of “Look. It’s great you finished these courses. But, you have not worked on anything. You don’t have a Github account. We don’t know your capabilities. So, we are sorry.”', 'Ouch! That hurt somewhere deep inside. I could feel that usual clogging of my wind pipe that prevents me from talking clearly, something that’s very characteristic of me when I am hit hard by emotion. But, it was clearly not their fault. They showed me the reality, one that I had always been skirting away from.', 'We all come across that one time in our lives when somebody picks up a mirror and places it in front of our faces. This was that time in my life.', 'Well, the choice is ours whether to shut our eyes or keep them open when this happens. I decided to keep them open and that’s probably what’s made all the difference. That’s what makes me write this article.', 'The reason we are all so keen on completing courses is because we tend to think of an online course as a positive abutment to our academic degree and one that would be favourably viewed by employers. As a college degree is mandatory in several organizations to even be considered for a job, we nurse the opinion that an online course would be equally valued and considered as “Extra Learning”. Well, it would be and there is little doubt on that.', 'However, given the amount of exposure the internet provides to everybody, an online course is available to anyone. Therefore, even if we complete a course, there is no particular edge that we have over others who have done the same. And companies compare candidates because that’s their only way to easily choose the candidates they feel will suit their needs. Competition is in the nature of everything we do. So, the only way to stay distinguished from others is to work on projects.', 'Another reason to work on projects is to learn. Online courses definitely teach us a great deal, but they are constrained by the very force that prevents teachers from giving all their expertise to students in the classroom — The Syllabus. Online courses have to be planned and anything that has to be planned will have a trade-off of not being able to address all possible aspects of the topic in discussion.', 'On the other hand, if we work on a project, every step we undertake will cause us to learn a new concept. The mistakes we make will be far more than what we make while following an online course. However, if we are willing to learn from these errors, the knowledge we assimilate will be rather plentiful and useful.', 'After being cut down to size in that interview, I went home as a determined individual. I was determined to start working on projects and not just bank on my certificates. But, making a transition from our natural inclination to a new practice is probably the toughest thing to do.', 'I read a few articles on how to apply data science and work on projects. Then I had a few light conversations with my peers in the following days. A few conversations were very discouraging, to be frank. Not that my peers were bad conversationalists, some of them just seemed way smarter than I was. They seemed to be doing very impressive stuff. Some were making a drone and the only time I ever saw a drone was on television. Funnily, I wanted to work with them the moment I heard that they were making a drone. But, I didn’t want to ask. I felt I was not good enough to work on cool projects like that. Sigh! We all make mistakes…', 'The most confounding challenge, however, was that I had only been used to courses where my progress was determined by weekly assignments. The grading system was formalized and somebody was grading me. In a project, things were different. I had to self-evaluate. And, I was unable to do this. I was never able to determine whether I had done something well enough. I was unable to be my own evaluator.', 'Sometimes, we are just too easily compelled to hand over the reigns of our life in another’s hands. And the force that compels us to do so is often our own inability to understand our strengths and weaknesses.', 'I realised that I needed to prepare myself to be my best evaluator. This is what I did. I sat down and drafted my project idea and even set goals to be achieved, ridden with time limits. Honestly, I overshot every deadline but made sure I at least finished 80% of what I had decided to do.', 'The whiteboard hanging in my room was witness to my daily planning, my cheat days, the concepts I learnt, the concepts I tried to redefine and the block diagrams I always loved drawing. My first project was to analyze chocolate bar ratings. A pure EDA project, it provided me with my first experience at working on a self-decided project. I worked on it because I was interested in knowing more about the chocolate ratings across the world.', 'It’s not difficult for us to work on a project if we feel connected to it in some way or the other. So, maintaining this personal touch and being excited by the cause than the tools used is important to being able to stick to finishing a project.', 'If that statement about being excited by the cause seems a bit difficult to understand, here is another article, where I have clearly been excited by the cause!', 'Often, I have switched off in between lectures at college and picked up a sheet of paper and scribbled furiously about ideas that would have just struck me. A good deal of these ideas never saw implementation because they didn’t seem impressive enough to go through with. It felt as though these ideas would not help me become a cool data scientist. Wanting to be a cool data scientist was too ambitious, given the fact that I was a far cry away from even being a data scientist (I still am). Yet, the hysteria of wanting to work on cool projects kept me chained to the pole of illogic. I was unknowingly searching for my drone.', 'Even the chocolate analysis project was initially shelved by me for not sounding cool enough. Thank god I reconsidered. Most of us shelve projects because we feel it’s not good when compared to what somebody else has worked on. It’s a self-harming thought, to be frank. We fail to see that no two people X and Y share the same background. So, we can’t expect to be as good or better than somebody else at all times. All we can do is try. The result is really not in our hands. Those who work on complex projects probably know way more than us.', 'We can’t expect to make a ship without knowing the property of buoyancy. Trying to do so would just be plain stupid.', 'So, do we want to be stupid? I assume not.', 'There is never a perfect way to work on academic projects. This is mainly because every student has a different approach to work on projects. Some do it for the grades, some do it to learn and some for both. Some see projects as a way to work in their comfort zones, while others view projects as ways to learn newer concepts. The combinations and approaches to working on academic projects are many. However, a few important points I have noticed during my undergraduate degree in CSE is what I will leave you with here.', 'Finally, I don’t discredit online courses. Some certifications really matter to employers. Also, courses are a wonderful place to learn specific skills. Some of them are taken by highly reputed practitioners and equip us well. In fact, I learnt my first steps in data science from the wonderful Datacamp. But, the equipment alone never makes you good.', 'This wonderful article by Sanyam Bhutani provides great insight into the right ways to do any online course, or as he would prefer it, the wrong ways to never do an online course.', 'So, what are you waiting for? Pick up a project, start work on it and notice the magic of applied learning for yourself.']"
05/2020,Do Not Use “+” to Join Strings in Python,A comparison of the approaches for joining strings in Python…,4.7K,31,https://towardsdatascience.com/@qiuyujx,https://towardsdatascience.com/do-not-use-to-join-strings-in-python-f89908307273?source=collection_archive---------3-----------------------,5,5,"['Do Not Use “+” to Join Strings in Python', 'Beginning', 'Join Multiple Strings', 'Logic Behind join() Method', 'Summary']",22,"['When I start to use Python, it is very intuitive and easy to come out to use the plus operator + to join string, as many programming languages do such as Java.', 'However, soon I realised that many developers seem to like to use the .join() method rather than +. In this article, I’ll introduce what’s the differences between these two approaches and why you should not use +.', 'As a beginner, or someone has just switched from other languages that use + to join strings, it is very easy to write code like this:', 'As you use Python more and more, you may realise that someone else prefers to use the join() method like this:', 'Honestly, when I saw the above method first time, I was thinking that this is not intuitive and looks kind of ugly.', 'Nevertheless, one time I need to join multiple strings in a list.', 'Initially, I have done it like this:', 'In this example, I have to write a for-loop to join the strings one by one. Also, the result string needs to be trimmed a white space I added at the beginning because all the strings need to be added a white space in the front, but not the first one. You may have other solutions such as adding an index to the for loop so that the string at the index = 0 should not be added this white space. Anyway, you will still need this for-loop and do something for the white spaces.', 'After that, I recalled that I’ve seen the .join() method before, maybe this is the time that I need to use it!', 'How easy it is! One line of code does everything. Since the .join() method is called by a string object, the string object will be utilised to join every string in the list, so you don’t need to worry about the white spaces at the beginning.', 'But wait, do you really think this is the only reason why we need to use the join() method rather than +? No, please read the next section.', 'Now, let’s compare these two methods in terms of their performance. We can use the magic method%timeit of Jupyter Notebook to evaluate them.', 'The performance shown above is based on 100k trials so that the results are very confident and obvious. Using thejoin() method can be 4 times faster than using + to join the strings in the list.', 'Why?', 'Here is a conceptual graph that I drew for demonstrating of the approach using + to join the strings.', 'This shows what the for-loop and the + operator did:', 'However, what happened for join() method?', 'Therefore, it is obvious that the major difference is that the number of times for memory allocation is the main reason for the performance improvement.', 'Imagine that it is already 4x faster to use thejoin() method to join 6 strings together. What if we are joining a very large number of strings? It will make a much larger difference!', 'In this short article, I have compared the differences between the + operator and the join() method when joining strings in Python. Apparently, the join() method is preferred because of its performance.', 'Learning a programming language is usually a long curve, but Python makes it relatively shorter for beginners, which is absolutely great. After we have entered the door, start to use Python, we should not stop there and satisfy what we can do use Python. Usually, the difference between a master and a regular developer comes from the knowledge in details.', 'Let’s keep finding more tips of Python to make ourselves closer to a Python Master!']"
05/2020,Coding Mistakes I Made As A Junior Developer,There is hope if you’re struggling in your first job…,2.4K,7,https://towardsdatascience.com/@chris-writes-code,https://towardsdatascience.com/coding-mistakes-i-made-as-a-junior-developer-e151dd3b3c7d?source=collection_archive---------4-----------------------,4,11,"['Coding Mistakes I Made As A Junior Developer', '1. Writing Clever Rather Than Readable Code', '2. Using Variable Names That Give No Context', '3. Allowing Security Holes', '4. Writing Code Immediately After Reading A Feature Ticket', '5. Commenting Too Much Or Too Little', '6. Pushing Duplicate and Unused Code', '7. Writing Inefficient Database Queries', '8. Using Error Based Conditional Logic', '9. Submitting Code For Review That Combines Multiple Features', 'Conclusion']",35,"['Your first job in software engineering or data science can be demoralizing. Especially if you don’t have a background writing code.', 'I often get messages from people asking for advice on how to improve. But what they really need is someone to tell them — “you can do it!”', 'Below are mistakes I personally made during my first software engineering job. This should make you feel better if you’re having a tough time.', 'Writing good code is hard. Understanding bad code is harder. But this isn’t intuitive when you’re started out.', 'Thankfully I had a senior developer who called me out on each of the below points more than once.', 'Compressing logic into as little space as possible made me feel smart. But it also made my code unreadable. Now I always try to err on the side of readability.', 'Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.- Kernighan’s Law', 'Coming up with good variable names is surprisingly hard and I wanted to complete tickets as fast as possible.', 'So I’d choose the first name that popped into my head.', 'Both are bad ideas and made it difficult for anyone to understand what I’d written (including myself).', 'This is another situation where I’m thankful to an awesome senior developer who saved my code from getting hacked.', 'I’ve done all the following:', 'It took a long time to build up a mental checklist of best security practises, which I now use when reviewing other developers’ code.', 'Spending a week on a feature and then realizing its the wrong feature is embarrassing. I’ve done it more than once.', 'Taking a breath, understanding the business problem, and planning code around it is a huge multiplier for engineers.', 'Learning from this, I make new developers in my own startup plan out tickets in detail before beginning. This level of micro planning helps clarify thought and develop more effective solutions.', 'In the beginning I commented nothing.', 'Then I went through a phase where I commented every line. A method called add_two_numbers would have been commented with # adds 2 numbers. This is too much.', 'In retrospect, the right amount of commenting didn’t click until I’d read enough code written by other developers, and noticed where I wished they had added comments.', 'I’ve done all the following:', 'Some frameworks autogenerate a lot of unnecessary files. You also don’t know all the existing code when you start working on an app.', 'Anecdotally, I’ve found the best way to avoid these problems is walking through code you’ve written in detail before submitting it for review.', 'I knew zero about databases when I started my first job. It probably took me a year before I figured out database indices.', 'In that time I wrote a lot of N+1 queries, and created db tables to store large amounts of data without indices.', 'Both are recipes for an annoyingly slow app.', 'Conditional if/else statements are a core part of software.', 'In pseudocode, they typically look like this.', 'But the first app I wrote for my portfolio was filled with logic like this.', 'Sometime we need to rescue an error, like when hitting an unreliable API. But this should be the exception rather than the norm.', 'One of the first things I learned was to not combine multiple features in the same pull request. It’s not nice to the person reviewing code.', 'Going beyond a couple hundred lines can make it difficult for others to mentally walk through different paths of execution.', 'Sometimes this is the result of poorly scoped tickets. So I always tell new developers to push back if they think a ticket could be further broken down into sub tickets. Smaller is better.', 'Learning to write software is hard. There’s a hundred moving pieces that you can only learn by doing.', 'I hope that reading about my fumbles makes you feel better if you’re struggling yourself.', 'What helped me the most was having a senior developer give detailed feedback on every piece of code I submitted. Find a company/team where you get that. It’s the fastest way to improve.']"
05/2020,Why Data Science might just not be worth it,OPINION,2K,33,https://towardsdatascience.com/@radecicdario,https://towardsdatascience.com/why-data-science-might-just-not-be-worth-it-c7f3daee7d8d?source=collection_archive---------5-----------------------,6,5,"['Why Data Science might just not be worth it', '1. Data Science is Boring', '2. Data Science is Getting Automated', '3. Job Listings are Just Awful', 'The Verdict']",32,"['The supposed hottest job of the 21st century might not be so hot after all. Data science has been with us for some time now, and it’s no longer just a buzzword. Both people and companies have utilized it to create value and money, but is it really a profession of the future?', 'Note from the author: This is an opinion piece, so it’s probably biased to a degree. Jobs in your country and with your skillset may vary. We don’t see the world through the same eyes. Please leave your thoughts and experiences in the comment section.', 'If you’re doing anything software related you’ve probably considered the option of switching to the field of data science. And why shouldn’t you, the jobs are supposedly everywhere, salaries are generally higher than in software development, and having the word “scientist” in your job title would make your mother proud.', 'Well, maybe not the last one, but you get the point.', 'After being in the field for a while now, exploring a good bit of libraries and other cool stuff, writing around 80 data science-related article, whilst in the same time exploring other options (like web and mobile development) on the side, I find my self to be qualified enough to break down the good and bad things about the field.', 'The focus today will primarily be on the bad things because the Internet is full of “why you should become a data scientist” and “learn data science in a month” type of articles.', 'With that being said, the ideal reader is someone who knows what will be the benefits from enrolling in the field but would also want to know what are the possible drawback. Also, someone who’s already in the field for a while might also find these points useful too.', 'Okay, without further ado, let’s begin with the first one!', 'Yes, you’ve read that right. Most of the data science boils down to data extraction from the source table(s), performing some aggregations and calculations down the road and then storing results in the new table which is suitable for analysis. Well, that describes an ETL process perfectly, not data science.', 'Further, you’d spend some time cleaning and preparing data, which once again isn’t purely data science-related. Finally, we have the segment which deals with predictive modeling — this one also isn’t new, but gained in popularity a lot in the last couple of years.', 'Those three components combined, alongside with decent presentation and communication skills make your average data scientist.', 'But wait, won’t data science will revolutionize the world?', 'Yes and no. Yes in the term that professionals will be able to do their jobs better (like doctors), and no in the term that your ETL pipeline interest absolutely no one, and has nothing to do with your “data scientist” job title.', 'Or at least, the fun parts of it. You know, the buzzwords which made you enroll in the field in the first place. Stuff like predictive modeling, machine learning, etc. Don’t get me wrong, a lot of stuff can’t be automated here (yet), but a good portion of it already is.', 'And that’s sad, because whilst masses were worried about routine jobs getting automated we’ve actually automated everything interesting about our job. Nice.', 'I mean, have you seen professional cloud environments? If not, that’s okay, because they are pretty sad to look at anyways. Basically you have a limited number of algorithms that you can fit, and as long as the data is prepared in the right way, anyone who knows that it’s better to have a higher accuracy can test out all combinations and get a pretty decent solution!', 'I mean, it won’t be better than the one developed by a senior data scientist with 10+ years of experience, but ask yourself — how important really is that 2% increase of accuracy?', 'Try to think of this both as an employee and an employer. Is the marginally better model produced by a team of data scientists actually worth the time and money? It is, for some companies, but most of them will be perfectly fine with what “enterprise cloud environment” spit out.', 'Go over to your favorite job listing website and search for data science jobs. You’d expect skills like SQL, Python, R, Statistics to pop up — and you’d be absolutely right to make this assumption. The problem is, those will be only 30% of the requirements!', 'Some others may include programming in general, APIs, version controlling, and maybe even some frontend skills. And that sucks for you.', 'Even if you are coming from the software developer background and have all of the mentioned skills under your belt, there still isn’t that jobs in data science.', 'Let’s take a look at listing from Indeed.com, taken on the 16th of May, 2020, limited only to the United States:', 'Now that’s a big difference.', 'You might be thinking now that software development is much broader than data science, hence more jobs are available. That’s absolutely true, and that’s why I’ve also searched for a narrower field in software development — Java developer. Let’s see how many job listings there are:', 'Yeah, around 3 times more.', 'But even if this doesn’t convince you enough, the following few words most certainly will — most data science positions are senior.', 'That’s right. Most small and medium-sized companies don’t have the same need for data scientists as they have for software developers. Maybe they are even searching for their first data scientist! Do you really think they would hire an intern or a junior to handle the data science-related work? Think twice.', 'Maybe I came a bit too harsh on data science. Maybe the situation is different in your country. But maybe it isn’t — and you should be aware of both pros and cons when making a decision this big.', 'In the world where machine learning is automated to a degree by which regular software developers feel confident enough to use it, I’d think twice if I was to choose my future profession today.', 'Don’t get me wrong here, data science is still awesome, but just be prepared to spend around 90–95% of the time on the ETL, data processing and preparation, and the other 5–10% doing what really interests you (predictive modeling).', 'What are your thoughts? What’s the situation like in your country? Let me know in the comments below.', 'Thanks for reading.']"
05/2020,Machine Learning Engineer vs Data Scientist (Is Data Science Over?),vs Data Analyst vs Research…,1.3K,9,https://towardsdatascience.com/@jasjung,https://towardsdatascience.com/mlevsds-3c89425baabb?source=collection_archive---------6-----------------------,11,7,"['Machine Learning Engineer vs Data Scientist (Is Data Science Over?)', 'Hello I’m Jason', 'Introduction', 'Trend of Data Science Industry', 'Different Data Scientists and How to Choose Them', 'ML Engineer vs Data Scientist', 'Conclusion']",33,"['I work as a data scientist (which we will define more later in this article) in Silicon Valley, and I love to learn new things!', 'Man, this topic has been in the back of my mind for a long time. But because there are so many things to potentially cover, I couldn’t get myself to finish this daunting task. But, stuck in my room due to the shelter-in-place order and running out of things to waste time with, I finally decided to finish it.', 'As its popularity has exploded since 2013, the data science industry has been wildly evolving yet slowly converging into more specific roles. Inevitably, this caused confusions and inconsistent job functions during its growth. For example, there are seemingly many different titles with the exact same roles or same titles with different roles:', 'Analytics Data Scientist, Machine Learning Data Scientist, Data Science Engineer, Data Analyst/Scientist, Machine Learning Engineer, Applied Scientist, Machine Learning Scientist…', 'The list goes on. Even for me, recruiters have reached out to me for positions like data scientist, machine learning (ML) specialist, data engineer, and more. Clearly, the industry is confused. One of many reasons for such a high variance is that companies have very different needs and uses of data science. Regardless of the reason, it appears that the field of data science is branching and merging into these top few categories: Analytics, Software Engineering, Data Engineering, and Research. No matter what the similar titles say, they usually fall into these categories. This specialization is most true in larger tech companies that can afford it.', 'In this article, we will first look into the overall trend of the data science industry and then compare ML engineer and data scientist in more depth. I do not mean to provide an extensive history but rather narrate what I have seen and experienced while living in Silicon Valley as a data scientist. Even when I wrote my article How to Data Science Without a Degree in 2017, my perspective on data science was very different.', 'Last year, I covered this topic when I was invited to give a short talk to data science students at Metis Bootcamp. I want to use this opportunity to explain the differences and help you find the role that suits you best. Let’s find out if this industry is still booming or ending with data, because that is what data scientists do, right? (Maybe not). Regardless, I hope you find it useful and informative.', 'Before we dig deeper, take a look at the following two job descriptions that I found on LinkedIn. Try to guess what title these descriptions are for. I highlighted some key points in red:', 'Very different, right? Surprisingly, both are for a data scientist position. Left is for Facebook, right is for Etsy. I do not mean that one is better than the other. The main point is seeing how different they are.', 'Even at work, people have active discussions on trying to figure out what exactly defines a data scientist. I’ve seen people describe data scientists as computer science PhDs or new data analysts. This is because different companies use the term data scientist for very different positions. However, I believe the industry has been learning to be more specific and have more specialized roles, instead of bucketing everything into the broad scope of data science.', 'Then, what are some different roles that data scientist can imply? Largely, I think they are software engineers, data analysts, data engineers, and applied/research scientists. I have seen my friends with the same data scientist title but their role is one of the four. Check out the diagram that I created below. In the early days of data science, data scientist might have included all of these four roles. However, today positions are becoming more specific and specialized, as seen in the diagram below.', 'Is this trend surprising? According to the famous article Data Scientist: The Sexiest Job of the 21st Century, not so much:', 'Data scientists’ most basic, universal skill is the ability to write code. This may be less true in five years’ time, when many more people will have the title “data scientist” on their business cards.', 'As the article suggests, you have less reasons to be a good coder today as a data scientist. Before, tools and methods to analyze big and nasty data were not as accessible and user friendly before. This required data scientist to have a relatively strong engineering skill on top of other skills. But tools for ML and data science have developed quickly and are now more accessible than ever before, such that you can access state of the art (SOTA) models with just a few lines of code. This makes the separation of roles into analytics or engineering easier. Now we do not have to focus on learning all of analytics, engineering, and statistics to become a data scientist, which seemed like the case before.', 'For example, Facebook led this trend in which data analyst jobs have evolved into data scientists. This was a natural process because with an increasing data size and more challenging data problems, more skills and training were needed to perform good analysis. Not only Facebook, but many other companies like Apple, Airbnb have been putting a clearer distinction between analytics/product data scientist vs ML data scientist.', 'It is worth mentioning that specialization occurs more in larger tech companies. Unlike software engineers, who are needed in tech companies of all sizes, not all of these companies need specialized research scientists or ML engineers. Having a few data scientists might be enough. So in smaller companies, there still are data scientists who might be functioning within all four roles.', 'As a rule of thumb today, data scientists in big companies (FANG) are often similar to advanced analysts, while data scientists in smaller companies are more similar to ML engineers. Both functions are important and needed. Going forward, I will stick to my new definitions by which data scientist implies an analytics function.', 'In the chart below, I tried to show a similar picture as the above diagram but with a bit more detailed view of the four functions. The descriptions aren’t perfect but you can refer to it.', 'If you are trying to get into this field, whether as an ML engineer or a data scientist, you might wonder which one you should choose. Let me list out a simplified (and stereotypical) description of the four main ML-related roles to help you clarify. Though I have not personally worked as all of those titles, I have learned insights from friends in each field. I also provided potential interview content in the parenthesis (think of it as four rounds of interviews).', 'Obviously, these descriptions aren’t exhaustive. But when talking to my friends and looking at many job descriptions, I found these ideas to be common. If you are unsure about the role to which you are applying, here are a few tips to learn more:', 'Okay, that was long. Now back to our topic. In recent years, I started to hear people say more negative things about the data science job. A few reasons for this is that there are more and more data scientist jobs that no longer seem to have a cool machine learning factor and seem easier to obtain. Perhaps five years ago most job descriptions required at least a Master’s degree to get a data scientist job, but that is no longer the case. Whatever the reason why people think the data science (of old days’ at least) is over, let’s look at some data.', 'The below data and chart are from a world-renowned salary database engine, Salary Ninja. It searches over the H1-B database based on foreign workers in the United States. You will see the average salary and number of job positions that have either “Data Scientist” or “Machine Learning Engineer” in the job title between 2014 and 2019.', 'Are you surprised by the result? Even though the average salary is similar for both titles, you can see that the average decreased for data scientists in 2015 and 2016. Perhaps that is what people mean by good days are over for data scientists. In terms of sheer quantity, data science is much bigger than ML engineering, but you can see that ML engineers are growing faster and have higher salaries.', 'For your amusement, I included a summary statistics that I gathered from Salary Ninja of the few roles we have discussed in this article. I did an overall summary of the past six years (first table) and its subset with the most recent year in 2019 (second table). Lastly, I included a table for just one company, Microsoft (third table).', 'I learned a few interesting insights:', 'According to this data, I cannot say that the data science industry is a bust. It is still growing but possibly with more focus in analytics. From what I have observed, it seems to be true that there are more data science jobs that require fewer prerequisites, but that is not a bad thing.', 'I talked about a lot of things but I hope you stayed with me. I wrote this article because I myself was confused about all the changes that were going on in the industry. Also, it seemed like people have so many different opinions about what data science is. Regardless of who is right or wrong, I hope you can see the trend and decide for yourself.', 'In the end, do not choose a job or industry because it has the higher average salary or because of the buzz words. It does not matter if your title is data scientist or ML engineer or data analyst. It does not matter if someone says data scientist is an engineer or an analyst because both can be true.', 'Though it is easy to compare job titles based on pay, it is far more important to choose a role you enjoy and are good at. Focus on the actual work you do and make sure it suits you. Just because the average pay may be lower, it doesn’t have to mean that you will actually get paid less. As you saw earlier, all of the roles I discussed have a very high maximum pay.', 'Before I conclude, there are a few other resources that you can refer to for more information:', 'Thank you again for reading. My wish is that this article has given you some insights so that you won’t be lost while looking into the world of data science and machine learning. As always, comment below if you have any questions. I wish you the best during this difficult time, and I hope you find this article useful. Until next time.', 'If you are bored, check out my previous articles and projects.', '2020–06 Update:']"
05/2020,A Better Way To Become A Data Scientist Than Online Courses,Opinion,3.8K,24,https://towardsdatascience.com/@chris-writes-code,https://towardsdatascience.com/a-better-way-to-become-a-data-scientist-than-online-courses-2abc343d4d55?source=collection_archive---------7-----------------------,4,9,"['A Better Way To Become A Data Scientist Than Online Courses', '1. Solve A Real Problem With Machine Learning', '2. Find A Mentor Who Is An Artificial Intelligence Expert', '3. Do A Machine Learning Internship', '4. Start Doing Data Science In Your Current Job', '5. Do A Data Science Bootcamp', '6. Become A Software Engineer First', '7. Do A Technical PhD or Master’s Degree Then Apply For Jobs', 'What About Online Courses?']",44,"['This is an opinion piece. I’d love to hear your counter-arguments below.', 'Do you want to be a data scientist?', 'I met 50+ data scientists for coffee and worked with a few more.', 'Here I’ll explain how these people made the transition into data science.', 'P.S. It was not with online courses.', 'Pick a real problem, then solve it with machine learning.', 'This is hard because there is no roadmap. But it provides real experience and a story to market yourself, regardless if you succeed or fail.', 'Here are problems you could solve:', 'If your solution works (or even barely works), build a UI that others can use and post it on Hacker News or Product Hunt.', 'Add the experience to your resume with your title as “Data Scientist”. If it solves a problem with machine learning, no one will care that it was a one-person show.', 'Now you can tell this as a story in an interview, which will carry more weight than an online certification.', 'Build a relationship with someone experienced who can recommend AI-driven solutions to problems you’re trying to solve.', 'This is how I broke into data science.', 'As a software engineer, a startup accelerator loaned my company an AI PhD for a few hours each week.', 'Each week, we discussed problems and potential solutions, I worked on the implementation, then we reviewed and repeated. After 6 months we solved several important problems and the experience was invaluable.', 'I followed the below process to find subsequent data science mentors.', 'Take a short-term job where you’ll be paid less but will get your hands on a real projects implementing AI.', 'A former ML intern at my startup is now a data engineering intern at Facebook, which will probably roll into a full time offer.', 'This path isn’t for everyone and works better if you’re still young or in school. Not everyone can afford to quit their job and become an intern, but you may be able to find something part-time or online.', 'The important part of this is getting AI-related experience on your resume.', 'Figure out how your current company can use AI to solve a problem. Then solve it.', 'You may not have time between 9 and 5 while you’re at work. But if you’re motivated, do it at night or on the weekend. Then share what you accomplished.', 'If your current company is small, no one will argue against you adding more value. If it’s really valuable, you may be allowed to work on it as one of your day-to-day projects.', 'Afterwards, put the project on your resume and update your job title, if you can.', 'Attend a paid data science bootcamp.', 'This costs money and not all bootcamps are equal, but I know at least 10 people who broke into data science post-bootcamp, and all with large reputable companies.', 'The best bootcamps only accept PhDs, so it is possible that candidate success relies on survivorship bias (bootcamps accept students who would be successful anyway).', 'Bootcamps benefit candidates in several ways.', 'That said, not every graduate will get a job.', 'I wrote about this here.', 'As long as data scientists solve problems with code, there will be heavy overlap with software engineering.', 'After gaining experience as a software engineer, find a data science job that uses a similar tech stack to your experience (ie: database, language, framework, packages).', 'If you can check off most job requirement boxes, you have a good chance of getting an interview.', 'There are other benefits of becoming a software engineer first.', 'Do you have 2 to 6 years to study? I don’t.', 'But most of the data scientists I met followed this path.', 'They have either:', 'I wouldn’t recommend going back to school to break into data science. But if you’re currently in school, and can transfer to an AI-related degree, do it. Anecdotally, the highest paying AI salaries require advanced degrees.', 'While costly and time consuming, traditional degrees carry a level of trust that online certification don’t.', 'There IS a place for online courses. But it’s not for getting a job.', 'The benefits of courses include learning what you don’t know, and deep diving into specific techniques.', 'But on the flip side, courses give the feeling of accomplishment, without making you do the hardest and most sellable thing, solving a real problem.', 'I’d say, find a problem to solve, then use online courses to learn to solve it.', 'How have you seen people break into data science? I love to hear about it below.']"
05/2020,Confusion Matrix for Your Multi-Class Machine Learning Model,A beginner’s guide on how to calculate…,237,2,https://towardsdatascience.com/@joydwipmohajon,https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826?source=collection_archive---------8-----------------------,6,3,"['Confusion Matrix for Your Multi-Class Machine Learning Model', 'Confusion Matrix for Binary Classification', 'Confusion Matrix for Multi-Class Classification']",34,"['A confusion matrix is a tabular way of visualizing the performance of your prediction model. Each entry in a confusion matrix denotes the number of predictions made by the model where it classified the classes correctly or incorrectly.', 'Anyone who is already familiar with the confusion matrix knows that most of the time it is explained for a binary classification problem. Well, this explanation is not one of them. Today we will see how does a confusion matrix work on multi-class machine learning models. However, we will start with a little background using a binary classification just to put things in perspective.', 'As you can see, a binary classification problem has only two classes to classify, preferably a positive and a negative class. Now let’s look at the metrics of the Confusion Matrix.', 'True Positive (TP): It refers to the number of predictions where the classifier correctly predicts the positive class as positive.', 'True Negative (TN): It refers to the number of predictions where the classifier correctly predicts the negative class as negative.', 'False Positive (FP): It refers to the number of predictions where the classifier incorrectly predicts the negative class as positive.', 'False Negative (FN): It refers to the number of predictions where the classifier incorrectly predicts the positive class as negative.', 'It’s always better to use confusion matrix as your evaluation criteria for your machine learning model. It gives you a very simple, yet efficient performance measures for your model. Here are some of the most common performance measures you can use from the confusion matrix.', 'Accuracy: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: (TP+TN)/(TP+TN+FP+FN).', 'Misclassification Rate: It tells you what fraction of predictions were incorrect. It is also known as Classification Error. You can calculate it using (FP+FN)/(TP+TN+FP+FN) or (1-Accuracy).', 'Precision: It tells you what fraction of predictions as a positive class were actually positive. To calculate precision, use the following formula: TP/(TP+FP).', 'Recall: It tells you what fraction of all positive samples were correctly predicted as positive by the classifier. It is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN).', 'Specificity: It tells you what fraction of all negative samples are correctly predicted as negative by the classifier. It is also known as True Negative Rate (TNR). To calculate specificity, use the following formula: TN/(TN+FP).', 'F1-score: It combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. It can be calculated as follows:', 'Now, in a perfect world, we’d want a model that has a precision of 1 and a recall of 1. That means a F1-score of 1, i.e. a 100% accuracy which is often not the case for a machine learning model. So what we should try, is to get a higher precision with a higher recall value. Okay, now that we know about the performance measures for confusion matrix, Let’s see how we can use that in a multi-class machine learning model.', 'For simplicity’s sake, let’s consider our multi-class classification problem to be a 3-class classification problem. Say, we have a dataset that has three class labels, namely Apple, Orange and Mango. The following is a possible confusion matrix for these classes.', 'Unlike binary classification, there are no positive or negative classes here. At first, it might be a little difficult to find TP, TN, FP and FN since there are no positive or negative classes, but it’s actually pretty easy. What we have to do here is to find TP, TN, FP and FN for each individual class. For example, if we take class Apple, then let’s see what are the values of the metrics from the confusion matrix.', 'Since we have all the necessary metrics for class Apple from the confusion matrix, now we can calculate the performance measures for class Apple. For example, class Apple has', 'Similarly, we can calculate the measures for the other classes. Here is a table that shows the values of each measure for each class.', 'Now we can do more with these measures. We can combine the F1-score of each class to have a single measure for the whole model. There are a few ways to do that, let’s look at them now.', 'This is called micro-averaged F1-score. It is calculated by considering the total TP, total FP and total FN of the model. It does not consider each class individually, It calculates the metrics globally. So for our example,', 'Hence,', 'Now we can use the regular formula for F1-score and get the Micro F1-score using the above precision and recall.', 'Micro F1 = 0.28', 'As you can see When we are calculating the metrics globally all the measures become equal. Also if you calculate accuracy you will see that,', 'Precision = Recall = Micro F1 = Accuracy', 'This is macro-averaged F1-score. It calculates metrics for each class individually and then takes unweighted mean of the measures. As we have seen from figure “Precision, Recall and F1-score for Each Class”,', 'Hence,', 'Macro F1 = (0.40+0.22+0.11)/3 = 0.24', 'The last one is weighted-averaged F1-score. Unlike Macro F1, it takes a weighted mean of the measures. The weights for each class are the total number of samples of that class. Since we had 11 Apples, 12 Oranges and 13 Mangoes,', 'Weighted F1 = ((0.40*11)+(0.22*12)+(0.11*13))/(11+12+13) = 0.24', 'Finally, let’s look at a script to calculate these measures using Python’s Scikit-learn.', 'Here is the output of the script.', 'Hope you found what you were looking for. Thanks for reading.']"
05/2020,Forget about Python. Learn COBOL and become a crisis hero,Opinion,2K,36,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/forget-about-python-learn-cobol-and-become-a-crisis-hero-7f15e75ff377?source=collection_archive---------9-----------------------,5,5,"['Forget about Python. Learn COBOL and become a crisis hero', 'Why nobody knows COBOL any more…', '…And why it’s still everywhere', 'Demand is sky-high', 'Not sexy, but worth it']",23,"['What’s the oldest programming language that is still on the market? Python is in its thirties. C is almost fifty years old. Fortran is in its sixties. And still in use!', 'There’s another computer language dinosaur that’s used intensely. One that you’ve probably never heard of. Meet sixty-odd COBOL.', 'Used in 80 percent of all in-person transactions and 95 percent of all cash machines, COBOL is the programming language of money. More than 200 billion lines of COBOL are still in use today, and all that needs to be maintained.', 'Now more than ever. As unemployment claims are going through the roof, the ancient — and COBOL-based — social security systems are overloaded. So governors are willing to do anything to get their hands on COBOL programmers.', 'So if you want to get a few big paychecks and do something to help those who are affected by this crisis, COBOL is the way to go.', 'First of all, because COBOL — COmmon Business Oriented Language — is a language for business people, not for programmers. It’s designed such that a businessperson with no knowledge of code understands what’s going on. Which means that the wellbeing of the programmer behind the code isn’t a priority.', 'COBOL has a bunch of syntactical oddities that make it palatable for business, but not for hardcore nerds. Like the fact that it doesn’t have any functions or subroutines — instead, there are divisions, sections, paragraphs, and statements. Very much to the disgust of systems programming pioneers in the 1970s.', '“The use of COBOL cripples the mind; its teaching should, therefore, be regarded as a criminal offense.” — Edsger W. Dijkstra', 'But by that time it was too late. The core codes of banks and bureaucracy had already been built.', 'Given the odd structure of COBOL, you might think it was built by people who had no idea what computer code is. Far from that. It was a kick-ass team around the legendary Grace Hopper that made its development possible.', 'Compared to the programming languages that were around in the fifties, COBOL was a revolution. It was easy to use and understand, portable, machine-independent and capable of change. And all of that for a fraction of the cost. It was a language that came straight from the future.', 'So it was adopted everywhere: Banking, insurance, federal government agencies, you name it. By 1970, there was hardly a system that was not written in COBOL.', 'Paradoxically, it was the fact that newer languages were emerging — such as Fortran & Co. — that cemented the legacy of COBOL. The business-computer-language was proclaimed dead in the eighties, and the trend went so fast that new programmers didn’t even bother learning it. And so, while there were enough older programmers left to maintain the existing code, there were not enough qualified people to transform it into newer languages.', 'Fast-forward to 2020: While the code-maintainers have done incredible work in keeping the federal systems rock-solid for decades, nobody had anticipated such a surge in demand. And suddenly we need a lot more hands-on deck to keep on top of this crisis.', '“So many of our Departments of Labor across the country are still on the COBOL system. You know very, very old technology. […] Our Department of Labor had recognized that that was an issue and had initiated modernization, and, unfortunately, that’s something that takes time. This (virus) interfered and they had to cease the transition to a much more robust system. So they’re operating on really old stuff.” — Laura Kelly, governor of Kansas', 'With unemployment rates going through the sky, governors of several states are scrambling for help right now. Suddenly, they need to ramp up systems that take years to modernize in normal times. Suddenly, everything is out of control.', 'The governor of New Jersey even made a television appearance to spread the word. IBM has been trying to help by publishing their calls for COBOL programmers, but the search remains difficult.', 'People who know how to code are in big demand right now. Specifically, people who know how to COBOL.', 'This isn’t your cool-ass hot new programming language. This isn’t even Python — people who read my articles know that I have a love-hate relationship with the latter. This isn’t some sexy data science, or freaky artificial intelligence, or both.', 'If you choose to do COBOL, you’re likely doing maintenance work. You’re sucking up business logic of decades ago instead of developing cutting-edge algorithms. You’re working in the stone age of computer science with a technology that still looks like it’s designed for one of those antiquated hole-punch cards.', 'But right now you can’t do anything more important. Millions of people have already lost their job. And although a number of states are already re-opening, a bounce-back of the economy is unlikely.', 'Which means we’ll see another few million people losing their jobs. Many, many people will need those state benefits to get by. And the current situation — some people have literally called their bureaus hundreds of times — is more than desperate.', 'So if you can do anything to help, you should. COBOL ain’t sexy — but helping fellow humans is, let’s face it, goddarn sexy. In that sense: happy coding!']"
06/2020,Dual Boot is Dead: Windows and Linux are now One,Turn your Windows machine into a developer…,7.5K,82,https://towardsdatascience.com/@dpoulopoulos,https://towardsdatascience.com/dual-boot-is-dead-windows-and-linux-are-now-one-27555902a128?source=collection_archive---------0-----------------------,7,6,"['Dual Boot is Dead: Windows and Linux are now One', 'What is WSL 2', 'Installation', 'A Developer Workstation', 'Roadmap', 'Conclusion']",32,"['I used to have an Apple laptop as my daily driver. I could do almost everything there; development, proposal writing, music composition etc. But the fear of vendor lock-in, the concern that I am depended on Apple’s whims and vices — which are arguably very expensive — led me to seek a new solution.', 'I started building a machine learning workstation; a great CPU, lots of RAM and a competent GPU, among others. My OS of choice for almost anything was Ubuntu, except I needed Microsoft Office for proposal writing. Office online is just not there yet and, let’s face it, LibreOffice is a disaster. So, the solution was to dual boot Ubuntu and Windows 10. The freedom you experience moving from Apple to Ubuntu is unparalleled, and the options you have building your own PC are almost infinite.', 'Dual boot was the answer for a long time. One million of context switches later, WSL came. Thus, I started moving a portion of my workflow to Windows. But still, there were many things missing. However, WSL 2 seems to be a game-changer. In this story, I will show you how to move your development workflow to Windows 10 and WSL 2, its new features and what to expect in the near future.', 'Learning Rate is my weekly newsletter for those who are curious about the world of AI and MLOps. You’ll hear from me every Friday with updates and thoughts on the latest AI news, research, repos and books. Subscribe here!', 'WSL 2 is the new version of the architecture in WSL. This version comes with several changes that dictate how Linux distributions interact with Windows.', 'With this release, you get increased file system performance and a full system call compatibility. Of course, you can choose to run your Linux distribution as either WSL 1 or WSL 2, and, moreover, you can switch between those versions at any time. WSL 2 is a major overhaul of the underlying architecture and uses virtualization technology and a Linux kernel to enable its new features. But Microsoft handles the nitty-gritty details so you can focus on what matters.', 'Microsoft promises a smooth installation experience in the near future for WSL 2 and the ability to update the Linux kernel via Windows updates. For now, the installation process is a bit more involved but nothing scary.', 'In this example, we will install Ubuntu 20.04 on Windows 10. But the process is the same for any distribution available in Microsoft store. First, you should enable the Windows Subsystem for Linux optional feature. Open PowerShell as Administrator and run the following command:', 'On the next step, we will update our system to WSL 2. For this, Windows 10 must be updated to version 2004 and Intel’s virtualization technology must be enabled in BIOS settings. Launch PowerShell as Administrator and run the following command:', 'Restart your machine to complete the WSL install and update to WSL 2. Then, you need to set WSL 2 as our default version when installing a new distribution. For this, open PowerShell as Administrator and run the following command:', 'You might see this message after running that command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. Follow the link and install the MSI from that page to install a Linux kernel on your machine for WSL 2 to use. Once you have the kernel installed, run the command again and it should complete successfully without showing the message.', 'Last but not least, we should install a Linux distribution. Open the Microsoft store and search for Ubuntu 20.04 LTS. After installing it, you should be able to find a new Ubuntu app on our start menu. Launch it and follow the instructions (mainly create a new UNIX user) to finalize the installation.', 'To check whether your Linux distribution is installed on WSL 2 run wsl --list --verbose. If the result indicates that it uses WSL 1 you can change it by running wsl --set-version <distribution name> <versionNumber>.', 'And that’s it. You now have a complete Ubuntu distribution running inside Windows 10!', 'Having Ubuntu up and ready, you can now install whatever you need. For example, you can install the latest Anaconda distribution if you are a data scientist, angular and npm if you are a front-end engineer and many more. But there are two tools I would like to focus on: Visual Studio Code and Docker + Kubernetes.', 'Visual Studio Code is the IDE of choice for many developers. Now that we have WSL 2 enabled, the absolutely necessary extension for VS Code is Remote Development.', 'This plug-in enables remote development against source code that exists on WSL 2, a container image or even a remote VM via SSH. Thus, we can now create our project folders inside our Linux distribution running on WSL 2 and use the Visual Studio Code editor installed on Windows 10 as our IDE.', 'All the features are there: full language support with IntelliSense, git integration, Visual Studio Code extensions we know and love, the debugger and the terminal. So, get your hands dirty and start coding!', 'Docker for Windows is good but not great. That was the thing that I was missing the most, the thing that was making me switch between Windows and Ubuntu whenever I needed to build a docker image for my code. But WSL 2 comes with full docker support which, in my opinion, is even better than the pure Linux experience.', 'To enable it, navigate to your Docker Desktop settings and enable the Use the WSL 2 based engine option.', 'Moreover, you can run a local Kubernetes cluster, by navigating to the Kubernetes section of the settings and ticking the box.', 'You can now return to Ubuntu on WSL 2, run docker version or kubectl version and see that the two are up and running.', 'For an added bonus, you can install the new Windows Terminal. The store description defines the new Windows Terminal as a modern, fast, efficient, powerful, and productive terminal application for users of command-line tools and shells like Command Prompt, PowerShell, and WSL. Its main features include multiple tabs, panes, Unicode and UTF-8 character support, a GPU accelerated text rendering engine, and custom themes, styles, and configurations.', 'Moreover, it is very beautiful and you can style it however you want, through its accessible settings that are just a JSON file. Look here for inspiration! More on the new Windows Terminal here:', 'There are still some features missing, but WSL 2 is on the right path. In the upcoming months, you will be able to install WSL with a single command. Just open a Windows Terminal and enter wsl.exe --install. Also, WSL 2 will be the new default when installing for the first time.', 'But there are two features that developers expect the most: GPU support and GUI app support.', 'Adding CUDA and/or GPU Compute support to WSL is the most requested feature since the release of WSL 1. Over the last years, the WSL, Virtualization, DirectX, Windows Driver teams, and other partners have been working hard on this engineering feature. So stay tuned!', 'Furthermore, support for Linux GUI apps is coming as well. For example, you will be able to run your preferred Linux GUI text editor or IDE in the Linux environment you have installed. You will be able to even develop Linux GUI apps, all in your Windows machine!', 'In this story, we saw how WSL 2 can turn your Windows PC into a developer workstation running a Linux distribution. The speed is there, the features are there, more are coming up, thus, I would argue that dual boot is dead!', 'Learning Rate is my weekly newsletter for those who are curious about the world of AI and MLOps. You’ll hear from me every Friday with updates and thoughts on the latest AI news, research, repos and books. Subscribe here!', 'My name is Dimitris Poulopoulos and I’m a machine learning researcher working for BigDataStack. I am also a PhD student at the University of Piraeus, Greece. I have worked on designing and implementing AI and software solutions for major clients such as the European Commission, Eurostat, IMF, the European Central Bank, OECD, and IKEA.', 'If you are interested in reading more posts about Machine Learning, Deep Learning, Data Science and DataOps follow me on Medium, LinkedIn or @james2pl on twitter.']"
06/2020,New Features in Python 3.9,A look at the best features included in the latest iteration of Python,6.3K,19,https://towardsdatascience.com/@jamescalam,https://towardsdatascience.com/new-features-in-python39-2529765429fe?source=collection_archive---------1-----------------------,5,5,"['New Features in Python 3.9', 'Dictionary Unions', 'Type Hinting', 'String Methods', 'New Parser']",33,"['It’s that time again, a new version of Python is imminent. Now in beta (3.9.0b3), we will soon be seeing the full release of Python 3.9.', 'Some of the newest features are incredibly exciting, and it will be amazing to see them used after release. We’ll cover the following:', 'Let’s take a first look at these new features and how we use them.', '(Versione in Italiano)', 'One of my favorite new features with a sleek syntax. If we have two dictionaries a and b that we need to merge, we now use the union operators.', 'We have the merge operator |:', ""[Out]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}"", 'And the update operator |=, which updates the original dictionary:', ""[Out]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}"", 'If our dictionaries share a common key, the key-value pair in the second dictionary will be used:', ""[Out]: {1: 'a', 2: 'b', 3: 'c', 6: 'but different', 4: 'd', 5: 'e'}"", 'Another cool behavior of the |= operator is the ability to update the dictionary with new key-value pairs using an iterable object — like a list or generator:', ""[Out]: {'a': 'one', 'b': 'two', 0: 0, 1: 1, 2: 4}"", 'If we attempt the same with the standard union operator | we will get a TypeError as it will only allow unions between dict types.', 'Python is dynamically typed, meaning we don’t need to specify datatypes in our code.', 'This is okay, but sometimes it can be confusing, and suddenly Python’s flexibility becomes more of a nuisance than anything else.', 'Since 3.5, we could specify types, but it was pretty cumbersome. This update has truly changed that, let’s use an example:', 'In our add_int function, we clearly want to add the same number to itself (for some mysterious undefined reason). But our editor doesn’t know that, and it is perfectly okay to add two strings together using + — so no warning is given.', 'What we can now do is specify the expected input type as int. Using this, our editor picks up on the problem immediately.', 'We can get pretty specific about the types included too, for example:', 'Type hinting can be used everywhere — and thanks to the new syntax, it now looks much cleaner:', 'Not as glamourous as the other new features, but still worth a mention as it is particularly useful. Two new string methods for removing prefixes and suffixes have been added:', '[Out]: ""llo world""', '[Out]: ""Hello wor""', 'This one is more of an out-of-sight change but has the potential of being one of the most significant changes for the future evolution of Python.', 'Python currently uses a predominantly LL(1)-based grammar, which in turn can be parsed by a LL(1) parser — which parses code top-down, left-to-right, with a lookahead of just one token.', 'Now, I have almost no idea of how this works — but I can give you a few of the current issues in Python due to the use of this method:', 'All of these factors (and many more that I simply cannot comprehend) have one major impact on Python; they limit the evolution of the language.', 'The new parser, based on PEG, will allow the Python developers significantly more flexibility — something we will begin to notice from Python 3.10 onwards.', 'That is everything we can look forward to with the upcoming Python 3.9. If you really can’t wait, the most recent beta release — 3.9.0b3 — is available here.', 'If you have any questions or suggestions, feel free to reach out via Twitter or in the comments below.', 'Thanks for reading!', 'If you enjoyed this article and want to learn more about some of the lesser-known features in Python, you may be interested in my previous article:']"
06/2020,Why is Data Science Losing Its Charm?,Opinion,4.91K,57,https://towardsdatascience.com/@hahuja7,https://towardsdatascience.com/why-is-data-science-losing-its-charm-3f7780b443f5?source=collection_archive---------2-----------------------,4,1,['Why is Data Science Losing Its Charm?'],18,"['Earlier every Computer Science student wanted to pursue a career in the data science field. The field also attracted students from many other educational backgrounds. While the hype around data science still exists, the job profile isn’t readily available for all.', 'The past two decades have been a revolution for the data science community. The developments in the past two decades are phenomenal and have taken everyone by storm. The applications of data science have amassed all the industries and had readily increased the demand of data scientists.', 'But the trends are changing nowadays. The demand is no longer the same as before. Even if there is a demand for data scientists, either people lack the skill set or the experience. I have tried to list all the potential reasons I think of when I see the community losing its charm.', 'Once out of universities, people want to kick start their careers as a data scientist but the job offerings require a minimum of 2–3 years in most of the cases. People are not able to directly start as a data scientist and have to start their career in different profiles.', 'Companies are not ready to invest their time on new incoming talents instead they want the ones with the excellent skill set and experience in the field. While almost all the tech companies have their own Data Science departments, the others which do not have one, need a person with a lot of experience in this field to start one.', 'There’s only one way I think that could help them is doing internships while they study and gain experience to meet the demands of the companies.', 'Another major reason is that data science enthusiasts nowadays do not know the difference between different job profiles available in the field. All of them want the ‘Data Scientist’ title without knowing what the actual work of a data scientist is. They mistakenly consider Data Analyst, Business Analyst and Data Scientist as being similar profiles.', 'Without knowing what they want to work upon they apply into roles they aren’t a perfect fit for and end up empty-handed.', 'People directly start working on learning algorithms, ways of tweaking the data but what they do not consider is the Math behind the algorithms. With average programming knowledge and knowledge of the Machine Learning algorithms, they think they are ready to face real-world problems.', 'People usually ignore statistics, the actual hard work just because they do not find it interesting. Data science is one such field where the development isn’t stagnant. Natural Language Processing has seen some massive developments in the past 2–3 years. One has to keep themselves updated with the state-of-the-art models.', 'People also find data science easy because they haven’t worked on real-life data. All the years that they have spent learning, they have worked on structured data or some pre-processed data that was made available for people to learn.', 'On the other, almost 99% of the data in the real world is unstructured. Data scientists need to spend most of their time pre-processing the data so that they can extract something meaningful from the data.', 'As soon as tech giants Google, Microsoft launched AutoML, it shook the aspiring data scientists. Companies’ interest and their curiosity grew into AutoML, while data scientists fear losing jobs.', 'AutoML is automating the process of applying machine learning to the datasets. AutoML can preprocess and transform the data. It can cover the complete pipeline from working on raw data to deploying machine learning algorithms.', 'AutoMLs are good at building model but when it comes to preprocessing, they cannot outshine humans. The major work of the data scientist lies in pre-processing the data. It is clear that as of now AutoMLs cannot replace human data scientist.', 'Although the fact that AutoMLs reduce the costs cannot be overlooked. The average annual salary of data scientists in the US is around $120k whereas the annual cost incurred by Google and Microsoft AutoMLs is somewhere around $4k to $40k.', 'Though the effectiveness of data scientists at pre-processing data cannot be denied because the data in the real world is highly unstructured and requires a lot of pre-processing.', 'There so much to learn and no one is willing to do the hard work. It is difficult for someone to start with the basics and excel in this field. This would take a lot of time and people need to be patient. There is a lot of scope in this field but the lack of people with the actual skills needed is snatching the title of most promising job away from Data Science and people are walking away from it.']"
06/2020,Data Science is Dead. Long Live Business Science!,BUSINESS SCIENCE,5.5K,18,https://towardsdatascience.com/@fab-evo,https://towardsdatascience.com/data-science-is-dead-long-live-business-science-a3059fe84e6c?source=collection_archive---------3-----------------------,9,11,"['Data Science is Dead. Long Live Business Science!', '70 years’ history in 2 paragraphs and 1 picture', 'Common misunderstandings surround data science', 'Data science can be a career dead-end', 'Developing Business Science to succeed', 'Salmon lesson 1: start from the end', 'Salmon lesson 2: reject the waterfall approach', 'Salmon lesson 3: 80/20 to avoid the bears', 'Salmon lesson 4: less (data) is more (storytelling)', 'Salmon strategy 5: proof in the pudding', 'Be a Business Scientist at heart, the money will follow']",54,"['Data is an unreliable friend, and hardly anything about it is actually scientific. So what, Data Science?', 'Over the past 5 years, I have interviewed more than 1,000 candidate data scientists for an apparently highly coveted set of jobs at Evo Pricing. In the process I have learned that the media are portraying a fundamental lie about this profession: throwing data at off-the-shelf algorithms is really not the point.', 'A fundamental rethink would be appropriate, and it is likely overdue.', 'At its heart, data science is a noble name for a broad set of number crunching activities that were mostly invented long ago, but recently received a new lease of life from being applied with greatly enhanced technical devices: more data, more processing power, more reasonable outcomes at a cheaper price.', 'As the cost of storing and processing data went down, the volume of data collected went up: very simple law of supply & demand, or you can call it the Price Elasticity of Data if you will. Price goes down, volume goes up. Someone will then have to do something with all this stuff. Enter Data Science.', 'What is data science?', 'According to Berkeley: one of the most promising and in-demand career paths for skilled professionals.', 'According to me, the name ‘data science’ suggests the particular approach of being a solution in search of a problem. Here, some data; what can we do with it, anything?', 'Actually sounds sub-optimal, not only career-wise to set up one’s profession, but also as a business strategy: let’s invest big money to gather all this data, one day something good will come out of it.', 'Unfortunately, the industrial revolution in the XIX century gave us schools and universities to train large numbers of blue collar workers to provide uniform answers to pre-packaged questions efficiently; and little has changed ever since.', 'What about training humans to ask the right questions instead, and letting the machines find the answers?', 'Even if many flavors of data science are gaining new popularity, like Artificial Intelligence and all other marketing hype that goes with it, the profession is mostly good for early tenure learners only.', 'Good salary prospects of 80k+ yearly average may sound appealing, but averages hide the full complexity of the challenge. To truly succeed with data one must excel at specific, impactful and well-defined problems, rather than become a generalist expert of data or even worse science, which is mostly old from an academic point of view – as the opening image shows.', 'Data and algorithms are powerful tools. But, like with any tool, they can only be as good as the use that one makes of them.', 'How can one become successful with data? Focus on the problem to be solved, the job-to-be-done, instead of the data.', 'For those who focus on for-profit use cases, Business Science suggests all the right ideas:', 'For not-for-profit and other use cases, the logic is nevertheless similar: start with question/hypothesis, use rigorous methodology, then go back to the learning criteria/question and validate if any impact was proven or not. Rinse and repeat without distraction.', 'Now the problem is how to get the job done? So much we can directly learn about this, from an apparently hilarious analogy.', 'The humble salmon, besides having a yummy taste, also gets a lot of things right during its 5–10 years’ lifespan: it first starts from the end (river mouth) and only then goes back to the source (river spring) to lay eggs/spawn, before leaving space for the next generation of salmon.', 'The baby salmon is born next to the spring, then it swims downstream as it grows, learns about all the exciting stuff happening in the ocean, before turning back to the river, where it can assert its own reproductive claim.', 'The average data scientist could learn a lot from the humble salmon. Spending too long close to the data, and swimming down towards more and more data, may make for comfortable (intellectually lazy) swimming, but is a juvenile strategy that does not lead to long-term success.', 'A more mature salmon would instead start downstream, with a keen focus on establishing which question (river) they plan to address and the impact they want to have, before starting to slowly and painfully swim upstream while progressively narrowing down the amount of data (water) they swim through.', 'I have worked for 10 years at McKinsey & Company as a management consultant. Throughout my tenure, I followed rigorously the traditional waterfall approach: investing large amounts of time, effort and client budget upfront. Researching everything in-depth. Boiling the ocean, if you will — and, in that process, killing all the poor salmons!', 'By and large, back then my team would formulate an initial hypothesis, and then look for appropriate data to prove or disprove this. Called hypothesis-driven thinking. At its best, an efficient quasi-scientific approach; at its worst, an expensive example of confirmation bias, where data are used to justify a decision that had already reached consensus beforehand anyways.', 'The theory may be appropriate for highly strategic, long-term plans, but certainly leaves clients none the wiser about what to do tomorrow morning, and then the day after, as the world becomes a faster, more complex, chaotic place. Businesses run as a movie, not a picture, as my chairman Robert Diamond likes to say.', 'This approach risks answering the wrong question and certainly fails to create the self-learning feedback that is crucial to continued success in spite of constant market disruption. Today the data are the model!', 'At the end of the day, that is why the whole concept of Agile Development was invented. Allow for incremental adjustments.', 'At the top of every respectable waterfall, even the nimble salmon must face its nemesis, the big hairy beast.', 'While swimming upstream, every salmon may face unexpected challenges, apparently insurmountable obstacles, scary predators. What used to look like a calm water flow becomes a tumbling waterfall, suddenly hard to navigate.', 'In a desperate attempt at jumping ahead of its strength, the salmon meets its nemesis, the big hairy bear waiting for its lunch.', 'Perfectionism is the Business Scientist’s nemesis.', 'Perfectionism is a trait that makes life an endless report card on accomplishments or looks. It can be a fast and enduring track to unhappiness.', 'Water (data) may quickly become a hideout for the bear to bite. A place to drown rather than the happy fluid to swim into. Better take a different, more pragmatic approach.', '80/20 is the cure — focus on what really matters and go around the obstacles rather than try to cut through them. Swim around and look for ways to avoid the bears. Edge cases very often have little to no business impact! So why bother?', 'More time should be spent preparing data-driven results than researching them. That’s MORE time as in actually more time, and not the last-minute rush variant of ‘more’.', 'The salmon is born around little water (data) — formulating a narrow question; then goes into the wide ocean of research, with big data and big water to help; but then goes back into little water territory. Because explaining a result means cherry-picking and designing for impact.', 'Data-driven work is bottom-up; but communication must be top-down to be effective.', 'At some point the Business Scientist should stop boiling the ocean of data and start thinking about the message: from scratch, how to convey that message alone? Switch from bottom-up to top-down mode.', 'Not creating fancy data viz that are highly dynamic and confusing, thus only interesting to techies. But actually, distilling down the message. LESS data and MORE time planning the communication.', 'Effective communication always starts from the so-what and therefore from the end, before swimming backwards into the million reasons why this claim is asserted, and the million supporting data points to support the conclusion.', 'I highly recommend reading The Pyramid Principle by Barbara Minto for great insight into effective logical storytelling using facts.', 'Starting from the end, from the tangible impact and its measurement approach, is key to win approval and gain confidence in what otherwise could be considered obscure algorithmic prowess pro se. Determining if a black box works requires usage.', 'You can only learn that a SatNav works by actually using it, not just by studying it.', 'I am particularly passionate about pricing and supply chain applications, and in both cases, the biggest prize is very often in the strategic stuff done upstream: planning, designing. BUT there is a but. You never get to touch that type of stuff if you first cannot demonstrate tangible value quickly.', 'So I recommend starting with end-of-lifecycle things like reordering (for supply chain) and markdowns (for pricing). Even if I am very well aware that, for example, the best markdown is the one you do NOT offer at all, because planning was accurate to begin with. But how hard would it be to disrupt planning, without first having won hearts & minds on something tangible?', 'Would it pay off more to learn the exact same data science techniques that everyone else is studying, or rather investing in learning about specific, unloved business applications where gut-feel cannot compete with data-driven impact?', 'Find your own niche first, and forget about machine learning until you will have an exciting problem you want to solve. After quitting McKinsey I have taught myself all I needed to know about R in just an afternoon to get started with implementing my Business Science idea, but did I know VERY well what I wanted to do!', 'Probably took me over ten years before that day, to reach a level of expertise that made me comfortable about the specific problem I wanted to deal with.', 'To do a PhD, you need a research question first — unlike for an MsC where someone else gives you all the questions.', 'To become an entrepreneur, you need a unique business idea first — unlike a professional career, where someone else gives you all the questions.', 'It’s a philosophy of life.', 'Dare to swim against the current, and rather than just becoming another data scientist choose to pursue the more fulfilling & rewarding Business Scientist career.', 'Happy salmon-swimming!', 'PS if interested in Business Science as a topic, more reading & insights:']"
06/2020,How I passed the TensorFlow Developer Certification Exam,And how you can too,2.7K,14,https://towardsdatascience.com/@mrdbourke,https://towardsdatascience.com/how-i-passed-the-tensorflow-developer-certification-exam-f5672a1eb641?source=collection_archive---------4-----------------------,12,9,"['How I passed the TensorFlow Developer Certification Exam', 'What is the TensorFlow Developer Certification?', 'Why might you want to get TensorFlow Developer Certified?', 'How to prepare for the exam', 'Curriculum — what I studied to build the skills necessary for passing the exam', 'How did I prepare for the exam?', 'Examination details — what happens during the actual exam?', 'What happens after you finish the exam?', 'Questions']",101,"['At the start of May, I decided to get TensorFlow Developer Certified. So I set myself up with a curriculum to sharpen my skills and took the certification exam a couple of days ago (June 3rd). Turns out, I passed.', 'Let me tell you how I did it and how you can too.', 'Hold on. What even is TensorFlow?', 'TensorFlow is an open-source numerical computing framework which allows you preprocess data, model data (find patterns in it, typically with deep learning) and deploy your solutions to the world.', 'It’s what Google uses to power all of its machine learning services. Chances are, the device you’re reading this on has run some kind of TensorFlow before.', 'Typically, you’ll write TensorFlow code in very comprehensible Python (what the exam is in) or JavaScript (tensorflow.js) and it’ll trigger a series of underlying functions written in C which execute what you’ve told it to do (lots of numerical calculations).', 'Okay, now we know what TensorFlow is, what is the TensorFlow Developer Certification? And why might you be interested in it?', 'The TensorFlow Developer Certification, as you might’ve guessed, is a way to showcase your ability to use TensorFlow.', 'More specifically, your ability to use TensorFlow (the Python version) to build deep learning models for a range of tasks such as regression, computer vision (finding patterns in images), natural language processing (finding patterns in text) and time series forecasting (predicting future trends given a range of past events).', 'My first reason was fun. I wanted to give myself a little challenge to work towards and a reason to read a new book I’d purchased (more on this later).', 'But two other valid reasons are:', 'Speaking of future employers, based on data from Hacker News’s Who’s Hiring page (a page which lists monthly collections of software developer jobs), it looks like compared to other deep learning frameworks, TensorFlow pulls out ahead.', 'I want to be clear, a paid certificate is no guarantee of getting a job. But in the world of online learning, where skills are becoming commoditised, it’s another way to showcase what you’re capable of.', 'I think of it as a nice addition to add to your existing list of personal projects you’ve worked on — courses build foundation knowledge, projects build specific knowledge.', 'So how do you do it?', 'When I decided I wanted to I went through the certification website and read the TensorFlow Developer Certification Handbook.', 'From these two resources, I built the following curriculum.', 'It should be noted that before I started studying for the exam, I had some hands-on experience building several projects with TensorFlow.', 'The experienced TensorFlow and deep learning practitioner will likely find they can go through the following curriculum at about the same pace I did (3 weeks total), maybe faster.', 'The beginner will want to take as much time as needed. Remember: building any worthwhile skill takes time.', 'I’ve listed timelines, costs ($USD) and helpfulness level (towards passing the exam) for each resource. The timelines are based on my experience.', 'If you want to create a curriculum for yourself, I’d recommend something like the following.', 'Note: For paid resources, affiliate links have been used. This doesn’t change the price of the resource but if you do happen to purchase one, I will receive a portion of the payment: money I use towards creating resources like this.', 'Time: 1-hour.', 'Cost: Free.', 'Helpfulness level: Required.', 'This should be your first stop. It outlines the topics which will be covered in the exam. Read it and then read it again.', 'If you’re new to TensorFlow and machine learning, you’ll likely read this and get scared at all the different topics. Don’t worry. The resources below will help you become familiar with them.', 'Time: 3 weeks (advanced user) to 3 months (beginner).', 'Cost: $59 per month after a 7-day free trial, financial aid available through application. If you can’t access Coursera, see the equivalent free version on YouTube.', 'Helpfulness level: 10/10.', 'This is the most relevant resource to the exam (and getting started with TensorFlow in general). The careful student will notice the TensorFlow Certification handbook and the outline of this specialization are almost identical.', 'It’s taught by Laurence Moroney and Andrew Ng, two titans of TensorFlow and machine learning and if I had to only choose one resource to prepare for the exam, this would be it.', 'I appreciated the short video format and focus on hands-on examples as soon as possible. The multiple code notebooks at the end of each section were must-haves for any practical learner.', 'A tip for the programming exercises: don’t just fill in the code gaps, write the entire thing out yourself.', 'Time: 3 weeks (reading cover to cover, no exercises) — 3 months (reading cover to cover and doing the exercises).', 'Cost: Price varies on Amazon but I picked up a hard copy for $55. You can see all the code for free on GitHub.', 'Helpfulness level: 7/10 (only because some chapters aren’t relevant to the exam).', 'At 700+ pages, this book covers basically all of machine learning and thus, some topics which aren’t relevant to the exam. But it’s a must-read for anyone interested in setting themselves a solid foundation for a future in machine learning and not just to pass an exam.', 'If you’re new to machine learning, you’ll probably find this book hard to read (to begin with). Again, not to worry, you’re not in a rush, learning useful skills takes time.', 'Put it this way, if you want an idea of the quality of the book, I read the first edition during morning commutes to my machine learning engineer job. And I can tell you, more often than not, I’d end up using exactly what I read in the book during the day.', 'The 2nd edition is no different, except it’s been updated to cover the latest tools and techniques, namely TensorFlow 2.x — what the exam is based on.', 'If you’re only after relevant chapters to the exam, you’ll want to read:', 'But for the serious student, I’d suggest the whole book and the exercises (maybe not all, but pick and the choose the ones which suit spark your interests most).', 'Time: 3-hours (I only watched 3 lectures) — 24-hours (1-hour per lecture, plus 1-hour review each).', 'Cost: Free.', 'Helpfulness level: 8/10.', 'World-class deep learning information from a world-class university, oh and did I mention? It’s free.', 'The first 3 lectures, deep learning (in general), Convolutional Neural Networks (usually used for computer vision) and Recurrent Neural Networks (usually used for text processing) are the most relevant to the exam.', 'But again, for the eager learner, going through the whole course wouldn’t be a bad idea.', 'Be sure to check out the labs and code they offer on GitHub, especially the Introduction to TensorFlow one. And again, I can’t stress the importance of writing the code yourself.', 'Time: 3-hours (depending on how fast your computer is).', 'Cost: Free.', 'Helpfulness level: 10/10 (using PyCharm is a requirement).', 'The exam takes place in PyCharm (a Python development tool). Before the exam, I’d never used PyCharm. And it’s suggested you get at least somewhat familiar with it before you start.', 'So to familiarise myself with PyCharm, I went through their getting started series on YouTube which was very straightforward, “here’s what this button does.”', 'But the main tests were making sure TensorFlow 2.x ran without any issues and my computer could run deep neural networks in a respectable time (my MacBook Pro doesn’t have a Nvidia GPU).', 'To test this, I replicated the following two TensorFlow tutorials on my local machine:', 'Both of these worked fine locally, however, as we’ll see below, as soon as I started the exam, I ran into an issue.', 'Armed with the resources above, I put together an outline in Notion.', 'Every morning throughout May, I’d get up, do some writing, go for a walk, read 1-hour of the Hands-on Machine Learning book, do 2–3 hours work of TensorFlow in practice (watching the lectures first, then completing all of the coding exercises in Google Colab) then at the end of each module I’d watch the corresponding MIT Introduction to Deep Learning Lecture.', 'For example, once I finished the computer vision section of the TensorFlow in Practice Specialization, I watched the Convolutional Neural Network (a type of computer vision algorithm) lecture from MIT.', 'This tribrid approach turned out to work particularly well.', 'A concept I’d read in the book would get cemented by code examples in the Coursera specialization and eventually summarised by the MIT video.', 'For an idea of the timeline, I started studying for the exam on May 11th and took it on June 3rd.', 'By my tracking (in Notion) and on my handwritten bookmark, I averaged 20-pages per hour and about 1-week of course content per 2–3-hour study block (no distractions).', 'Finally, a couple of days before the exam, I downloaded PyCharm and made sure a few of the code examples I’d been through worked on my local machine.', 'So you’ve done your study? Now what?', 'Well, let’s start with two important factors.', 'Exam cost: $100 USD (per attempt, if you fail, you have to wait 2 weeks to try again and longer for each fail thereafter).', 'Time-limit: 5-hours. Without the error at the start of the exam, I’d say I would’ve comfortably completed it within 3-hours. However, the extended time limit is to give you enough time to train deep learning models on your computer (so make sure this works before starting).', 'I’m not going to reveal much here because that would be cheating. All I’ll say is read the TensorFlow Developer Handbook and you’ll get a fair idea of the major sections of the exam.', 'Practice each one of the techniques mentioned in the handbook (using the resources mentioned above) and you’ll be fine.', 'Training models — If your computer can’t train deep learning models fast enough (part of the marking criteria is submitting trained models), you can train them in Google Colab using a free GPU, then download them, put them in the relevant directories for the exam and submit them through PyCharm.', 'My broken Python interpreter — The exam preparation material stresses that Python 3.7 is required for the exam. When I started, I had Python 3.7.3. And for some reason, even though TensorFlow was working the day before on my local machine using PyCharm, after starting the exam (which automatically creates a TensorFlow environment for you), it broke.', 'Namely, every time I ran a single line of TensorFlow code, I got the error:', 'Now I’m not sure whether it’s the version of TensorFlow the exam installs (2.0.0) or the specific version of Python I had (3.7.3).', 'None the less, after some cursing and heated searching through the depths of an old GitHub issue thread, I discovered a strange fix which meant having to alter the source code of the Python version I was using (specifically, line 48 of lincache.py).', 'Note: This was only a quick fix, due to having being done during the exam, so I’m not sure if it has any long-term benefits or repercussions.', 'During my frantic searching, I also read an alternative is to just update/reinstall the version of TensorFlow you’re using in PyCharm (e.g. 2.0.0 -> 2.2.x). I tried this and it didn’t work, though, being a novice to PyCharm, I suspect some user error.', 'After implementing the fix I was able to complete the exam with no issues.', 'You’ll get notified via email when/if you passed the exam. There will be no feedback except “Congratulations you passed” or “Unfortunately you didn’t pass this time”.', 'Without spoiling too much, you’ll get a pretty clear indication whilst taking the exam if you’re likely to pass or not (each time you submit a model, it gets marked).', 'But if you do pass congratulations!', 'Be sure to fill out the form in the email to make sure you get added the TensorFlow Certified Developers network.', 'Registering yourself here means anyone who’s looking for skilled TensorFlow developers will be able to search for you based on your certification type, experience and region.', 'Finally, within a couple of weeks (I haven’t got mine yet) you’ll be emailed an official TensorFlow Developer Certification and badge. Two things you can add alongside the projects you’ve worked on.', 'Can I just do the courses, read the book and practice myself, do I really need the certificate?', 'Of course. At the end of the day, skills are what you should be after, not certificates. Certificates are nice to haves not need to haves.', 'If you say certificates aren’t needed, why’d you get it?', 'I like having a challenge to work towards. Setting a date for myself, as in, “I’m taking the exam on June 3rd”, gave me no choice but to study.', 'Can I do this with free resources?', 'Yes, of course you can. You can go learn all the skills you need by going through the TensorFlow documentation. In fact, when I need to practice something, I copy the documentation examples verbatim (every line of code), practice understanding it line by line, then see if I can do it myself.', 'Why not PyTorch?', 'I love PyTorch. But they don’t offer certification, if they did, I’d probably do it too (for fun). Plus, the experienced user of both frameworks (PyTorch and TensorFlow) will start to see that recent updates have meant the two are getting very similar. If anything, TensorFlow has an edge in the enterprise world (see graph above).', 'I don’t know any machine learning, where can I start?', 'Read the article 5 Beginner-Friendly Steps to Learn Machine Learning.', 'I’ve passed the exam, and registered with the Google Developers Certification network, what do I next?', 'Time to build! Use the skills you’ve learned to make something you’d like to see in the world. And don’t forget to share your work, you never know who’ll see it.', 'Something not mentioned? Feel free to comment below or ask it via email. And I’ll answer it.', 'PS if you prefer to watch things, I made a video version of this article.']"
06/2020,10 Smooth Python Tricks For Python Gods,10 Tricks that will individualize and better your Python code,3.3K,15,https://towardsdatascience.com/@emmettgb,https://towardsdatascience.com/10-smooth-python-tricks-for-python-gods-2e4f6180e5e3?source=collection_archive---------5-----------------------,6,12,"['10 Smooth Python Tricks For Python Gods', '№1: Reverse A String', '№2: Dims as variables', '№3: Itertools', '№4: Intelligent Unpacking', '№5: Enumerate', '№6: Name Slices', '№7: Group Adjacent Lists', '№8: next() iteration for generators', '№9: Counter', '№10: Dequeue', 'Conclusion']",17,"['Although on the surface Python might appear to be a language of simplicity that anyone can learn, and it is, many might be surprised to know just how much mastery one can obtain in the language. Python is one of those things that is rather easy learn, but can be difficult to master. In Python, there are often multiple ways of doing things, but it can be easy to do the wrong thing, or reinvent the standard library and waste time simply because you were not aware of a module’s existence.', 'Unfortunately, the Python standard library is quite a vast beast, and furthermore, its ecosystem is absolutely terrifyingly enormous. Although there are probably two-million gigabytes of Python modules, there are some useful tips that you can learn with the standard library and packages usually associated with scientific computing in Python.', 'Though it might seem rather basic, reversing a string with char looping can be rather tedious and annoying. Fortunately, Python includes an easy built-in operation to perform exactly this task. To do this, we simply access the indice ::-1 on our string.', 'In most languages, in order to get an array into a set of variables we would need to either loop through the values iteratively or access the dims by position like so:', 'In Python, however, there is a way cooler and quicker way to do so. In order to change a list of values into variables we can simply set variable names equal to the array with the same length of the array:', 'If you’re going to spend any time whatsoever in Python, you will definitely want to get familiar with itertools. Itertools is a module within the standard library that will allow you to get around iteration constantly. Not only does it make it far easier to code complex loops, it also makes your code both faster and more concise. Here is just one example of a use for Itertools, but there are hundreds:', 'Unpacking values iteratively can be rather intensive and time consuming. Fortunately, Python has several cool ways in which we can unpack lists! One example of this is the *, which will fill in unassigned values and add them to a new list under our variable name.', 'If you’re not aware of enumerate, you probably should get familiar with it. Enumerate will allow you to get indexes of certain values in a list. This is especially useful in data science when working with arrays rather than data-frames.', 'Slicing apart lists in Python is incredibly easy! There are all sorts of great tools that can be used for this, but one that certainly is valuable is the ability to name slices of your list. This is especially useful for linear algebra in Python.', 'Grouping adjacent loops could certainly be done rather easily in a for loop, especially by using zip(), but this is certainly not the best way of doing things. To make things a bit easier and faster, we can write a lambda expression with zip that will group our adjacent lists like so:', 'In most normal scenarios in programming, we can access an indice and get our position number by using a counter, which will just be a value that is added to:', 'Instead of this, however, we can use next(). Next takes an iterator that will store our current position in memory and will iterate across our list in the background.', 'Another great module from the standard library is collections, and what I would like to introduce to you today is Counter from collections. Using Counter, we can easily get counts of a list. This is useful for getting the total number of values in our data, getting a null count of our data, and seeing the unique values of our data. I know what you’re thinking,', '“ Why not just use Pandas?”', 'And this is certainly a valid point. However, using Pandas for this is certainly going to be a lot harder to automate, and is just another dependency you are going to need to add to your virtual environment whenever you deploy your algorithm. Additionally, a counter type in Python has a lot of features that Pandas Series don’t have, which can make it far more useful for certain situations.', 'Another great thing coming out of the collections module is dequeue. Check out all the neat things we can do with this type!', 'So there you have it, these are some of my favorite Python tricks that I use all the time. Though some of these might be used a little more rarely, these tricks tend to be very versatile and useful. Fortunately, the Python tool-box of standard library functions certainly doesn’t start to become bare there, and there are certainly more tools inside of it. More than likely there are some that I don’t even know, so there’s always something to learn which is exciting!']"
06/2020,30 Magical Python Tricks to Write Better Code,Turn Your Logic Into More Elegant Codes With Python…,905,14,https://towardsdatascience.com/@felix.antony,https://towardsdatascience.com/30-magical-python-tricks-to-write-better-code-e54d1642c255?source=collection_archive---------6-----------------------,10,32,"['30 Magical Python Tricks to Write Better Code', 'Trick 01 - Multiple Assignment for Variables', 'Trick 02 - Swapping Two Variables', 'Trick 03 - Reversing a String', 'Trick 04 - Splitting Words in a Line', 'Trick 05 - List of words into a line', 'Trick 06 - Printing a string multiple times', 'Trick 07 - Joining Two strings using addition operator', 'Trick 08 - More than one Conditional Operators', 'Trick 09 - Find most frequent element in a list', 'Trick 10 - Find Occurrence of all elements in list', 'Trick 11 - Checking for Anagram of Two strings', 'Trick 12 - Create Number Sequence with range', 'Trick 13 - Repeating the element multiple times', 'Trick 14 - Using Conditions in Ternary Operator', 'Trick 15 - List Comprehension with Python', 'Trick 16 - Convert Mutable into Immutable', 'Trick 17 - Rounding off with Floor and Ceil', 'Trick 18 - Returning Boolean Values', 'Trick 19 - Create functions in one line', 'Trick 20 - Apply function for all elements in list', 'Trick 21 - Using Lambda with Map function', 'Trick 22 - Return multiple values from a function', 'Trick 23 - Filtering the values using filter function', 'Trick 24 - Merging Two Dictionaries in Python', 'Trick 25 - Getting size of an object', 'Trick 26 - Combining two lists into dictionary', 'Trick 27 - Calculating execution time for a program', 'Trick 28 - Removing Duplicate elements in list', 'Trick 29 - Printing monthly calendar in python', 'Trick 30 - Iterating with zip function', 'Closing Thoughts']",102,"['Python is quite a popular language among others for its simplicity and readability of the code. It is one of the simplest languages to choose as your first language. If you are a beginner with the basic concepts of python then this is the best time to learn to write better codes.', 'There are a lot of tricks in python that can improve your program better than before. This article will help you to know various tricks and tips available in python. Practice them continuously until it becomes a part of your programming habit.', 'Python allows us to assign values for more than one variable in a single line. The variables can be separated using commas. The one-liners for multiple assignments has lots of benefits. It can be used for assigning multiple values for multiple variables or multiple values for a single variable name. Let us take a problem statement in which we have to assign the values 50 and 60 to the variables a and b. The usual code will be like the following.', 'Output', 'Condition I - Values equal to Variables', 'When the variables and values of multiple assignments are equal, each value will be stored in all the variables.', 'Output', 'Both the programs gives the same results. This is the benefit of using one line value assignments.', 'Condition II - Values greater than Variables', 'Let us try to increase the number of values in the previous program. The multiple values can be assigned to a single variable. While assigning more than one value to a variable we must use an asterisk before the variable name.', 'Output', 'The first value will be assigned to the first variable. The second variable will take a collection of values from the given values. This will create a list type object.', 'Condition III - One Value to Multiple Variables', 'We can assign a value to more than one variable. Each variable will be separated using an equal to symbol.', 'Output', 'Swapping is the process of exchanging the values of two variables with each other. This can be useful in many operations in computer science. Here, I have written two major methods used by the programmer to swap the values as well as the optimal solution.', 'Method I - Using a temporary variable', 'This method uses a temporary variable to store some data. The following code is written with temporary variable name.', 'Output', 'Method II - Without using a temporary variable', 'The following code swaps the variable without using a temporary variable.', 'Output', 'Method III - Optimal Solution in Python', 'This is a different approach to swap variables using python. In the previous section, we have learned about multiple assignments. We can use the concept of swapping.', 'Output', 'There is an another cool trick for reversing a string in python. The concept used for reversing a string is called string slicing. Any string can be reversed using the symbol [::-1] after the variable name.', 'Output', 'No special algorithm is required for splitting the words in a line. We can use the keyword split() for this purpose. Here I have written two methods for splitting the words.', 'Method I - Using iterations', 'Output', 'Method II - Using split function', 'Output', 'This is the opposite process of the previous one. In this part we are going to convert a list of words into a single line using join function. The syntax for using join function is given below.', 'Syntax: “ ”.join(string)', 'Output', 'We can use the multiplication operator to print a string for multiple times. This is a very effective way to repeat a string.', 'Output', 'Joining various strings can be done without using the join function. We can just use the addition operator (+) to do this.', 'Output', 'Two combine two or more conditional operators in a program we can use the logical operators. But the same result can be obtained by chaining the operators. For example, if we need to do print something when a variable has the value greater than 10 and less than 20, the code will be something like the following.', 'Instead of this we can combine the conditional operator into single expression.', 'Output', 'Learn more about operators in the following article.', 'The element which occurs most of the time in a list then it will be the most frequent element in the list. The following snippet will help you to get the most frequent element from a list.', 'Output', 'The previous code will give the most frequent value. If we need to know the occurrence of all the unique element in a list, then we can go for the collection module. The collections is a wonderful module in python which gives great features. The Counter method gives a dictionary with the element and occurrence pair.', 'Output', 'Two strings are anagrams if one string is made up of the characters in the other string. We can use the same Counter method from the collections module.', 'Output', 'The function range() is useful for creating a sequence of numbers. It can be useful in many code snippets. The syntax for a range function is written here.', 'Syntax: range(start, end, step)', 'Let us try to create a list of even numbers.', 'Output', 'Similar to the string multiplication we can create a list filled with an element multiple times using multiplication operator.', 'Output', 'In most of the time, we use nested conditional structures in Python. Instead of using nested structure, a single line can be replaced with the help of ternary operator. The syntax is given below.', 'Syntax: Statement1 if True else Statement2', 'age = 25print(""Eligible"") if age>20 else print(""Not Eligible"")', 'Output', 'List comprehension is a very compact way to create a list from another list. Look at the following codes. The first one is written using simple iteration and the second one is created using list comprehension.', 'Output', 'Using List Comprehension', 'Output', 'The function frozenset() is used to convert mutable iterable into immutable object. Using this we can freeze an object from changing its value.', 'Output', 'As we applied the frozenset() function on the list, the item assignment is restricted.', 'Floor and Ceil are mathematical functions can be used on floating numbers. The floor function returns an integer smaller than the floating value whereas the ceil function returns the integer greater than the floating value. To use this functions we have to import math module.', 'Output', 'Some times we have to return a boolean value by checking conditions of certain parameters. Instead of writing if else statements we can directly return the condition. The following programs will produce the same output.', 'Method I - Using If Else Condition', 'Method II - Without If Else Condition', 'Output', 'Lambda is an anonymous function in python that creates function in one line. The syntax for using a lambda function is given here.', 'Syntax: lambda arguments: expression', 'Output', 'Map is a higher order function that applies a particular function for all the elements in list.', 'Syntax: map(function, iterable)', 'Output', 'The function can be replaced by a lambda function in python. The following program is created for creating square of list of numbers.', 'Output', 'Learn more about higher order functions here.', 'A python function can return more than one value without any extra need. We can just return the values by separating them by commas.', 'Output', 'Filter function is used for filtering some values from a iterable object. The syntax for filter function is given below.', 'Syntax: filter(function, iterable)', 'Output', 'In python, we can merge two dictionaries without any specific method. Below code is an example for merging two dictionaries.', 'Output', 'The memory size varies based on the type of object. We can get the memory of an object using getsizeof() function from the sys module.', 'Output', 'The zip unction has many advantages in python. Using zip function we can create a dictionary from two lists.', 'Output', 'Time is another useful module in python can be used to calculate the execution time.', 'Output', 'An element that occurs more than one time is called duplicate element. We can remove the duplicate elements simply using typecasting.', 'Output', 'Calendar module has many function related to the date based operations. We can print monthly calendar using the following code.', 'Output', 'The zip functions enables the process of iterating more than one iterable using loops. In the below code two lists are getting iterated simultaneously.', 'Output', 'I hope you enjoyed this article. As an end note, you have to understand that learning the tricks is not a must. But if you do so, you can stand unique among other programmers. Continuous practice is must to become fluent in coding. Thank you for reading this article. You can follow me on medium.', 'Happy Coding!']"
06/2020,PyCharm vs VSCode,Opinion,885,33,https://towardsdatascience.com/@ahmasoh,https://towardsdatascience.com/pycharm-vs-vscode-9ffbed46ac9e?source=collection_archive---------7-----------------------,5,5,"['PyCharm vs VSCode', 'PyCharm > VSCode', 'VSCode > PyCharm', 'Which is best?', 'Stick to PyCharm if you only code in Python. If not, VSCode.']",22,"['Maybe I’m a bit behind the curve, or maybe because JetBrains have such a big hold on the Python IDE market, it became clear to me in a previous post that a lot more Python coders are using VSCode than I was expecting.', 'Now I’ve used a combination of PyCharm and Notebooks for a while and I’m super happy with it. I love that if I have some data I want to explore then Notebooks is pretty easy to navigate, keep track of my work and also visualise data. On the other hand, PyCharm is just a pure machine when it comes to production: it’s never let me down and helps me churn through most tasks.', 'I also like the fact that the makers of PyCharm (JetBrains) are not some big American Goliath (like Microsoft), but comes from a much more humble region.', 'Either way, Visual Studio Code (or VSCode for short) is Microsofts open-source IDE. Its initial release was in 2015 and since then (according to Stack Overflow) it’s become the most in-demand IDE.', 'Given the fact that I’ve never really spent much time using VSCode and what it offers, I’ve decided to put it next to PyCharm try to figure out which is better, and which should I use?', 'One would expect that developing code would feel more natural in a purpose built IDE and as PyCharm was created with the sole purpose of coding in Python. Does that make a difference?', 'Let’s take the example of autocomplete support. VSCode struggles at times with autocomplete support whereas when using PyCharm, it works nearly perfectly in every instance. My personal experience of VSCode was that the autocomplete can at times work great and other times not. It’s not just me though, people on reddit complain about the same thing: it’s oddly temperamental.', 'Further, VSCode struggles to load extensions at times and I thought it may have been me, however, this seems to be a bit of a recurring theme as its been reported multiple times: here, and here, and here, and here, and here, and here, and the issue is still present.', 'Now at first, you’re thinking “Oh awesome, I can customise my VSCode to be exactly how I want” but in reality, it never works that well and you end up having to spend a lot more time trying to fix the bug and less time developing, which is something you just don’t need to worry about in PyCharm.', 'So for those reasons, PyCharm being native to Python and built to really capitalise on that gives it a huge edge over VSCode. However, VSCode has a lot to offer as well.', 'First and most importantly, VSCode is free. Yup, completely. The pure editor is pretty simple and you can expand its capabilities by installing plugins. PyCharm Professional, on the other hand, isn’t exactly cheap.', 'There is a free version of PyCharm (called the Community Edition) but it has fewer functionalities: it doesn’t include tools for developing databases or web related things, nor does it include advanced features such as performance profiling and remote debugging. VSCode has way more functionality than the free PyCharm Community edition, so let’s keep our focus on PyCharm Professional.', 'Now, something that PyCharm users are aware of is how big its memory footprint is. At the upper limit, it can take up to 1.5gb in disk space and that does have a knock on effect on your coding experience. If your computer can’t handle that then it’ll take ages to load up and sometimes it’ll take a bit longer to get through basic tasks: no one likes that!', 'Visual Studio Code has a much smaller footprint for memory consumption and physical disk space, about 30% that of PyCharm. So as VSCode is relatively light weight, it’s a particularly good editor for smaller projects or applications, and when performing quick edits to one or more files.', 'Finally, people generally seem OK with having to build a custom IDE in VSCode, as compared to PyCharm which works great out of the box and you don’t really need to do much more to it. However with VSCode, you have to build it from the beginning with plugins to even get Python working on it, so users are already comfortable with upgrading its functionality with plugins. This means that these users are also thinking about further enhancements which over time, leads to more development and a better coding experience, whereas with PyCharm, it’s mostly left to JetBrains.', 'Both PyCharm and VSCode allow the community to create plugins to enhance their user experience. Both have full-blown IDE’s and really do tick all the boxes in terms of what you need and want, although, neither are entirely perfect. Both have a strong community behind them and despite VSCode not being around for as long as PyCharm, both do have fairly mature systems in terms of technical capability.', 'I think it ultimately comes down to you. Do you want to pay for PyCharm professional and have a more specialised experience, or, would you rather have the free VSCode experience with a little bit less specialism, but, potentially more extensibility?', 'So what does my gut say?', 'The decision is ultimately up to you but the IDE you use can really alter your perception and experience in a coding language. I would expect advanced programmers to be using a variety of IDE’s depending on the project in hand (not to mention to the number of languages coders jump between) so being flexible with your tools definitely makes life easier.', 'Despite all that: I’ll probably stick to my Jupyter Notebooks and PyCharm combination, but I’d be interested to hear from any full-time VSCode users as to why they won’t be switching any time soon!', 'Thanks for reading again!! Let me know if you have any questions and I’ll be happy to help.', 'Keep up to date with my latest work here!']"
06/2020,8 Advanced Python Tricks Used by Seasoned Programmers,Apply these tricks in your Python code to make…,3.1K,12,https://towardsdatascience.com/@eriky,https://towardsdatascience.com/8-advanced-python-tricks-used-by-seasoned-programmers-757804975802?source=collection_archive---------8-----------------------,5,9,"['8 Advanced Python Tricks Used by Seasoned Programmers', '1. Sorting Objects by Multiple Keys', '2. List Comprehensions', '3. Check memory usage of your objects', '4. Data classes', '5. The attrs Package', '6. Merging dictionaries (Python 3.5+)', '7. Find the Most Frequently Occurring Value', '8. Return Multiple Values']",39,"['Here are eight neat Python tricks some I’m sure you haven’t seen before. Apply these tricks in your Python code to make it more concise and performant!', 'Visit python3.guide to learn all about Python!', 'Suppose we want to sort the following list of dictionaries:', 'But we don’t just want to sort it by name or age, we want to sort it by both fields. In SQL, this would be a query like:', 'There’s actually a very simple solution to this problem, thanks to Python’s guarantee that sort functions offer a stable sort order. This means items that compare equal retain their original order.', 'To achieve sorting by name and age, we can do this:', 'Notice how I reversed the order. We first sort by age, and then by name. With operator.itemgetter() we get the age and name fields from each dictionary inside the list in a concise way.', 'This gives us the result we were looking for:', 'The names are sorted primarily, the ages are sorted if the name is the same. So all the Johns are grouped together, sorted by age.', 'Inspired by this StackOverflow question.', 'A list comprehension can replace ugly for loops used to fill a list. The basic syntax for a list comprehension is:', 'A very basic example to fill a list with a sequence of numbers:', 'And because you can use an expression, you can also do some math:', 'Or even call an external function:', 'And finally, you can use the ‘if’ to filter the list. In this case, we only keep the values that are dividable by 2:', 'With sys.getsizeof() you can check the memory usage of an object:', 'Woah… wait… why is this huge list only 48 bytes?', 'It’s because the range function returns a class that only behaves like a list. A range is a lot more memory efficient than using an actual list of numbers.', 'You can see for yourself by using a list comprehension to create an actual list of numbers from the same range:', 'So, by playing around with sys.getsizeof() you can learn more about Python and your memory usage.', 'Since version 3.7, Python offers data classes. There are several advantages over regular classes or other alternatives like returning multiple values or dictionaries:', 'Here’s an example of a data class at work:', 'An in-depth guide can be found here.', 'Instead of data classes, you can use attrs. There are two reasons to choose attrs:', 'Theattrs package supports all mainstream Python versions, including CPython 2.7 and PyPy. Some of the extras attrs offers over regular data classes are validators, and converters. Let’s look at some example code:', 'The authors of attrs have, in fact, worked on the PEP that introduced data classes. Data classes are intentionally kept simpler (easier to understand), while attrs offers the full range of features you might want!', 'For more examples, check out the attrs examples page.', 'Since Python 3.5, it’s easier to merge dictionaries:', 'If there are overlapping keys, the keys from the first dictionary will be overwritten.', 'In Python 3.9, merging dictionaries becomes even cleaner. The above merge in Python 3.9 can be rewritten as:', 'To find the most frequently occurring value in a list or string:', 'Do you understand why this works? Try to figure it out for yourself before reading on.', 'You didn’t try, did you? I’ll tell you anyway:', 'So what we do in this single line of code is take all the unique values of test, which is {1, 2, 3, 4}. Next, max will apply the list.count function to them and return the maximum value.', 'And no — I didn’t invent this one-liner.', 'Update: a number of commenters rightfully pointed out that there’s a much more efficient way to do this:', 'Functions in Python can return more than one variable without the need for a dictionary, a list, or a class. It works like this:', 'This is alright for a limited number of return values. But anything past 3 values should be put into a (data) class.', 'That’s it! Did you learn anything new? Or do you have another trick up your sleeve that you want to share? Please let me know in the comments!']"
06/2020,Best Python IDEs and Code Editors You Must Use in 2020,Top Python IDEs and Code Editors with…,812,16,https://towardsdatascience.com/@harish_6956,https://towardsdatascience.com/best-python-ides-and-code-editors-you-must-use-in-2020-2303a53db24?source=collection_archive---------9-----------------------,12,15,"['Best Python IDEs and Code Editors You Should Know', 'What is an Integrated Development Environment (IDE)?', 'IDE vs. Code Editor', 'Best Python IDEs and Code Editors in 2020', '● PyCharm', '● Spyder', '● Eclipse + Pydev', '● IDLE', '● Wing', '● Cloud9 IDE', '● Sublime Text 3', '● Visual Studio Code', '● Atom', '● Jupyter', 'More Interesting Readings —']",154,"['Python is an experiment in how much freedom programmers need. Too much freedom and nobody can read another’s code; too little and expressiveness is endangered.', '- Guido van Rossum', 'Since its creation, Python has rapidly evolved into a multi-faceted programming language, becoming the choice of several diverse projects ranging from web applications to being deployed into Artificial Intelligence, Machine Learning, Deep Learning, and more.', 'Python comes with numerous features such as its simplicity, enormous collection of packages and libraries, with relatively faster execution of programs, to list a few.', 'GitHub’s second-most popular language and the most popular language for machine learning.', 'For a programmer, a Code Editor or an IDE is the first point of contact with any programming language, making its selection one of the most crucial steps in the journey ahead. Throughout this article, we’ll discuss some of the top Python IDEs and Code Editors, along with the reasons why you should and shouldn’t pick them for your next project.', 'According to StackOverflow, Python is the fastest-growing major programming language.', 'An IDE stands for Integrated Development Environment and includes not just the standard code editor for managing the code but also provides a comprehensive set of tools for its debugging, execution, and testing, which is an absolute must for software development. Some IDEs also come with built-in compilers and interpreters. Listed below are some of the standard features common IDEs offer within a single dedicated environment:', '● Syntax highlighting', '● Build automation', '● Version control', '● Visual programming', '● Code formatting and completion', '● Code refactoring', '● Support for integration with external tools', 'A Code Editor or an IDE is the most fundamental piece of software for any programmer, and it is what they start and end their day. To achieve its maximum potential, the best starting point is a Code Editor or an IDE that essentially lets you work with Python, but that’s not all. A host of programming languages can work entirely without an IDE, while some are IDE-dependent.', 'Code Editor — A Code Editor is a core piece of software that programmers use for application development. Think of it as a simple text editor but with additional programming-specific advanced features such as:', '● Syntax highlighting', '● Code formatting', '● Split file viewing and editing', '● Instant project switching', '● Multiple selections', '● Cross-platform support', '● Light-weight', 'IDE — On the other hand, an IDE comes with a suite of tools that help in not just developing the application, but also in its testing, debugging, refactoring, and automating builds. Needless to say, in most cases, an IDE can offer all features of a Code Editor, but a Code Editor cannot replace an IDE.', 'Choosing the right tools for a job is critical. Similarly, when starting a new project, as a programmer, you have a lot of options when it comes to selecting the perfect Code Editor or IDE. There are loads of IDEs and Code Editors out there for Python, and in this section, we’ll discuss some of the best ones available with their benefits and weaknesses.', 'Developed by JetBrains, PyCharm is a cross-platform IDE that offers a variety of features such as version control, graphical debugger, integrated unit tester, and pairs well for web development and Data Science tasks. With PyCharm’s API, developers can create their custom plugins for adding new features to the IDE. Other features include:', '● Code completion', '● Live updates to code changes', '● Python refactoring', '● Support for full-stack web development', '● Support for scientific tool such as matplotlib, numpy, and scipy', '● Support for Git, Mercurial and more', '● Comes with paid and community editions', '● Can boost productivity and code quality', '● Highly active community for support', '● Can be slow to load', '● Requires changing default settings for existing projects for best compatibility', '● The initial installation might be difficult', 'Screenshot for References-', 'Spyder is awith support for packages like NumPy, SciPy, Matplotlib, and Pandas. Targeted towards scientists, engineers, and data analysts, Spyder offers advanced data exploration, analysis, and visualization tools. Features of this cross-platform IDE include:', '● Code completion', '● Syntax highlighting', '● Code benchmarking via Profiler', '● Multi-project handling', '● Find in Files feature', '● History log', '● Internal console for introspection', '● Third-party plugins support', '● Includes support for numerous scientific tools', '● Comes with an amazing community support', '● Interactive console', '● Lightweight', '● Comes with execution dependencies', '● Can be a bit challenging at first for newcomers', 'Screenshot for References-', 'Eclipse is one of the top IDEs available, supporting a broad range of programming languages for application development, including Python. Primarily created for developing Java applications, support for other programming languages is introduced via plugins. The plugin used for Python development is Pydev and offers additional benefits over Eclipse IDE, such as:', '● Django, Pylint, and unittest integration', '● Interactive console', '● Remote debugger', '● Go to definition', '● Type hinting', '● Auto code completion with auto import', '● Easy to use', '● Programmer friendly features', '● Free', '● Complex user interface makes it challenging to work with', '● If you’re a beginner then using Eclipse will be difficult', 'Screenshot for References-', 'Short for Integrated Development and Learning Environment, IDLE has been bundled with Python as its default IDE for more than 15 years. IDLE is a cross-platform IDE and offers a basic set of features to keep it unburdened. The features offered, include:', '● Shell window with colorized code, input, output and error messages', '● Support for multi-window text editor', '● Code auto-completion', '● Code formatting', '● Search within files', '● Debugger with breakpoints', '● Supports smart indentation', '● Perfect for beginners and educational institutions', '● Lacks features offered by more advanced IDEs, such as project management capabilities', 'The feature-rich IDE for Python, Wing, was developed to make development faster with the introduction of intelligent features such as smart editor and simple code navigation. Wing comes in 101, Personal, and Pro variants with Pro being the most feature-rich and the only paid one. Other notable features by Wing include:', '● Code completion, error detection, and quality analysis', '● Smart refactoring capabilities', '● Interactive debugger', '● Unit tester integration', '● Customizable interface', '● Support for remote development', '● Support for frameworks such as Django, Flask, and more', '● Works well with version control systems such as Git', '● Strong debugging capabilities', '● Lacks a compelling user interface', 'Part of Amazon’s Web Services, Cloud9 IDE gives you access to a cloud-based IDE, requiring just a browser. All the code is executed on Amazon’s infrastructure, translating to a seamless and lightweight development experience. Features include:', '● Requires minimal project configuration', '● Powerful code editor', '● Code highlight, formatting and completion capabilities', '● Built-in terminal', '● Strong debugger', '● Real-time pair programming capabilities', '● Instantaneous project setup, covering most programming languages and libraries', '● Unobstructed access to several AWS services via terminal', '● Enables painless development of serverless applications', '● Remarkably robust and globally accessible infrastructure', '● Depends entirely on internet access', 'Sublime Text is one of the most commonly used cross-platform Code Editors and supports several programming languages, including Python. Sublime offers various features such as plenty of themes for visual customization, a clean and distraction-free user interface, and supports package manager for extending the core functionality via plugins. Other features include:', '● Up-to-date plugins via Package Manager', '● File auto-save', '● Macros', '● Syntax highlight and code auto-completion', '● Simultaneous code editing', '● Goto anything, definition and symbol', '● Uncluttered user interface', '● Split editing', '● Fast and high-performance editor', '● Annoying popup to buy sublime license', '● Confusingly large number of shortcuts', '● Complicated package manager', 'Developed by Microsoft, Visual Studio Code is an acclaimed cross-platform code editor that is highly customizable and allows development in several programming languages, including Python. It offers a wide variety of features to programmers, such as smart debugging, customizability, plugin support for extending core features. Key highlights include:', '● Built-in support for Git and version control', '● Code refactoring', '● Integrated terminal', '● IntelliSense for smarter code highlight and completion', '● Intuitive code debugging capabilities', '● Seamless deployment to Azure', '● Regularly updated with active community support', '● Free', '● Vast collection of plugins can make finding the right one challenging', '● Lackluster handling of large files', '● Longer launch time', 'Screenshot for References-', 'Developed by Github, the top dog in source-code hosting and software version controlling, Atom is a lightweight and cross-platform Code Editor for Python and many other programming languages. Atom provides a lot of features in the form of packages, that enhances its core features. It’s built on HTML, JavaScript, CSS, and Node.js, with the underlying framework being Electron. Features offered include:', '● Support for third-party packages via built-in Package Manager', '● Supports developer collaboration', '● Over 8000 feature and user experience-extending packages', '● Support for multi-pane file access', '● Smart code completion', '● Customizability options', '● Lightweight code editor', '● Community-driven development and support', '● Recent updates have increased RAM usage', '● Some tweaking required in settings before use', 'Also known as Project Jupyter, it is an open-source and cross-platform IDE that many data scientists and analysts prefer over other tools. Perfect for working on technologies such as AI, ML, DL, along with several programming languages, Python included. Jupyter Notebooks offer seamless creation and sharing of code, text, and equations for various purposes, including analysis, visualization, and development. Features offered include:', '● Code formatting and highlight', '● Easy sharing via email, Dropbox', '● Produces interactive output', '● Plays well with Big Data', '● Can be run from local and cloud machines', '● Requires minimal setup', '● Perfect for quick data analysis', '● Inexperienced users may find Jupyter complicated', 'Screenshot for References-', 'Picking the right IDE or Code Editor can mean the difference in saving time with quicker development or losing it due to reckless decisions. We have mentioned a lot of IDEs and Code Editors in the previous section with some of its noteworthy features. If you’re confused about which one you should pick for your next Python project, we recommend, you give it a quick read. After all, what would a programmer be without a proper set of IDEs and Code Editors?', 'Note: To eliminate problems of different kinds, I want to alert you to the fact this article represent just my personal opinion I want to share, and you possess every right to disagree with it.', 'I hope you’ve found this article useful! Below are some interesting readings hope you like them too-', 'About Author', 'Claire D. is a Content Crafter and Marketer at Digitalogy — a tech sourcing and custom matchmaking marketplace that connects people with pre-screened & top-notch developers and designers based on their specific needs across the globe. Connect with Digitalogy on Linkedin, Twitter, Instagram.']"
07/2020,10 Algorithms To Solve Before your Python Coding Interview,Programming | Interviewing | Office Hours,2.2K,17,https://towardsdatascience.com/@anbento4,https://towardsdatascience.com/10-algorithms-to-solve-before-your-python-coding-interview-feb74fb9bc27?source=collection_archive---------0-----------------------,7,7,"['10 Algorithms To Solve Before your Python Coding Interview', 'Why Practicing Algorithms Is Key?', 'There Is An Entire World Out There', 'Strings Manipulation', 'Arrays', 'Conclusion', 'You may also like:']",25,"['If you are relatively new to Python and plan to start interviewing for top companies (among which FAANG) listen to this: you need to start practicing algorithms right now.', 'Don’t be naive like I was when I first started solving them. Despite I thought that cracking a couple of algorithms every now and then was fun, I never spent too much time to practice and even less time to implement a faster or more efficient solution. Between myself, I was thinking that at the end of the day solving algorithms all day long was a bit too nerdy, it didn’t really have a practical use in the real daily work environment and it would not have brought much to my pocket in the longer term.', '“Knowing how to solve algorithms will give you a competitive advantage during the job search process”', 'Well…I was wrong (at least partially): I still think that spending too much time on algorithms without focusing on other skills is not enough to make you land your dream job, but I understood that since complex problems present themselves in every day work as a programmer, big companies had to find a standardized process to gather insights on the candidate’s problem solving and attention to detail skills. This means that knowing how to solve algorithms will give you a competitive advantage during the job search process as even less famous companies tend to adopt similar evaluation methods.', 'Pretty soon after I started solving algorithms more consistently, I found out that there are plenty of resources out there to practice, learn the most efficient strategies to solve them and get mentally ready for interviews (HackerRank, LeetCode, CodingBat and GeeksForGeeks are just few examples).', 'Together with practicing the top interview questions, these websites often group algorithms by company, embed active blogs where people share detailed summaries of their interview experience and sometimes even offer mock interview questions as part of premium plans.', 'For example, LeetCode let you filter top interview questions by specific companies and by frequency. You can also choose the level of difficulty (Easy, Medium and Hard) you feel comfortable with:', 'There are hundreds of different algorithmic problems out there, meaning that being able to recognize the common patterns and code an efficient solution in less then 10 mins will require a lot of time and dedication.', '“Don’t be disappointed if you really struggle to solve them at first , this is completely normal”', 'Don’t be disappointed if you really struggle to solve them at first, this is completely normal. Even more experienced Python programmers would find many algorithms challenging to solve in a short time without an adequate training.', 'Also don’t be disappointed if your interview doesn’t go as you expected and you just started solving algorithms. There are people that prepare for months solving a few problems every day and rehearse them regularly before they are able to nail an interview.', 'To help you in your training process, below I have selected 10 algorithms (mainly around String Manipulation and Arrays) that I have seen appearing again and again in phone coding interviews. The level of these problems is mainly easy so consider them as good starting point.', 'Please note that the solution I shared for each problem is just one of the many potential solutions that could be implemented and often a BF (“Brute Force”) one. Therefore feel free to code your own version of the algorithm, trying to find the right balance between runtime and employed memory.', 'A warm-up algorithm, that will help you practicing your slicing skills. In effect the only tricky bit is to make sure you are taking into account the case when the integer is negative. I have seen this problem presented in many different ways but it usually is the starting point for more complex requests.', 'Algorithms that require you to apply some simple calculations using strings are very common, therefore it is important to get familiar with methods like .replace() and .split()that in this case helped me removing the unwanted characters and create a list of words, the length of which can be easily measured and summed.', 'I find both approaches equally sharp: the first one for its brevity and the intuition of using the eval( )method to dynamically evaluate string-based inputs and the second one for the smart use of the ord( ) function to re-build the two strings as actual numbers trough the Unicode code points of their characters. If I really had to chose in between the two, I would probably go for the second approach as it looks more complex at first but it often comes handy in solving “Medium” and “Hard” algorithms that require more advanced string manipulation and calculations.', 'Also in this case, two potential solutions are provided and I guess that, if you are pretty new to algorithms, the first approach looks a bit more familiar as it builds as simple counter starting from an empty dictionary.', 'However understanding the second approach will help you much more in the longer term and this is because in this algorithm I simply used collection.Counter(s)instead of building a chars counter myself and replaced range(len(s)) with enumerate(s), a function that can help you identify the index more elegantly.', 'The “Valid Palindrome” problem is a real classic and you will probably find it repeatedly under many different flavors. In this case, the task is to check weather by removing at most one character, the string matches with its reversed counterpart. When s = ‘radkar’ the function returns Trueas by excluding the ‘k’ we obtain the word ‘radar’ that is a palindrome.', 'This is another very frequently asked problem and the solution provided above is pretty elegant as it can be written as a one-liner. An array is monotonic if and only if it is monotone increasing, or monotone decreasing and in order to assess it, the algorithm above takes advantage of the all() function that returns Trueif all items in an iterable are true, otherwise it returns False. If the iterable object is empty, the all() function also returns True.', 'When you work with arrays, the .remove() and .append() methods are precious allies. In this problem I have used them to first remove each zero that belongs to the original array and then append it at the end to the same array.', 'I was asked to solve this problem a couple of times in real interviews, both times the solution had to include edge cases (that I omitted here for simplicity). On paper, this an easy algorithm to build but you need to have clear in mind what you want to achieve with the for loop and if statement and be comfortable working with None values.', 'The problem is fairly intuitive but the algorithm takes advantage of a few very common set operations like set() , intersection() or &and symmetric_difference()or ^that are extremely useful to make your solution more elegant. If it is the first time you encounter them, make sure to check this article:', 'I wanted to close this section with another classic problem. A solution can be found pretty easily looping trough range(n) if you are familiar with both the prime numbers definition and the modulus operation.', 'In this article I shared the solution of 10 Python algorithms that are frequently asked problems in coding interview rounds. If you are preparing an interview with a well-known tech Company this article is a good starting point to get familiar with common algorithmic patterns and then move to more complex questions. Also note that the exercises presented in this post (together with their solutions) are slight reinterpretations of problems available on Leetcode and GeekForGeeks. I am far from being an expert in the field therefore the solutions I presented are just indicative ones.']"
07/2020,How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning…,Informational,6.4K,16,https://towardsdatascience.com/@richmondalake,https://towardsdatascience.com/how-you-should-read-research-papers-according-to-andrew-ng-stanford-deep-learning-lectures-98ecbd3ccfb3?source=collection_archive---------1-----------------------,8,9,"['How You Should Read Research Papers According To Andrew Ng (Stanford Deep Learning Lectures)', 'Introduction', 'First, Who is Andrew Ng?', 'Reading Research Papers', 'Reading A Single Research Paper', 'Questions To Ask Yourself', 'Additional Resources To Assist Research', 'Conclusion', 'I hope you found the article useful.']",38,"['“Wisdom is not a product of schooling but of the lifelong attempt to acquire it.”', '— Albert Einstein', 'The ability to understand information produced by the individuals at the cutting edge of research within Artificial Intelligence and the Machine learning domain is a skill that every serious machine learning practitioner should acquire.', 'To stay relevant and increase your knowledge, machine learning practitioners need to have an academic mindset and habit. AI, ML and DL are evolving at a fast pace, and we have to equip ourselves with the knowledge to keep up with the field, knowledge that is only attainable through research papers.', 'This article will provide you within instructions on how to go through a research paper effectively, and also provide the following:', 'For those who would like to get to the key content within this article, scroll down to the section titled “Reading Research Papers”.', 'The information I provide in this article was derived from a Stanford lecture taught by Andrew Ng. I’ve also supplemented the information contained in this article with personal tips and information from resources on the internet.', 'But first, a brief introduction on Andrew Ng.', 'Andrew Ng is probably the most known(and watched) machine learning teacher on the internet. He is also the co-founder of both Deeplearning.ai and Coursera.', 'Apart from his ongoing work within online education, he’s also a professor at Stanford University.', 'More information on Andrew Ng is just a google search away.', 'It’s natural for a person to pick up skills and habits demonstrated by individuals around their environment; this is why most PhD students will acquire the skill of effectively digesting the content of a research paper appropriately. This is somewhat a fact, and Andrew mentions it very early in the video referenced earlier.', 'But we are not PhD students, well some might be, but how do we normal individuals gain the required skills to read a research paper and understand its content wholeheartedly.', 'Specialization within the machine learning domain is favourable if you are talented. For example, having a generalist knowledge on the field of Computer vision is commendable, but having specialized knowledge and expertise within key techniques such as Pose estimation will be more appealing to companies and organization looking for practitioners within that domain.', 'So let’s use Pose Estimation as a guide to how we would approach reading research papers related to the subject matter: pose estimation.', 'A quick google search on the phrase “pose estimation” will provide you with top resources that contain information in regards to the subject matter. At this initial step, the aim is to collate all resources that are relevant, such as YouTube videos, implementation documentations and of course research papers. Ideally, at this stage, there is no limit to the number of resources you consider important, but be sure to create a shortlist of papers, videos and articles that are useful.', 'It is advisable to ensure you go through at least 10–20% of the content of each paper you have added to the list; this will ensure that you have been exposed to enough of the introductory content within an identified resource and are able to gauge its relevancy accurately.', 'For the more relevant papers/resources identified, it is expected that you progress to a higher level of understanding. Eventually, you will have identified some appropriate resources with content that you understand fully.', 'You are probably asking yourself, “what number of papers/resource is sufficient”.', 'Well, I don’t have the answer, but Andrew does.', 'According to Andrew, an understanding of 5–20 papers will showcase a basic understanding within the subject matter, perhaps enough understanding to progress to implementation of techniques.', '50–100 papers will primarily provide you with a very good understanding of the domain.', 'After going through the resources and extraction of vital information, your table might look something similar to what’s shown below.', 'The following steps will now be focused on how to read a single research paper.', 'Reading for the purpose of understanding is not done through one pass of the contents within the paper. According to Andrew, reading a paper from the first word to the last word in one sitting might not be the best way to form an understanding.', 'Be prepared to go through a paper at least three times to have a good understanding of its content', 'The introduction and conclusion section of a paper contains clear and concise information on the content of the paper and a summary of any findings. The information presented in this section usually dismisses any supplementary information and only key information are included. This is beneficial to you as a reader as you get the vital information required to proceed to the other sections within the paper.', 'For those who are generally reading research papers for informational and engineering purposes, then in-depth research might be very time consuming, especially if you have 20 more papers to get through.', 'I went through the process presented in this article with the original paper introducing the LeNet convolutional neural network, and I summarised the key content in notes which I later then converted to a series of Medium articles.', 'Andrew provides a set of questions that you should ask yourself as you read a paper. These questions generally will show you understand the critical information presented in a paper. I use the questions below as beacons to ensure I don’t stray from the aim of understanding vital information.', 'They are as follow:', 'Several online resources have made the discovery and retrieval of relevant information relatively easy. Below are examples of resources that will assist you in your search for pertinent information.', '“Learn steadily rather than short burst for longevity.“', '— Andrew Ng', 'I’m still relatively new to the field of Machine Learning and Computer Vision, there is a lot that I do not know (that’s an understatement). Still, I believe that if an individual is consistent in their search for knowledge, regardless of the domain, they’ll be rewarded with an understanding and expertise that surpasses the norm.', 'From the techniques introduced by Andrew Ng, I’ll be reading at least four research papers a month, reading to the point of understanding. I’ll be honest and say that the LeNet paper took me about a week and a half to complete wholeheartedly. But you get better and faster at reading and understanding research papers the more times you do it.', 'Andrew states in his video that he carries a batch of research papers around with him, intending to read them. Andrew is a prominent figure within the field of machine learning, and I believe emulating his habits and learning techniques can be advantageous to your learning journey.', 'To connect with me or find more content similar to this article, do the following:']"
07/2020,Will GPT-3 Kill Coding?,AI can now code in any language without additional training.,3.1K,35,https://towardsdatascience.com/@frederikbussler,https://towardsdatascience.com/will-gpt-3-kill-coding-630e4518c04d?source=collection_archive---------2-----------------------,5,6,"['Will The Latest AI Kill Coding?', 'The Evolution of GPT-n', 'Evolve or Die', 'More Than Code — GPT-3 Applied to Any Language Task', 'More Than Language — GPT Applied to Images', 'Conclusion']",29,"['In 2017, researchers asked: Could AI write most code by 2040? OpenAI’s GPT-3, now in use by beta testers, can already code in any language. Machine-dominated coding is almost at our doorstep.', 'GPT-3 was trained on hundreds of billions of words, or essentially the entire Internet, which is why it can code in CSS, JSX, Python, — you name it.', 'Further, GPT-3 doesn’t need to be “trained” for various language tasks, since its training data is all-encompassing. Instead, the network constrains itself to the task at hand when given trivial instructions.', 'GPT achieved state-of-the-art in language tasks by pairing supervised learning with unsupervised pre-training (or using the parameters from an unsupervised step as a starting point for the supervised step). Compared to its successors, GPT was tiny. It was trained on just a few thousand books and an 8 GPU machine.', 'GPT-2 drastically scaled things up, containing 10X the parameters and fed with more than 10X the training data. Still, the dataset was relatively limited, and it was trained specifically on “outbound links from Reddit which received at least 3 karma.” GPT-2 was described as a “chameleon-like” synthetic text generator, but it wasn’t state-of-the-art in downstream tasks like question answering, summarization, or translation.', 'GPT-3 is the latest and greatest in the AI world, achieving state-of-the-art in a range of tasks. Its main breakthrough is eliminating the need for task-specific fine-tuning. In terms of size, the model drastically scales up once again, reaching 175 billion parameters, or 116x the size of its predecessor.', 'While GPT-3 need not be trained at all (an example of zero-shot learning), its already-impressive performance is eclipsed via one-shot or few-shot learning.', 'Here’s the situation: Beta testers are using GPT-3 to generate working code, with trivial knowledge necessary. From buttons to data tables, or even re-creating Google’s homepage. These examples are all being done with zero-shot learning.', 'Besides the rapid evolution of AI, two other major tech trends are compounding the reality that programming jobs won’t be safe in the future: No-code and AutoML.', 'No-code refers to visual tools that make it easier for anyone to build new products, whether it’s websites, designs, data analyses, or models. WordPress, Wix, and Shopify are good examples of no-code tools that enabled millions of people to do things on their own rather than hire a developer or a designer.', 'The second trend is AutoML, or automated machine learning, which drastically shortens the time it takes to bring AI to production.', 'Tools like Apteo combine these trends, and enable anyone to deploy AI models with no coding skills required.', 'GPT-3 will spark an additional wave of no-code and AutoML tools. Many would-be employers will opt for these tools rather than hire expensive programmers.', 'Naturally, the lowest-skilled swaths of programmers will be the first to go, while experts will enjoy job security for longer — the same as in any field.', 'To direct GPT-3 to a specific language task, you simply feed it an example of what you hope to achieve. So, while you can direct GPT-3 to write code, you can also direct it to write poetry, music, social media comments, or any other text.', 'For instance, if you want to generate a news article, you might input a title like “United Methodists Agree to Historic Split” and a subtitle like “Those who oppose gay marriage will form their own denomination.”', 'With this input, GPT-3 yields the following:', '“After two days of intense debate, the United Methodist Church has agreed to a historic split — one that is expected to end in the creation of a new denomination, one that will be “theologically and socially conservative,” according to The Washington Post. The majority of delegates attending the church’s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will “discipline” clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination…”', 'Only 12% of humans correctly stated that this was written by an AI. 88% were fooled.', 'Like a human, GPT-3 can be taught new words with just one example. For instance, given the context:', 'A “Burringo” is a car with very fast acceleration. An example of a sentence that uses the word Burringo is: ____________', 'GPT-3 outputs:', 'In our garage we have a Burringo that my father drives to work every day.', 'These results are incredibly impressive. Bear in mind that AI’s inevitable evolution, so any criticism of current performance will come to naught.', 'GPT can write code, or, well, anything, but it can also generate images.', 'How is this possible?', 'The same model architecture can be trained on pixel sequences instead of text encodings, thus generating novel images instead of novel text. In fact, it’s so good at doing so, that it’s competitive with top CNNs.', 'I mention this because it shows that GPT (and its successors) doesn’t just have the potential to one day replace coders, but entire industries, given its versatility.', 'GPT-3’s mind-boggling performance has convinced many that superintelligence is closer than we think — or at least, that AI-generated code is closer than we think. It generates creative, insightful, deep, and even beautiful content. For more creative examples of GPT-3 (and if you need more convincing of how powerful it is), check out this Gwern post:']"
07/2020,Why You Should Get Google’s New Machine Learning Certificate,Don’t miss the boat ⛵️,1.8K,13,https://towardsdatascience.com/@frederikbussler,https://towardsdatascience.com/why-you-should-get-googles-new-machine-learning-certificate-56af4204744f?source=collection_archive---------3-----------------------,4,5,"['Why You Should Get Google’s New Machine Learning Certificate', 'Where to Start', 'Alternatives', 'More Than Certificates', 'Conclusion']",19,"['Google has just opened the gates to a new ML Engineer certificate. Before you charge in, bear in mind that this is geared towards professionals who want to display their competency in topics like distributed model training and scaling to production.', 'Students who don’t have practical work experience would be better served by first doing hands-on projects.', 'Getting this certificate won’t be easy. In fact, looking at the exam guide, you need very in-depth knowledge in six areas, including highly specialized topics like permission issues, dataset lineage, and data feasibility.', 'That’s why Google recommends you have 3+ years of experience with GCloud products. If you do fit that profile, then this certificate is extremely valuable for one simple reason. No one has it yet.', 'To remind you of the most basic lesson in economics: As supply increases, demand decreases.', 'Intuitively, if there are 1,000 job openings and 600 ML Engineer certificate holders, they’ll have a much easier time landing jobs and getting high salaries than if there were 6,000 or 60,000 certificate holders.', 'As beta registration just opened, there are now 0 people with this certificate. However, as Google is a leader in AI, cloud computing, and one of the world’s biggest tech companies, it won’t be long until you see these new certifications all over the place.', 'Indeed, the famous AI researcher Andrew Ng has a whopping 4 million learners across his Coursera courses. In the meantime, there are just 17,100 ML Engineers worldwide.', 'While it’s great to learn from those courses, and it’s possible that having a certification from them may provide a modest boost in some scenarios, they don’t have nearly the value they did a few years ago.', 'So, if you’re looking to get into ML Engineering, it’s best not to hesitate on Google’s new opportunity. If you want a more general data science job, check out this guide:', 'Note that Google Cloud is not the most popular cloud platform — that award goes to AWS, which has a Machine Learning certificate of its own.', 'At first glance, career-wise, going with AWS would be the better option. However, if we head to LinkedIn and search for “AWS Certified Machine Learning” (including the quotes), we get almost 2,000 results. Those are only the certified people who (1) have a LinkedIn and (2) bothered to add its exact name to their profile.', 'Remember from earlier that there’s only a very limited number of ML Engineering jobs to go around, so to be competitive, you’ll want to find a less-explored niche.', 'With 0 current holders, getting the Google Professional Machine Learning Engineer certificate is a “blue ocean” strategy: It’s a wide, open space without competition. Getting a Coursera certificate is on the extreme other end of the spectrum, and is a “red ocean” strategy: There are millions of other sharks in the water. An AWS certificate lies somewhere in the middle.', 'At the end of the day, it’s important to remember that certificates aren’t everything. In fact, they shouldn’t even be the core of your application — they should be supplemental to a strong profile based on practical skills and experience.', 'A simple process you can use to spice up your profile is picking a topic you’re interested in, analyzing the associated data, creating insightful visuals and commentary, and sharing it with your network.', 'Data science can benefit practically any industry — whether you’re interested in churn analysis, driving e-commerce sales, or people analytics, pick a topic that suits you and spread your learnings.', 'The wrong thing to do is just share certificates on social media.', 'Certificates aren’t the end-all-be-all, but the new Google Professional Machine Learning Engineer certificate is a great option for professionals seeking to advance their careers.']"
07/2020,ML Engineers Are Losing Their Jobs. Learn ML anyway,Opinion,4.2K,34,https://towardsdatascience.com/@chris-writes-code,https://towardsdatascience.com/ml-engineers-are-losing-their-jobs-learn-ml-anyway-87e19523cd9b?source=collection_archive---------4-----------------------,4,8,"['ML Engineers Are Losing Their Jobs. Learn ML anyway', 'AI Winter will not affect most AI/ML/DS jobs', 'You do not need bleeding edge AI to solve problems', 'Use machine learning, but focus on creating value over changing the world', 'Learning ML Is the best way to fight fear of AI', 'There is a gap in tools to make ML easy', 'Do software engineering first', 'Conclusion']",39,"['Disclaimer: This is an opinion piece. I’d love to hear your thoughts and counter arguments in the comments.', 'There’s a lot of doom and gloom in the field.', 'Hiring is frozen.', '…some hypothesize that investors will lose hope in AI altogether. Google has freezed hiring for ML researchers. Uber laid off the research half of their AI team…there will be far more people with ML skills than ML jobs.- Chip Huyen', 'We have a recession.', 'People are talking about an AI Winter.', 'It makes sense that artificial intelligence (AI), machine learning (ML) and data science (DS) are the first to go in a crunch. They’re luxuries for most businesses.', 'But that doesn’t mean the future isn’t bright.', 'If you create value.', 'An AI winter is a period of decreased funding and interest in AI research.', 'But most of us don’t do research. We read papers, get ideas and innovate... but we use existing techniques.', 'Additionally, the popularity of building ML-powered products doesn’t necessarily correlate with the volume of research coming out.', 'If anything, there’s an increasing amount of research that’s not being applied. Anecdotally, industry is still catching up in its implementation of machine learning invented decades ago.', '“AI-powered” products are more popular now because ML is more approachable, not because of new research.', 'The opposite is true.', 'Classic algorithms + domain knowledge + niche datasets are going to solve most real problems, not deep neural nets. Most of us aren’t working on self-driving cars.', 'I wrote about this in “Democratizing AI is irrelevant, Data is siloed, And how to build an AI company anyway”.', 'In my opinion, focusing on extreme technical competency is overrated outside large tech companies, in contrast to a problem-solving mentality and general development skills.', 'Outside of tech, there’s still a tonne of boring/manual work that should have been automated a long time ago. And it doesn’t require breakthroughs.', 'When you solve a problem (any problem), everyone wins.', 'Silicon Valley has deluded us to believe we should be taking moonshots rather than improving our local communities and the lives of people we know.', 'I love Uber, and it has changed the world. But if keeping Uber alive costs $5 billion per quarter, maybe something is wrong.', 'Yes, some companies are long term plays and will affect 7 billion people. But simpler improvements like reducing data entry mistakes in a “boring” industry also creates value.', 'We hear of automation killing jobs, because nothing sells like fear. Not because technological unemployment is around the corner.', 'Pick up machine learning. Then try conceptualizing, training and deploying a model to solve a real problem. You’ll quickly see how hard this still is, and secondly, how far we must be away from an AGI takeover.', 'Infrastructure is super underdeveloped and real data is messy.', 'When you download a CSV from Kaggle to train a model for a specific problem, 99% of the work has been done for you.', 'If more people did this, they’d sleep better at night.', 'Ease-of-use has done more for ML adoption than any algorithm breakthroughs over the last 10 years.', 'We’re almost at the point where a software engineer can cobble together an ML solution using out-of-the-box components, but it’s not easy enough yet.', 'As tools progress, we’ll see less pure ML jobs but a huge increase in software engineers using ML to solve all sorts of problems. And more companies outside of “tech” benefiting.', 'If you become an engineer who builds great tools for ML, I’d forever be in your debt….- Chip Huyen', 'ML is driving value around the world, but I think we’ve barely scratched the surface. Wait until the right tools are here.', 'Unless you have an advanced degree in an AI related subject, do yourself a favour and learn software engineering. Then move into AI.', 'Learning software engineering is like getting an MBA in technology (when MBAs were valuable). You’ll learn fundamentals, create full stack solutions, and understand the code that facilitates ML.', 'There are also more jobs and it will be easier to change careers as the industry landscape shifts.', 'Many software engineers go on to successful careers in ML/DS. But you rarely see the opposite.', 'There is mega hype around AI. And with any rise comes a “fall”, but that doesn’t have to be a bad thing if we’re prepared.', 'If we focus on developing a general skillset (including ML), solving real problems, and creating value, there will always be something for us to do.']"
07/2020,Do You Know Python Has A Built-In Database?,An introduction of Python built-in library — sqlite3,3.7K,8,https://towardsdatascience.com/@qiuyujx,https://towardsdatascience.com/do-you-know-python-has-a-built-in-database-d553989c87bd?source=collection_archive---------5-----------------------,6,4,"['Do You Know Python Has A Built-In Database?', 'Import and Usage', 'Seamless Integrate with Pandas', 'Summary']",33,"['If you are a software developer, I believe you must know or even have used an extremely light-weighted database — SQLite. It has almost all the features you need as a relational database, but everything is saved in a single file. In the official site, here are some scenarios that you could use SQLite.', 'There are more reasons that you may want to use SQLite, please check out the documentation.', 'Most importantly, SQLite is actually built-in as a Python library. In other words, you don’t need to install any server-side/client-side software, and you don’t need to keep something running as a service, as long as you imported the library in Python and start coding, then you have a relational database management system!', 'When we say “built-in”, it means that you don’t even need to run pip install to acquire the library. Simply import it by:', 'Don’t be bothered with the drivers, connection strings and so on. You can create an SQLite database and have a connection object as simple as:', 'After we run this line of code, we have created the database and connected to it already. This is because the database we asked Python to connect to is not existing so that it automatically created an empty one. Otherwise, we can use exactly the same code to connect to an existing database.', 'Then, let’s create a table.', 'In this USER table, we added three columns. As you can see, SQLite is indeed light-weight, but it supports all the basic features of a regular RDBMS should have, such as the data type, nullable, primary key and auto-increment.', 'After running this code, we should have created a table already, although it outputs nothing.', 'Let’s insert some records into the USER table we just created, which can also prove that we indeed created it.', 'Suppose we want to insert multiple entries in one go. SQLite in Python can achieve this easily.', 'We need to define the SQL statement with question marks ? as placeholder. Then, let’s create some sample data to be inserted. With the connection object, we can then insert these sample rows.', 'It didn’t complain after we’ve run the code, so it was successful.', 'Now, it’s time to verify everything we have done in a tangible way. Let’s query the table to get the sample rows back.', 'You can see how simple it is.', 'Also, even though SQLite is light-weighted, but as a widely-used database, most of the SQL clients software support to consume it.', 'The one I use the most is DBeaver, let’s see how it looks like.', 'Because I’m using Google Colab, so I’m going to download the my-test.db file to my local machine. In your case, if you run Python on your local machine, you can use your SQL client to connect directly to the databases file.', 'In DBeaver, create a new connection and select SQLite as DB type.', 'Then, browse to the DB file.', 'Now, you can run any SQL query on the database. It is nothing different from other regular relational databases.', 'Do you think that’s all? No. In fact, as a built-in feature of Python, SQLite can seamlessly integrate with Pandas Data Frame.', 'Let’s define a data frame.', 'Then, we can simply call to_sql() method of the data frame to save it into the database.', 'That’s it! We even don’t need to create the table in advance, the column data types and length will be inferred. Of course, you can still define it beforehand if you want to.', 'Then, let’s say we want to join the table USER and SKILL, and read the result into a Pandas data frame. It’s also seamless.', 'Super cool! Let’s write the results to a new table called USER_SKILL.', 'Then, we can also use our SQL client to retrieve the table.', 'Indeed, there are many surprises hidden in Python. They do not mean to be hidden, but just because there are too many out-of-box features existing in Python for one to discover all of them.', 'In this article, I have introduced how to use the Python built-in library sqlite3 to create and manipulate tables in an SQLite DB. Of course, it also supports updating and deleting but I think you would try it yourself after this.', 'Most importantly, we can easily read a table from an SQLite DB into a Pandas data frame, or vice versa. This allows us to even more easily to interact with our light-weight relational database.', 'You may notice that SQLite doesn’t have authentication, that’s it designed behaviour as everything needs to be light-weight. Go discovering more surprising features in Python, enjoy it!', 'All the code in this article can be found in my Google Colab Notebook.']"
07/2020,8 ML/AI Projects To Make Your Portfolio Stand Out,Interesting project ideas with source code and…,3.8K,10,https://towardsdatascience.com/@techykajal,https://towardsdatascience.com/8-ml-ai-projects-to-make-your-portfolio-stand-out-bfc5be94e063?source=collection_archive---------6-----------------------,9,9,"['8 ML/AI Projects To Make Your Portfolio Stand Out', '1. Sentiment analysis for depression based on social media post', '2. Sports match video to text summarization using neural network', '3. Handwritten equation solver using CNN', '4. Business meeting summary generation using NLP', '5. Facial recognition to detect mood and suggest songs accordingly', '6. Finding out habitable exo-planet from images captured by space vehicles like Kepler', '7. Image regeneration for old damaged reel picture', '8. Music generation using deep learning']",34,"['Are you excited to enter the Data Science world?Congrats! That’s still the right choice because of the ultimate boost in need of work done in Data Science and Artificial Intelligence during this pandemic.', 'Although, because of the crisis, the market currently gets tougher to be able to set it up again with more men force as they are doing earlier. So, It might possible that you have to prepare yourself mentally for long run hiring journey and many rejections in a way.', 'Hereby, while writing this article, I am assuming that you have already known that data-science portfolio is crucial and how to build it up. You might spend most of your time, doing data crunching and wrangling and not applying fancy models.', 'One question that I have asked on and on by data science enthusiasts is that what kind of projects should they include in their portfolio to build tremendously good and unique portfolio.', 'Below I have given the 8 unique ideas for your data science portfolio with attached reference articles from where you will get the insights of how to get started with any particular idea.', 'This topic is so sensitive to be considered nowadays and in urgent need to do something about it. There are more than 264 million individuals worldwide who are suffering from depression. Depression is the main cause of disability worldwide and is a significant supporter of the overall global burden of disease and nearly 800,000 individuals consistently bite the dust because of suicide every year. Suicide is the second driving reason for death in 15–29-year-olds. Treatment for depression is often delayed, imprecise, and/or missed entirely.', 'Internet-based life gives the main edge chance to change early melancholy mediation services, especially in youthful grown-ups. Consistently, roughly 6,000 Tweets are tweeted on Twitter, which relates to more than 350,000 tweets sent for each moment, 500 million tweets for every day, and around 200 billion tweets for each year.', 'As indicated by the Pew Research Center, 72% of the public uses some sort of internet-based life. Datasets released from social networks are important to numerous fields, for example, human science and brain research. But the supports from a specialized point of view are a long way from enough, and explicit methodologies are desperately out of luck.', 'By analyzing linguistic markers in social media posts, it’s possible to create a deep learning model that can give an individual insight into his or her mental health far earlier than traditional approaches.', 'So this project idea is basically based on getting precise summary out of Sports match videos. There are sports websites that tell about highlights of the match. Various models have been proposed for the task of extractive text summarization but neural networks do the best job. As a rule, Summarization alludes to introducing information in a brief structure, concentrating on parts that convey facts and information, while safeguarding the importance.', 'Automatically creating an outline of a game video gives rise to the challenge of distinguishing fascinating minutes, or highlights, of a game.', 'So, one can achieve that using some deep learning techniques like 3D-CNN (three-dimensional convolutional networks), RNN(Recurrent neural network), LSTM (Long short term memory networks) and also through Machine learning algorithms by dividing the video into different sections and then applying SVM(Support vector machines), NN(Neural Networks), k-means algorithm.', 'For better understanding, do refer to the attached articles in detail.', 'Among all the issues, handwritten mathematical expression recognition is one of the confounding issues in the region of computer vision research. You can train Handwritten equation solver by handwritten digits and mathematical symbols using Convolutional Neural Network (CNN) with some image processing techniques. Developing such a system requires training our machines with data, making it proficient to learn and make the required prediction.', 'Do refer to the below-attached articles for better understanding.', 'Ever got stuck in a situation, where everyone wants to see a summary not full reports. Well, I face it during my school and college days where we spend a lot of time preparing a whole report but the teacher only has time to read the summary.', 'Summarization has risen as an inexorably helpful way to tackle the issue of data over-burden. Extracting information from conversations can be of very good commercial and educational value. This can be done by feature capture of the statistical, linguistic, and sentimental aspects with the dialogue structure of the conversation.', 'Manually changing the report to a summed up form is too time taking, isn’t that so? But one can rely on Natural Language Processing (NLP) techniques to achieve that.', 'Text summarization using deep learning can understand the context of the entire text. Isn’t it a dream come true for all of us who need to come up with a quick summary of a document !!', 'Do refer to the below-attached articles for better understanding.', 'The human face is an important part of an individual’s body and it particularly plays a significant role in knowing a person’s state of mind. This eliminates the dreary and tedious task of manually isolating or grouping songs into various records and helps in generating an appropriate playlist based on an individual’s emotional features.', 'People tend to listen to music based on their mood and interests. One can create an application to suggest songs for users based on their mood by capturing facial expressions.', 'Computer vision is an interdisciplinary field that helps convey a high-level understanding of digital images or videos to computers. computer vision components can be used to determine the user’s emotion through facial expressions.', 'There are these APIs too that I found interesting and useful, although I didn’t work on these but attaching here with a hope that these will gonna help you.', 'In the most recent decade, over a million stars were monitored to identify transiting planets. Manual interpretation of potential exoplanet candidates is labor-intensive and subject to human mistake, the consequences of which are hard to evaluate. Convolutional neural networks are fit for identifying Earth-like exoplanets in noisy time-series data with more prominent precision than a least-squares strategy.', 'I know, how time- consuming and painful it is to get back your old damaged photo in the original form as it was earlier. So, this can be done using deep learning by finding all the image defects (fractures, scuffs, holes), and using Inpainting algorithms, one can easily discover the defects based on the pixel values around them to restore and colorize the old photos.', 'Music is an assortment of tones of various frequencies. So, the Automatic Music Generation is a process of composing a short piece of music with the least human mediation . Recently, Deep Learning engineering has become the cutting edge for programmed Music Generation.', 'I know that it’s a real struggle to build up a cool data science portfolio. But with such collection that I have provided above, you can make above-average progress in that field. The collection is new which gives opportunity for research purposes too. So, researchers in Data-Science can also choose these ideas to work on so that their research would be a great help for Data Scientists to start with project. And moreover It’s a real fun to explore the sides that nobody has done before.Although, this collection is actually constitute of ideas from beginning to advanced level.', 'So, I will not only recommend this for newbies in the data science area but also senior data scientists. It will open many new paths during your career, not only because of the projects but also through the newly gained network.', 'These ideas show you the broad range of possibilities and give you the ideas to think out of the box.', 'For me and my friends, the learning factors, adding value to the society and the unexplored knowledge is important and the fun in a way is essential. So, basically I enjoys doing such projects that give us a way to gain immense knowledge in a way and let us explore the unexplored dimensions. That is our main focus when dedicating time to such projects.', 'I hope you guys will find this article informative & useful for you. Do share your thoughts about these project ideas in the comment box & do let me know about other cool ideas if you have any✌️', 'You can reach me via the following :', 'Check out my other Blogs as well:']"
07/2020,Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT,"Preprocessing, Model Design, Evaluation…",1.1K,12,https://towardsdatascience.com/@mdipietro09,https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=collection_archive---------7-----------------------,22,1,['Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT'],76,"['In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with Word2Vec), and the cutting edge Language models (with BERT).', 'NLP (Natural Language Processing) is the field of artificial intelligence that studies the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP is often applied for classifying text data. Text classification is the problem of assigning categories to text data according to its content.', 'There are different techniques to extract information from raw text data and use it to train a classification model. This tutorial compares the old school approach of Bag-of-Words (used with a simple machine learning algorithm), the popular Word Embedding model (used with a deep learning neural network), and the state of the art Language models (used with transfer learning from attention-based transformers) that have completely revolutionized the NLP landscape.', 'I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example (link to the full code below).', 'I will use the “News category dataset” in which you are provided with news headlines from the year 2012 to 2018 obtained from HuffPost and you are asked to classify them with the right category, therefore this is a multiclass classification problem (link below).', 'In particular, I will go through:', 'First of all, I need to import the following libraries:', 'The dataset is contained into a json file, so I will first read it into a list of dictionaries with json and then transform it into a pandas Dataframe.', 'The original dataset contains over 30 categories, but for the purposes of this tutorial, I will work with a subset of 3: Entertainment, Politics, and Tech.', 'In order to understand the composition of the dataset, I am going to look into the univariate distribution of the target by showing labels frequency with a bar plot.', 'The dataset is imbalanced: the proportion of Tech news is really small compared to the others, this will make for models to recognize Tech news rather tough.', 'Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set.', 'That function removes a set of words from the corpus if given. I can create a list of generic stop words for the English vocabulary with nltk (we could edit this list by adding or removing words).', 'Now I shall apply the function I wrote on the whole dataset and store the result in a new column named “text_clean” so that you can choose to work with the raw corpus or the preprocessed text.', 'If you are interested in a deeper text analysis and preprocessing, you can check this article. With this in mind, I am going to partition the dataset into training set (70%) and test set (30%) in order to evaluate the models performance.', 'Let’s get started, shall we?', 'The Bag-of-Words model is simple: it builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. To put it another way, each word in the vocabulary becomes a feature and a document is represented by a vector with the same length of the vocabulary (a “bag of words”). For instance, let’s take 3 sentences and represent them with this approach:', 'As you can imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the feature matrix will be a huge sparse matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming/lemmatization) aimed to reduce the dimensionality problem.', 'Terms frequency is not necessarily the best representation for text. In fact, you can find in the corpus common words with the highest frequency but little predictive power over the target variable. To address this problem there is an advanced variant of the Bag-of-Words that, instead of simple counting, uses the term frequency–inverse document frequency (or Tf–Idf). Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.', 'Let’s start with the Feature Engineering, the process to create features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. “new” and “york”) and bigrams (i.e. “new york”). I will provide the code for the classic count vectorizer as well:', 'Now I will use the vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix.', 'The feature matrix X_train has a shape of 34,265 (Number of documents in training) x 10,000 (Length of vocabulary) and it’s pretty sparse:', 'In order to know the position of a certain word, we can look it up in the vocabulary:', 'If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.', 'In order to drop some columns and reduce the matrix dimensionality, we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:', 'I reduced the number of features from 10,000 to 3,152 by keeping the most statistically relevant ones. Let’s print some:', 'We can refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary.', 'The new feature matrix X_train has a shape of is 34,265 (Number of documents in training) x 3,152 (Length of the given vocabulary). Let’s see if the matrix is less sparse:', 'It’s time to train a machine learning model and test it. I recommend using a Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes’ Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.', 'I’m going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step.', 'We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:', 'The BoW model got 85% of the test set right (Accuracy is 0.85), but struggles to recognize Tech news (only 252 predicted correctly).', 'Let’s try to understand why the model classifies news with a certain category and assess the explainability of these predictions. The lime package can help us to build an explainer. To give an illustration, I will take a random observation from the test set and see what the model predicts and why.', 'That makes sense: the words “Clinton” and “GOP” pointed the model in the right direction (Politics news) even if the word “Stage” is more common among Entertainment news.', 'Word Embedding is the collective name for feature learning techniques where words from the vocabulary are mapped to vectors of real numbers. These vectors are calculated from the probability distribution for each word appearing before or after another. To put it another way, words of the same context usually appear together in the corpus, so they will be close in the vector space as well. For instance, let’s take the 3 sentences from the previous example:', 'In this tutorial, I’m going to use the first model of this family: Google’s Word2Vec (2013). Other popular Word Embedding models are Stanford’s GloVe (2014) and Facebook’s FastText (2016).', 'Word2Vec produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space. That can be done using 2 different approaches: starting from a single word to predict its context (Skip-gram) or starting from the context to predict a word (Continuous Bag-of-Words).', 'In Python, you can load a pre-trained Word Embedding model from genism-data like this:', 'Instead of using a pre-trained model, I am going to fit my own Word2Vec on the training data corpus with gensim. Before fitting the model, the corpus needs to be transformed into a list of lists of n-grams. In this particular case, I’ll try to capture unigrams (“york”), bigrams (“new york”), and trigrams (“new york city”).', 'When fitting the Word2Vec, you need to specify:', 'We have our embedding model, so we can select any word from the corpus and transform it into a vector.', 'We can even use it to visualize a word and its context into a smaller dimensional space (2D or 3D) by applying any dimensionality reduction algorithm (i.e. TSNE).', 'That’s pretty cool and all, but how can the word embedding be useful to predict the news category? Well, the word vectors can be used in a neural network as weights. This is how:', 'Let’s start with the Feature Engineering by transforming the same preprocessed corpus (list of lists of n-grams) given to the Word2Vec into a list of sequences using tensorflow/keras:', 'The feature matrix X_train has a shape of 34,265 x 15 (Number of sequences x Sequences max length). Let’s visualize it:', 'Every text in the corpus is now an id sequence with length 15. For instance, if a text had 10 tokens in it, then the sequence is composed of 10 ids + 5 0s, which is the padding element (while the id for word not in the vocabulary is 1). Let’s print how a text from the train set has been transformed into a sequence with the padding and the vocabulary.', 'Before moving on, don’t forget to do the same feature engineering on the test set as well:', 'We’ve got our X_train and X_test, now we need to create the matrix of embedding that will be used as a weight matrix in the neural network classifier.', 'That code generates a matrix of shape 22,338 x 300 (Length of vocabulary extracted from the corpus x Vector size). It can be navigated by word id, which can be obtained from the vocabulary.', 'It’s finally time to build a deep learning model. I’m going to use the embedding matrix in the first Embedding layer of the neural network that I will build and train to classify the news. Each id in the input sequence will be used as the index to access the embedding matrix. The output of this Embedding layer will be a 2D matrix with a word vector for each word id in the input sequence (Sequence length x Vector size). Let’s use the sentence “I like this article” as an example:', 'My neural network shall be structured as follows:', 'Now we can train the model and check the performance on a subset of the training set used for validation before testing it on the actual test set.', 'Nice! In some epochs, the accuracy reached 0.89. In order to complete the evaluation of the Word Embedding model, let’s predict the test set and compare the same metrics used before (code for metrics is the same as before).', 'The model performs as good as the previous one, in fact, it also struggles to classify Tech news.', 'But is it explainable as well? Yes, it is! I put an Attention layer in the neural network to extract the weights of each word and understand how much those contributed to classify an instance. So I’ll try to use Attention weights to build an explainer (similar to the one seen in the previous section):', 'Just like before, the words “clinton” and “gop” activated the neurons of the model, but this time also “high” and “benghazi” have been considered slightly relevant for the prediction.', 'Language Models, or Contextualized/Dynamic Word Embeddings, overcome the biggest limitation of the classic Word Embedding approach: polysemy disambiguation, a word with different meanings (e.g. “ bank” or “stick”) is identified by just one vector. One of the first popular ones was ELMO (2018), which doesn’t apply a fixed embedding but, using a bidirectional LSTM, looks at the entire sentence and then assigns an embedding to each word.', 'Enter Transformers: a new modeling technique presented by Google’s paper Attention is All You Need (2017) in which it was demonstrated that sequence models (like LSTM) can be totally replaced by Attention mechanisms, even obtaining better performances.', 'Google’s BERT (Bidirectional Encoder Representations from Transformers, 2018) combines ELMO context embedding and several Transformers, plus it’s bidirectional (which was a big novelty for Transformers). The vector BERT assigns to a word is a function of the entire sentence, therefore, a word can have different vectors based on the contexts. Let’s try it using transformers:', 'If we change the input text into “bank money”, we get this instead:', 'In order to complete a text classification task, you can use BERT in 3 different ways:', 'I’m going with the latter and do transfer learning from a pre-trained lighter version of BERT, called Distil-BERT (66 million of parameters instead of 110 million!).', 'As usual, before fitting the model there is some Feature Engineering to do, but this time it’s gonna be a little trickier. To give an illustration of what I’m going to do, let’s take as an example our beloved sentence “I like this article”, which has to be transformed into 3 vectors (Ids, Mask, Segment):', 'First of all, we need to select the sequence max length. This time I’m gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like “zzdata” is given, BERT would split it into [“z”, “##z”, “##data”]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length.', 'Please note that I’m using the raw text as corpus (so far I’ve been using the clean_text column).', 'The feature matrix X_train has a shape of 3 x 34,265 x 50. We can check a random observation from the feature matrix:', 'You can take the same code and apply it to dtf_test[“text”] to get X_test.', 'Now, I’m going to build the deep learning model with transfer learning from the pre-trained BERT. Basically, I’m going to summarize the output of BERT into one vector with Average Pooling and then add two final Dense layers to predict the probability of each news category.', 'If you want to use the original versions of BERT, here’s the code (remember to redo the feature engineering with the right tokenizer):', 'As I said, I’m going to use the lighter version instead, Distil-BERT:', 'Let’s train, test, evaluate this bad boy (code for evaluation is the same):', 'The performance of BERT is slightly better than the previous models, in fact, it can recognize more Tech news than the others.', 'This article has been a tutorial to demonstrate how to apply different NLP models to a multiclass classification use case. I compared 3 popular approaches: Bag-of-Words with Tf-Idf, Word Embedding with Word2Vec, and Language model with BERT. I went through Feature Engineering & Selection, Model Design & Testing, Evaluation & Explainability, comparing the 3 models in each step (where possible).', 'Please note that I haven’t covered explainability for BERT as I’m still working on that, but I will update this article as soon as I can. If you have any useful resources about that, feel free to contact me.', 'This article is part of the series NLP with Python, see also:', 'Contacts: LinkedIn | Twitter']"
07/2020,GPT-3: Creative Potential of NLP,New ML milestone by OpenAI — in action,1.8K,6,https://towardsdatascience.com/@merzmensch,https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab?source=collection_archive---------8-----------------------,7,3,"['GPT-3: Creative Potential of NLP', 'Various experiments (and alerting signals).', 'Summary.']",54,"['It was last year in February, as OpenAI published results on their training of unsupervised language model GPT-2. Trained in 40Gb texts (8 Mio websites) and was able to predict words in proximity. GPT-2, a transformer-based language applied to self-attention, allowed us to generated very convincing and coherent texts. The quality was that good, so the main model with 1.5 billion parameters wasn’t initially publicly accessible, to prevent uncontrolled fake news. Luckily, the complete model was later published and could be even used with Colab Notebooks.', 'This year OpenAI strikes back with new language model GPT-3. With 175 billion parameters (read also: GPT-3 Paper).Unnecessary spoiler: it’s incredibly good.', 'There are already some profound articles on TDS examining features and paper of GPT-3:', 'OpenAI is building an API, currently accessible via waiting list:', 'Fortunately, I could get access and experiment with GPT-3 directly. Here are some of my initial outcomes.', 'The AI Playground interface looks simple, but it bears the power within. For the first, here is a setting dialog, which lets you configure text length, temperature (from low/boring to standard to chaotic/creative), and other features.', 'You also can define where the generated text has to start and to stop, these are some of the control functions that have a direct impact on textual results.', 'The simple interface provides also some GPT-3 presets. The amazing thing about transformer-driven GPT-models is among others the ability to recognize a specific style, text character, or structure. In case you begin with lists, GPT-3 continues generating lists. In case your prompt has a Q&A structure, it will be kept coherently. If you ask for a poem, it writes a poem.', 'You can do your own presets, or use the existing, which are:', 'Chat.', 'A typical setting for a chatbot. You ask - AI answers. It’s possible to change the “characters” or setting also. As you can see, the chat situation was accomplished perfectly (even if my, Human’s, third question was kind of unfair).', 'To demonstrate the contextual impact, let’s change the AI character from “helpful” and “very friendly” to “brutal, stupid and very unfriendly”. You will see how the whole dialogue will be influenced:', 'I think, we re-invented Marvin the Paranoid Android.', 'Q&A', 'This preset consists of a clear dual structure: Question and Answer. You need some training before it starts to answer the question (and get the rules), but then it works perfectly. I asked some random questions from various areas and here you go:', 'I’d say, perfect!', 'Parsing unstructured data', 'This one is fascinating and shows a good comprehension of the unstructured text — extracting structured data from the full text.', 'Summarizing for a 2nd grader', 'This preset shows another level of comprehension — including rephrasing of difficult concepts and sentences in clear words.', 'I tried Wittgenstein:', 'The simple proverb can be paraphrased convincingly:', 'Or look at this pretty well and clear transition of Sigmund Freud’s time distancing concept:', 'As you see, compression of text and its coherent “translation” is one of the strengths of GPT-3.', 'GPT-2 was already a great language model when it was about English. You could generate amazing texts, especially with 1.5 billion parameters. I used GPT-2 for a screenplay of this short movie — and its absurdity could be rather understood as a good tradition of David Lynch and Beckett:', 'The dialogues were logical, even if spontaneous. But it was regarding English. If you’ve tried with inputs in other languages, you would face the barrier of understanding. GPT-2 tried to imitate languages, but you needed to fine-tune it on text corpus in a specific language to get good results.', 'GPT-3 is different.', 'Its processing in other languages is phenomenal.', 'I tried German, Russian, and Japanese.', 'German.', 'It was rather my daughter, who tried to let GPT-3 write a fairy tale. She began with “Eine Katze mit Flügeln ging im Park spazieren” (“A cat with wings took a walk in a park”).', 'The emerged story was astonishingly well written. With irony, vivid characters, and some leitmotifs. This is not just a collection of topoi or connected sentences. This is… a story!', 'Russian.', 'I trained once GPT-2 on Pushkin’s poetry and have got some interesting neologisms, but it was a grammar mess. Here I input some lines of Pushkin’s poem — and the result I’ve got was… interesting. It hadn’t rhymes, but stylistically intense power. It was not Pushkin style, though. But almost without any mistakes or weird grammar. And… it works as poetry (especially if you are ready to interpret it).', 'Japanese.', 'This was something special. I entered just a random sentence:', '今日は楽しい一日になりますように！と言いました。// Today was funny and entertaining day, I said.', 'And the result was a small story about prayer, happiness, wisdom, and financial investment. In well written Japanese (neutral politeness form, like the input).', 'It does mean: GPT-3 is ready for multilingual text processing.', 'My first try was, of course, to write a Shakespearean sonnet. So the prompt was just:', 'The result was this:', 'Perfect iambic verse, great style, nice rhymes… If not one thing:', 'The first two lines are actually from Alexander Pope, The Rape of the Lock. And here we have a reason to be cautious: GPT-3 produces unique and unrepeatable texts, but it can reuse the whole quotes of existing texts it was trained on.', 'Re-examination of results is inevitable if you want to guarantee a singularity of a text.', 'I wonder, if there are some possibilities for “Projection” like StyleGAN2 feature, just in opposite to StyleGAN2 (where it compares the image with latent space), in GPT-3 it would compare with the dataset it was trained on? To prevent accidental plagiarism.', 'But the thing is: GPT-3 can write poems on demand, in particular styles.', 'Here is another example:', 'As I still hadn’t accessed, I asked a friend to let GPT-3 write an essay on Kurt Schwitters, a German artist, and Dadaist:', 'The outcome is: GPT-3 has already a rich knowledge, which can be recollected. It is not always reliable (you have to fine-tune it to have a perfect meaning match), but it’s still very close to the discourse.', 'Another mindblowing possibility is using GPT-3 is quite different cases than just text generation:', 'You can get support by CSS:', 'And calling it General Intelligence is already a thing:', 'We are still at the beginning, but the experiments with GPT-3 made by the AI community show its power, potential, and impact. We just have to use it with reason and good intention. But that’s the human factor. Which is not always the best one.', 'For more wonderful text experiments I highly recommend you to read Gwern:']"
07/2020,Top 9 Data Science certifications to know about in 2020,Some of the best data science certification…,1.8K,17,https://towardsdatascience.com/@rashidesai2424,https://towardsdatascience.com/top-9-data-science-certifications-in-2020-40b0192ade43?source=collection_archive---------9-----------------------,9,11,"['Top 9 Data Science certifications to know about in 2020', '1. IBM Data Science Professional Certification', '2. Tableau Data Scientist | Desktop Specialist', '3. HarvardX’s Data Science Professional Certificate', '4. Business Analytics Specialization', '5. Advanced Business Analytics Specialization', '6. Amazon AWS Big Data Certification', '7. SAS Academy for Data Science', '8. MCSE: Data Management and Analytics', '9. Microsoft Certified Azure Data Scientist Associate', 'Know your author']",56,"['The growing popularity of MOOCs is an undeniable fact. There is evidence of a growing number of corporates using MOOCs for workforce training development. To my belief (and the recruiters I’ve met), online certifications are an evidence of your abilities beyond textbook knowledge on a standard reference platform.', 'Learners take advantage of MOOCs out of personal interest or enhancing their job prospects and skill set. The primary goal, however, remains to gain credibility or relevance with skills in current times.', '“How are courses and certificates viewed by potential employers?” In my share of discussing this with the recruiters, I’ve had a chance to, the bottom line is: certifications put you on a common ground where your performance and skills laud for you. Certifications exhibit the extra mile you went for honing your skills.', 'Having said that, let’s take a look at the top 9 online certifications that you can complete for Data Science this year!', 'Duration — 3 months (flexible)Level — BeginnerPlatform — Coursera', 'You get: Certificates and digital badge', 'Having completed this certification, I can assure y’all that this one is the best beginner-level data science certification program for enthusiasts looking to kickstart their professional data science career.', 'From explaining what data science is and why is it so popular to making the learners do a capstone with APIs integration — I highly recommend taking up this 9-course challenge!', 'The certification requires no pre-requisites. However, if you wish a better grasp of learning, I’d suggest completing a crash course on Python beforehand. By course 6, you will start building projects from scratch, making this a perfect way to get some fancy projects on your resumes!', 'Access the certification: Link', 'Duration — 3 months (flexible)Level — BeginnerPlatform — Tableau e-learningCost — FREE', 'You get: Digital role badge', 'Tableau is offering discounts on many of its certifications but this one — Data Scientist Learning Path is for FREE. A sister certification is Tableau Data Analyst.', 'The Tableau Data Scientist path includes —', 'Tableau offers three major paid certifications —', 'Access the certification: Link', 'This exam is for those who have basic, foundational skills and understanding of Tableau Desktop and at least three months of applying an understanding, and experience with Tableau.', 'Fee =$50 through June 30, 2020 ($100 after June 30)Time Limit: 60 minutesQuestion Format: Multiple choice, multiple responseNumber of Questions: 30Passing score: 70%', 'Duration — 1 year 5 months (flexible)Level — BeginnerPlatform — edXCost = $441', 'Harvard University partnered with edX has a Data Science Certification that covers fundamental R programming skills, statistical concepts such as probability, inference, and modeling, experience with packages as tidyverse, ggplot2, and dplyr.', 'The best part about this certification is the course touching base on essential tools for practicing data scientists such as Unix/Linux, Git and GitHub, and RStudio', 'Another crest for this certification is its realism — the courses introduce the learners to motivating real-world case studies in the likes of —', 'Access the certification: Link', 'Platform — CourseraLevel — BeginnerDuration — 6 months (3 hr/week)', 'The Business Analytics Specialization is hosted on Coursera developed with Wharton School of the University of Pennsylvania. It provides a good foundational introduction to big data analytics across business professions such as marketing, human resources, operations and finance. The courses require NO prior analytics experience.', 'A stats on the website says 46% of learners started a new career after completing this specialization and 21% of learners got a pay increase or promotion.', 'Having audited the course and completing it, I can say it surely develops a sense of how we as data analysts should and can describe, predict, and inform business decisions in specific business areas. I am sure, after completing the specialization, any learner will develop an analytic mindset to help make better strategic decisions based on data, than having done before.', 'The capstone requires learners to combine knowledge from the four courses and encourages to make data-driven decisions for real business challenges faced by global technology giants as Google, Facebook and Yahoo.', 'Access the certification: Link', 'Platform — CourseraLevel — IntermediateDuration — 5 months (3 hr/week)', 'The Advanced Business Analytics Specialization is hosted on Coursera developed with the University of Colorado, Boulder. The specialization merges the thin line between academia and business-world to combine learnings from experienced practitioners from both domains to share real-world data analytics skills to the learners.', 'A stats on the website says 50% of learners started a new career after completing this specialization! That’s something we are talking about!', 'Again, after completing this specialization, I am affirmed that I can now better draft and recognize the maximum value for shareholders. The course walked me through a thorough experience in data extraction and manipulation using SQL and to leverage statistical techniques for descriptive, predictive, and prescriptive analytics for different business domains. More importantly, the course effectively teaches how to interpret and present analytic results for an efficient decision making.', 'Access the certification: Link', 'Duration — 170 minutes to complete the examLevel — AdvancedPlatform — AWSCost — $300Type — Multiple choice, multiple answer', 'One of the other tough certifications, AWS Big Data Certification requires test takers to hold either of these certifications', 'The test requires a minimum of five years of hands-on experience in a data analytics field (not an easy challenge for college grads!)', 'Access the Certification: Link', 'Duration — Few months to several yearsCost — $250Level — AdvancedPlatform — SAS Academy for Data Science', 'The SAS Academy for Data Science is one of the prestigious platforms to learn SAS for Data Science. It offers courses in data curation, advanced analytics, AI, and machine learning to advance your career in data. Mind you, SAS itself is a tricky horizon to explore for beginners. If you are ready to take up one of the below examinations, make sure you are in complete knowledge of the prerequisites.', 'There are three basic pathways in SAS Academy', 'SAS Academy focuses on concepts of —', 'Access the certifications: Link', 'Duration — 180 minutes examLevel — AdvancedPlatform — MicrosoftPrice —$165Total Questions — 45 to 55Type — Single and multi-choice questions', 'With Microsoft Certified Solutions Expert (MCSE), you can demonstrate broad skillsets in database administration, SQL, building enterprise-scale data solutions, and leveraging business intelligence data.', 'Access the certification: Link', 'Duration — 180 minutes examLevel — AdvancedPlatform — Microsoft', 'Price — $165Total Questions — 51Type — Single and multi-choice questions (~50% questions from ML studio, 40% from ML service & 10% generic data science questions.)', 'By far the most tenuous certification I’ve come across and therefore the last on my list, the Microsoft Azure Data Scientist Certification is aimed for learners who wants to apply their data science and machine learning knowledge to implement and run machine learning models on Azure.', 'The good part about this certification is that you complete the examination with deploying a model as a service. What better than a product-ready project!?', 'The path is simple: Take one exam and earn the certificate', 'Access the certification: Link', 'That’s it from my end for this blog. Thank you for reading! I hope you enjoyed the article. Do let me know what certification are you looking forward to learning or exploring in your Data Science journey this summer?', 'Happy Data Tenting!', 'Disclaimer: The views expressed in this article are my own and do not represent a strict outlook.', 'Rashi is a graduate student at the University of Illinois, Chicago. She loves to visualize data and create insightful stories. When not rushing to meet school deadlines, she adores writing about technology, UX, and more with a good cup of hot chocolate.']"
08/2020,Why developers are falling in love with functional programming,"From Python to Haskell, the trend isn’t…",2.7K,44,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/why-developers-are-falling-in-love-with-functional-programming-13514df4048e?source=collection_archive---------0-----------------------,7,5,"['Why developers are falling in love with functional programming', 'It’s all about killing side effects', 'What functional programming is not (only)', 'Some languages are getting more functional than others', ""Big data is coming. And it's bringing a friend: functional programming.""]",42,"['Functional programming has been around for the last 60 years, but so far it’s always been a niche phenomenon. Although game-changers like Google rely on its key concepts, the average programmer of today knows little to nothing about it.', 'That’s about to change. Not only are languages like Java or Python adopting more and more concepts from functional programming. Newer languages like Haskell are going completely functional.', 'In simple terms, functional programming is all about building functions for immutable variables. In contrast, object-oriented programming is about having a relatively fixed set of functions, and you’re primarily modifying or adding new variables.', 'Because of its nature, functional programming is great for in-demand tasks such as data analysis and machine learning. This doesn’t mean that you should say goodbye to object-oriented programming and go completely functional instead. It is useful, however, to know about the basic principles so you can use them to your advantage when appropriate.', 'To understand functional programming, we need to understand functions first. This might sound boring, but at the end of the day it’s pretty insightful. So keep reading.', 'A function, naively stated, is a thing that transforms some input into some output. Except that it’s not always that simple. Consider this function in Python:', 'This function is dumb and simple; it takes one variable x, presumably an int, or perhaps a float or double, and spits out the square of that.', 'Now consider this function:', 'On the first glance, it looks like the function takes a variable x, of whichever type, and returns nothing since there is no return statement. But wait!', 'The function wouldn’t work if global_list hadn’t been defined beforehand, and its output is that same list, albeit modified. Even though global_list was never declared as an input, it changes when we use the function:', 'Instead of an empty list, this returns [1,2]. This shows that the list is indeed an input of the function, even though we weren’t explicit about it. And that could be a problem.', 'These implicit inputs — or outputs, in other cases — have an official name: side effects. While we were only using a simple example, in more complex programs these can cause real difficulties.', 'Think about how you would test append_to_list: Instead of just reading the first line and testing the function with any x, you need to read the whole definition, understand what it’s doing, define global_list, and test it that way. What’s simple in this example can quickly become tedious when you’re dealing with programs with thousands of lines of code.', 'The good news is that there is an easy fix: being honest about what the function takes as an input. This is much better:', 'We haven’t really changed much. The output is still [1,2], and everything else remains the same, too.', 'We have changed one thing, however: the code is now free of side effects. And that’s great news.', 'When you now look at the function declaration, you know exactly what’s going on. Therefore, if the program isn’t behaving as expected, you can easily test each function on its own and pinpoint which one is faulty.', 'A function with clearly declared in- and outputs is one without side effects. And a function without side effects is a pure function.', 'A very simple definition of functional programming is this: writing a program only in pure functions. Pure functions never modify variables, but only create new ones as an output. (I cheated a bit in the example above: it goes along the lines of functional programming, but still uses a global list. You can find better examples, but it was about the basic principle here.)', 'Moreover, you can expect a certain output from a pure function with a given input. In contrast, an impure function may depend on some global variable; so the same input variables may lead to different outputs if the global variable is different. The latter can make debugging and maintaining code a lot harder.', 'There’s an easy rule to spot side effects: as every function must have some kind of in- and output, function declarations that go without any in- or output must be impure. These are the first declarations that you might want to change if you’re adopting functional programming.', 'Loops are not a thing in functional programming. Consider these Python loops:', 'For the simple operations that you’re trying to do, this code is rather long. It’s not functional, either, because you’re modifying global variables.', 'Instead, consider this:', 'This is fully functional. It’s shorter. It’s faster because you’re not iterating through many elements of an array. And once you’ve understood how filter, map, and reduce work, the code isn’t much harder to understand either.', 'That doesn’t mean that all functional code uses map, reduce and the likes. It doesn’t mean that you need functional programming to understand map and reduce, either. It’s just that when you’re abstracting loops, these functions pop up rather a lot.', 'When talking about the history of functional programming, many start with the invention of lambda functions. But although lambdas are without doubt a cornerstone of functional programming, they’re not the root cause.', 'Lambda functions are tools that can be used to make a program functional. But you can use lambdas in object-oriented programming, too.', 'The example above isn’t statically typed. Yet it is functional.', 'Even though static typing adds an extra layer of security to your code, it isn’t essential to make it functional. It can be a nice addition, though.', 'Perl takes a very different approach to side effects than most programming languages. It includes a magic argument, $_, which makes side effects one of its core features. Perl does have its virtues, but I wouldn’t try functional programming with it.', 'I wish you good luck with writing functional code in Java. Not only will half of your program consist of static keywords; most other Java developers will also call your program a disgrace.', 'That’s not to say that Java is bad. But it’s not made for those problems that are best solved with functional programming, such as database management or machine learning applications.', 'This is an interesting one: Scala’s goal is to unify object-oriented and functional programming. If you find this kind of odd, you’re not alone: while functional programming aims at eliminating side effects completely, object-oriented programming tries to keep them inside objects.', 'That being said, many developers see Scala as a language to help them transition from object-oriented to functional programming. This may make it easier for them to go fully functional in the years to come.', 'Python actively encourages functional programming. You can see this by the fact that every function has, by default, at least one input, self. This is very much à la the Zen of Python: explicit is better than implicit!', 'According to its creator, Clojure is about 80% functional. All values are immutable by default, just like you need them in functional programming. However, you can get around that by using mutable-value wrappers around these immutable values. When you open such a wrapper, the thing you get out is immutable again.', 'This is one of the few languages that are purely functional and statically typed. While this might seem like a time-drainer during development, it pays of bigly when you’re debugging a program. It’s not as easy to learn as other languages, but it’s definitely worth the investment!', 'In comparison to object-oriented programming, functional programming is still a niche phenomenon. If the inclusions of functional programming principles in Python and other languages are of any significance, however, then functional programming seems to be gaining traction.', 'That makes perfect sense: functional programming is great for big databases, parallel programming, and machine learning. And all these things have been booming over the last decade.', 'While object-oriented code has uncountable virtues, those of functional code, therefore, shouldn’t be neglected. Learning some basic principles can often be enough to up your game as a developer and be ready for the future.', 'Thanks for reading! If you’d like to know how to implement more elements of functional programming in your Python code, stay tuned. I’ll cover this in my next story.']"
08/2020,Amazon Wants to Make You an ML Practitioner— For Free,The tech giant plans to speed up ML proficiency…,4K,15,https://towardsdatascience.com/@anthonyagnone,https://towardsdatascience.com/amazon-wants-to-make-you-an-ml-practitioner-for-free-552c46cea9ba?source=collection_archive---------1-----------------------,4,6,"['Amazon Wants to Make You an ML Practitioner— For Free', 'The What and Why', 'Amazon’s Take', 'Opinions and Cautions', 'Resources', 'Stay Up To Date']",13,"['Amazon has long been striving to fix the issue of excess demand (vs supply) of individuals who have proficiency across the fields both Machine Learning and Software Engineering. To date, they have developed a slew of internal resources to get employees up to speed on the essentials. This is typically referred to as OJT, for “on the job training.”', 'OJT only goes so far — the size of your workforce. Aside from hired workers, companies depend on the education system to routinely supply capable talent to the workforce. This system has performed sufficiently for hundreds of years. However, the tide is turning. The speed of machine learning’s integration into industry workflows has largely outpaced the education system’s ability to provide fully-equipped talent. This is partially due to large systems necessarily moving slowly, but also due to a lack of convergence of dominant algorithms and tools in the field. Education systems are basically faced with a choice between overfitting on current trends versus sticking with classical techniques and allowing for OJT to solve the last-mile problem.', 'Amazon has a great idea — meet halfway.', 'Academic institutions will largely lean towards proven classical techniques for education, and that is the correct move. To help the last-mile OJT problem even more than post-hire education, Amazon is now making available course materials from their internal “ML University”. By doing this, they will be able to educate many eventual employees even before it is interview time. This helps both sides of the table. Prospective employees can learn much more relevant material ahead of job applications and feel more equipped in job selection and commitment. On the flip side, Amazon and similar companies can then judge talent more directly in interviews than they have been able to. Since so much learning material is publicly available, there is less room for “the benefit of the doubt” when an applicant does not have experience in a certain sub-area.', 'Just three courses have been released for immediate use: natural language, computer vision, and tabular data. However, more will be rolling out through the end of 2020, with the start of 2021 having all the material public.', '“By going public with the classes, we are contributing to the scientific community on the topic of machine learning, and making machine learning more democratic,” Werness adds. “This field isn’t limited to individuals with advanced science degrees, or technical backgrounds. This initiative to bring our courseware online represents a step toward lowering barriers for software developers, students and other builders who want to get started with practical machine learning.”- Amazon Science', 'Check out the intro to the “Accelerated Computer Vision” course below. The entire course is available on similar Youtube pages.', 'This is great for the democratization of machine learning in the industry. Academic has long been very open and cooperative with ML research. The same can be said for the open-source software movement. Recently, in the past decade or so, we have seen these ideologies extend into the ML industry space. Its continuation will ensure that the economy’s aggregate output will rise, while still fostering healthy competition.', 'I’ll add a word of caution, however. The phenomenon referred to as “vendor lock-in” occurs when a service provider produces so much incentive to continue acquiring its own products across its ecosystem that the consumer effectively becomes stuck buying the provider’s goods and services, lest he/she suffer either lackluster integrations or the switching cost of starting over with a new provider. Look no further than a comparison of Apple vs Microsoft vs Google products for examples of vendor lock-in at work.', 'The courses at ML University indeed appear at the outset to provide a lot of general applicability across the ML and software space. It is likely that 80–90% of all of its material will do so, which is great!', 'However, as you go through the courses, remain keen on staying up-to-date on how other providers are accomplishing similar products and services. To be a truly marketable ML practitioner in this evolving workforce, one must stay flexible in showing ML proficiency independent of algorithm, language, framework, and platform provider.', 'Aside from here on Medium, keep yourself updated with the LifeWithData blog, the Machine Learning UTD Newsletter, and my Twitter. Through those platforms, I provide more long-form and short-form thoughts, respectively.', 'If you’re not a fan of emails and social media, but still want to stay in the loop, consider adding lifewithdata.org/blog and lifewithdata.org/newsletter to a Feedly aggregation setup.']"
08/2020,5 Reasons why you should Switch from Jupyter Notebook to Scripts,Opinion,3K,47,https://towardsdatascience.com/@khuyentran1476,https://towardsdatascience.com/5-reasons-why-you-should-switch-from-jupyter-notebook-to-scripts-cb3535ba9c95?source=collection_archive---------2-----------------------,7,10,"['5 Reasons why you should Switch from Jupyter Notebook to Scripts', 'Motivation', 'Organized', 'Encourage Experiment', 'Ideal for Reproducibility', 'Easy to Debug', 'Ideal for Production', 'I didn’t like the Idea of Not Using Jupyter Notebook until I Pushed myself out of my Comfort Zone', 'So are you Suggesting me to Stop Using Jupyter Notebook?', 'Conclusion']",35,"['Like most people, the first tool I used when started learning data science is Jupyter Notebook. Most of the online data science courses use Jupyter Notebook as a medium to teach. This makes sense because it is easier for beginners to start writing code in Jupyter Notebook’s cells than writing a script with classes and functions.', 'Another reason why Jupyter Notebook is such a common tool in data science is that Jupyter Notebook makes it easy to explore and plot the data. When we type ‘Shift + Enter’, we will immediately see the results of the code, which makes it easy for us to identify whether our code works or not.', 'However, I realized several fallbacks of Jupyter Notebook as I work with more data science projects:', 'I knew there must be a better way to handle my code so I decided to give scripts a try. To make it less confusing, I refer to .py file when using word “script” in this article. These are the benefits I found when using scripts:', 'The cells in Jupyter Notebook make it difficult to organize the code into different parts. With a script, we could create several small functions with each function specifies what the code does like this', 'Better yet, if these functions could be categorized in the same category such as functions to process the data, we could put them in the same class!', 'Whenever we want to process our data, we know the functions in the class Preprocess can be used for this purpose.', 'When we want to experiment with a different approach to preprocess data, we could just add or remove a function by commenting out like this without being afraid to break the code! Even if we happen to break the code, we know exactly where to fix it.', 'With classes and functions, we could make the code general enough so that it will be able to work with other data.', 'For example, if we want to drop different columns in my new data, we just need to change columns_to_drop to a list of columns, we want to drop and the code will run smoothly!', 'I can also create a pipeline that specifies steps to process and train the data! Once I have a pipeline, all I need to do is to use', 'to apply the same processing to both the train and test data.', 'With functions, it is easier to test whether that function produces the output we expect. We can quickly spot out where in the code we should change to produce the output we want', 'If all of the tests pass but there is still an error in running our code, we know the data is where we should look next.', 'For example, after passing the test above, I still have a TypeError when running the script, which gives me the idea that my data has null values. I just need to take care of that to run the code smoothly.', 'I wrote an article on how to use Pytest for testing here.', 'We can use different functions in multiple scripts on top of something else like this', 'or to add a config file to control the values of the variables. This prevents us from wasting time tracking down a specific variable in the code just to change its value.', 'We could also easily add tools to track the experiment such as MLFlow or tools to handle configuration such as Hydra.cc!', 'If you don’t know about hydra, it is a Python tool to configure your data science projects. This tool allows you to experiment with different parameters and models without spending hours fixing your code.', 'I used to use Jupyter Notebook all the time. When some data scientists advise me to switch from Jupyter Notebook to script to prevent some problems listed above, I didn’t understand and felt resistant to do so. I didn’t like the uncertainty of not being able to see the outcome when I run the cell.', 'But the disadvantage of Jupyter Notebook grew as I started my first real data science project in my new company so I decided to push myself out of my comfort zone and experiment with scripts.', 'In the beginning, I felt uncomfortable but started to notice the benefits of using scripts. I started to feel more organized when my code is organized into different functions, classes, and into multiple scripts with each script serving different purposes such as preprocessing, training, and testing.', 'Don’t get me wrong. I still use Jupyter Notebook if my code is small and if I don’t plan to put my code into production. I use Jupyter Notebook when I want to explore and visualize the data. I also use it to explain how to use some python libraries. For example, I write use mostly Jupyter Notebooks in this repository as the medium to explain the code mentioned in all of my articles.', 'If you don’t feel comfortable with coding everything in scripts, you could use both scripts and Jupyter Notebook for different purposes. For example, you could create classes and functions in scripts then import them in the notebook so that the notebook is less messy.', ""Another alternative is to turn the notebook into the script after writing the notebook. I personally don't prefer this approach because it often takes me longer to organize the code in my notebook such as put them into functions and classes and write test functions."", 'I find writing a small function then writing a small test function is faster and safer. If I happen to want to speeds up my code with the new Python library, I could use the test function I already wrote to make sure it still works as I expected.', 'With that being said, I believe there are more ways to solve the disadvantage of Jupyter Notebook than what I mentioned here such as how Netflix uses put the notebook into production and schedule the notebook to run at a certain time.', 'Everybody has their own way to make their workflow more efficient and to me, it is to leverage the utility of scripts. If you have just switched from Jupyter Notebook to script, it might not be intuitive to write code in scripts, but trust me, you will get used to using scripts eventually.', 'Once that happens, you will start to realize many benefits of the scripts over the messy Jupyter Notebook and want to write most of your code in scripts.', 'If you are looking for methods to switch from Jupyter Notebook, this article provides some good tips to make your code reproducible, automatable, and deployable with scripts.', 'If you don’t feel comfortable with the big change, start small.', 'Big changes start with small steps', 'I like to write about basic data science concepts and play with different algorithms and data science tools. You could connect with me on LinkedIn and Twitter.', 'Star this repo if you want to check out the codes for all of the articles I have written. Follow me on Medium to stay informed with my latest data science articles like these']"
08/2020,How I’d start learning machine learning again (3-years in),Putting the engineer back in machine…,2.1K,11,https://towardsdatascience.com/@mrdbourke,https://towardsdatascience.com/how-id-start-learning-machine-learning-again-3-years-in-55c52aaee52a?source=collection_archive---------3-----------------------,10,10,"['How I’d start learning machine learning again (3-years in)', 'If you came for a list of courses, you’re in the wrong place', 'The curse of the engineer (and technology nerd)', '“I want to build things”', '“I want to do research”', 'Skill before certificates', 'How I’d start again', 'Share your work', 'What’s missing?', 'Video version of this article']",82,"['I’m underground, back where it all started. Sitting at the hidden cafe where I first met Mike. I’d been studying in my bedroom for the past 9-months and decided to step out of the cave. Half of me was concerned about having to pay $19 for breakfast (unless it’s Christmas, driving Uber on the weekends isn’t very lucrative), the other half about whether any of this study I’d been doing online meant anything.', 'In 2017, I left Apple, tried to build a web startup, failed, discovered machine learning, fell in love, signed up to a deep learning course with zero coding experience, emailed the support team asking what the refund policy was, didn’t get a refund, spent the next 3-months handing in the assignments four to six days late, somehow passed, decided to keep going and created my own AI Masters Degree.', '9-months into my AI Masters Degree, I met Mike, we had coffee, I told him my grand plan; use AI to help the world move more and eat better, he told me I should I meet Cam, I met Cam, I told Cam I’m going to the US, he said why not stay here, come in on Thursday, okay, went in on Thursday for a 1-day a week internship and two weeks later was offered a role as a junior machine learning engineer at Max Kelsen.', '14-months into my machine learning engineer role, I decided to leave and try it on my own. I wrote an article about what I’d learned, Andrei found it, emailed me asking if I wanted to build a beginner-friendly machine learning course, I said yes, we built the course and 6-months in we’ve got the privilege of teaching 27,177 students in 150+ countries.', 'Add it up and you get about 3-years. About the time my original undergraduate degree was supposed to take (due to several failures, I took 5-years to do a 3-year degree).', 'So as it stands, I feel like I’ve done a machine learning undergraduate degree.', 'Someone looking from the outside in might think I know a fair bit about machine learning and I do, I know a lot more than I started but I also know how much I don’t know. That’s the thing with knowledge.', 'But enough about me. That’s my story. Yours might be similar or you might be starting out today.', 'If you’re getting started, this article is for you. If you’re a veteran, you can offer your advice or critique my ideas.', 'Let’s get into it, shall we?', 'I’ve done a bunch of online courses. I’ve even created my own.', 'And guess what?', 'They’re all remixes of the same thing.', 'Instead of worrying about which course is better than another, find a teacher who excites you.', 'Learning anything is 10% material and 90% being excited to learn.', 'How many of your school teachers do you remember?', 'My guess is, regardless of what they taught, you remember the teacher themselves more than the material. And if you remember the material, it’s because they sparked a fire in you enough for it to be burned into your memory.', 'What then?', 'Dabble in a few resources, you’re smart enough to find the best ones. See which ones spark your interest enough to keep going and stick with those.', 'It isn’t an unpleasant task to learn a skill if the teacher gets you interested in it.', 'Show me an engineer who proclaims her use case of the latest and greatest tools and I’ll show you an amateur.', 'I’ll confess. I’m guilty. Every new shiny framework which comes out, every new state of the art model, I’m onto it.', 'Often I’ll catch myself trying to invent a problem to use whatever new tool is on the market. A classic cart before the horse scenario.', 'A chef’s entire work centres around two tools, the controlled use of fire and a knife.', 'This is embodied in the best programming advice I’ve ever received: learn the language, not the framework.', 'If you’re just starting out and can’t count the number of tools you’re learning on one hand, you’re trying to use too many.', 'If you want to build things, such as web applications or mobile applications, learn software engineering before (or at least alongside) machine learning.', 'Too many models live and die within Jupyter Notebooks.', 'Why?', 'Because machine learning is an infrastructure problem (infrastructure means all the things which go around your model so others can use it, the hot new term you’ll want to lookup is MLOps).', 'And deployment, as in getting your models into the hands of others, is hard.', 'But that’s exactly why I should’ve spent more time there.', 'If I was starting again today, I’d find a way to deploy every semi-decent model I build (with exceptions for the dozens of experiments leading to the one worth sharing).', 'How?', 'Don’t be afraid to make something simple. A basic front-end which someone can interact with is far more interesting than a notebook in a GitHub repo.', 'No really, how?', 'Train a model, build a front-end application around it with Streamlit, get the application working locally (on your computer), once it’s working wrap the application with Docker, then deploy the Docker container to Heroku or another cloud provider.', 'Sure, we’re going against the rule here of using a few too many tools, but pulling this off a few times will get you thinking about what it’s like to get your machine learning model into people’s hands.', 'Deploying your models will raise the questions you don’t get to ask when your machine learning model lives its life in a Jupyter Notebook, like:', 'Building things becomes research. You’ll want your models to work faster, better. To achieve this, you’ll need to research alternative ways of doing things. You’ll find yourself reading research papers, replicating them and improving upon them.', 'I’m often asked, “how much math should I know before I start machine learning?”', 'To which I usually reply, “how much walking should I know before I go for a run?”', 'I don’t really say this, I’m usually nicer and say something like, “can you solve the problem you’re currently working on?”, if so, you know enough, if not, learn more.', 'As a side note, I’ve just ordered the Mathematics for Machine Learning book. I’m going to be spending the next month or two reading it cover to cover. Having read the free text online it’s more than enough to cover the fundamentals.', 'I’ve got online course certificates coming out of my ass.', 'I got caught thinking more certificates equals more skills.', 'I’d burn through lectures on 1.75x speed just to get to the end, pass the automated exam and share my progress online.', 'I optimised for completing courses instead of creating skills. Because watching someone else explain it was easier than learning how to do it myself.', 'Idiot.', 'Here’s the thing. Everything I learned for an exam, I’ve forgotten. Everything I learned through experimenting, I remember.', 'Now, this isn’t to say online certifications and courses aren’t worth your time. Courses help to build foundational skills. But working on your own projects helps to build specific knowledge (knowledge which can’t be taught).', 'Learning (anything) isn’t linear, better to read the same book twice (as long as it’s got some substance) than to add more to the pile.', 'I often tell my students, despite the immense proudness I feel when I see someone share a graduation certificate, I’d prefer them not to finish my course and instead take the parts they need and use them for their own work.', 'Before you add something, ask yourself, “have I sucked the juice out of what I’ve already covered?”', 'First of all, more important than any resource is to get rid of the “I can’t learn it” mentality. That’s bullsh*t. You’ve got the internet. You can learn anything.', 'The internet has given rise to a new kind of hunter-gatherer. And if you decide to take on the challenge you can gather resources to create your own path.', 'The following path isn’t set either. It’s designed to be a compass rather than a map. And guess what? It’s all accessible online.', 'Let’s lay some foundations.', 'If I was starting again I’d learn far more software engineering practices intertwined with machine learning.', 'My main goal would be to build more things people could interact with.', 'The machine learning specific parts would be:', 'Alongside these, I’d go through:', 'There’s a lot here. So to consolidate my knowledge I’d build 1–2 milestone projects using Streamlit or the web development skills I’d learned from freeCodeCamp. And of course, these would be shared on GitHub.', 'Once I’d gotten some foundational machine learning skills, I’d build upon them with the following.', 'Again, after going through these, I’d consolidate my knowledge by building a project people can interact with.', 'An example would be a web application powered by a machine learning model.', 'Two of the biggest things you pay for with a college degree is accountability and structure.', 'Good news is, you can get both of these yourself.', 'I created my own AI Masters Degree as a form of accountability and structure. You can do something similar.', 'In fact, if I was starting again, I’d follow something more similar to Jason Benn’s How I learned web development, software engineering & ML. It’s similar to mine but includes more software engineering practices.', 'If you can find a (small) community to learn with others, that’s a big bonus. I’m still not quite sure how to do this.', 'A billion dollar idea is to develop a platform where people can create their own self-driven curriculums and interact with others who are on similar paths. I say self-driven here because all knowledge is largely self-taught. Rather than hand-feed knowledge, the role of an instructor is instead more to excite, guide and challenge.', 'Learning and reading is inhaling. Building and creating is exhaling. Don’t hold your breath.', 'Balance your consumption of materials with creations of your own.', 'For example, you might spend 6 weeks learning, then 6 weeks putting your knowledge together in a form of shared work.', 'Your shared work is your new resume.', 'Where?', 'GitHub and your own blog. Use the other platforms when needed. For machine learning projects, a runnable Colab notebook is your minimum requirement.', 'Everything here is biased by my own experience of graduating from a nutrition degree, spending 9-months studying machine learning in my bedroom whilst driving Uber on the weekends to pay for courses, getting a machine learning job, leaving the job and building a machine learning course.', 'I have no experience of going to a coding bootcamp or university to learn technological skills so therefore can’t compare the differences.', 'Though, since we’re talking about code and math, it either works or it doesn’t. Knowing this, the contents of the materials you choose doesn’t matter as much as how you learn it.', 'I put together some clips from the last three years as well as riffed on a few points to go along with this article. Not all points are the same but they stick with the theme.']"
08/2020,The Best Data Science Certification You’ve Never Heard Of,A practical guide to the most valuable…,1.7K,12,https://towardsdatascience.com/@nicolejaneway,https://towardsdatascience.com/best-data-science-certification-4f221ac3dbe3?source=collection_archive---------4-----------------------,9,1,['The Best Data Science Certification You’ve Never Heard Of'],45,"['**Update 11/23:', '**Update 10/12: I’m now recognized as a CDMP Associate after passing the Fundamentals Exam. Questions for me? Drop them in the comments or join the study group.', '**Update 8/15: it’s recently come to my attention that the certification exams are open book, which is extremely exciting because it means less time memorizing and more time working with data in a real world setting. Also, I am starting a study group on Facebook — join for help with your exam prep.', 'Eight years ago, data science was proclaimed “the sexiest job of the 21st century.” Yet plodding through hours of data munging still feels decidedly unsexy. If anything, the storied rise of the data science career has illustrated just how poorly most organizations are doing when it comes to managing their data.', 'Enter the Certified Data Management Professional (CDMP) from Data Management Association International (DAMA). The CDMP is the best data strategy certification you’ve never heard of. (And honestly, when you consider the fact that you’re probably working a job that didn’t exist ten years ago, it’s not surprising that this certification isn’t widespread just yet.)', 'Data strategy is a crucial discipline that spans end-to-end management of the data lifecycle as well as associated aspects of data governance and key considerations of data ethics.', 'This article outlines the hows and whys of getting the CDMP, which lays the groundwork for effective thought leadership on data strategy. It also includes a survey — you can offer your thoughts on the most important aspects of data management for data science and check out the consensus of the community.', 'In this guide:', 'Disclaimer: this post is not sponsored by DAMA International — views reflected are mine alone. I’m including an affiliate link to the DMBOK on Amazon, the reference guide that is required for the exam, given that it’s an open book test. Buying the book through this link helps support my writing on Data Science and Data Strategy — thanks in advance.', 'Training for the CDMP confers expertise across 14 areas related to data strategy (which I’ll cover in more detail in a later section). The test is open book, but the 100 questions on the exam must be completed within 90 minutes — not a lot of time to be looking things up. Therefore, it’s important to be extremely familiar with the reference material.', 'When you schedule the exam ($300), DAMA provides 40 practice questions that are pretty reflective of the difficulty of the actual exam. As a further resource, check out this article about the process of studying for a certification.', 'It’s possible to sit for the exam online while monitored via webcam ($11 proctoring fee). The format of the exam is multiple choice — choose the single correct option out of five. You can mark questions and come back to them. At the conclusion of test taking, you get immediate feedback on your score.', 'Anything over 60% is considered passing. This is just fine if you’re interested in getting your CDMP Associate certification and moving along. If you’re interested in the advanced tiers of CDMP certification, you’ll have to pass with a 70% (CDMP Practitioner) or 80% (CDMP Master). To get certified at the highest level, CDMP Fellow, you’ll need to attain the Master Certification and also demonstrate industry experience and contribution to the field. Each of these advanced certifications also require passing two Specialist exams.', 'This brings me to my final point, which is about why — purely from a career advancement standpoint — you should chose to put yourself through the studying and exam taking process for CDMP: certification from DAMA is associated with high-end positions in leadership, management, and data architecture. (Think of CDMP as getting credentialed into a semi-secret society of data ninjas.) Increasingly, enterprise roles and federal contracts related to data management are requesting CDMP certification. Read more.', 'Pros:', 'Cons:', 'Alternatives:', 'Given that CDMP is an open book test, to study for the exam, all that’s needed is the DAMA Body of Knowledge book (DMBOK $55). It’s around 600 pages, but if you mainly focus your study time on Chapter 1 (Data Management), diagrams & schemas, roles & responsibilities, and definitions, then this should get you 80% of the way toward a passing score.', 'In terms of how to use DMBOK, one test taker recommended 4–6 hours per weekend for 8–10 weeks. Another approach could be reading a couple pages each morning and evening. However you tackle it, make sure you’re incorporating spaced repetition into your studying methodology.', 'In addition to being your study guide for the exam, the DMBOK is of course useful as reference book, and you can drop it on your colleague’s desk if they need to learn data strategy or if they’ve nodded off during a webinar.', 'The CDMP covers 14 topics —I’ve listed them in order of the prevalence with which they occur on the exam and provided a brief definition for each.', 'Data Governance ( 11%) — practices and processes to ensure formal management of data assets. Read more.', 'Data Quality ( 11%) — assuring data is fit for consumption based on its accuracy, completeness, consistency, integrity, reasonability, timeliness, uniqueness/deduplication, validity, and accessibility. Read more.', 'Data Modelling and Design ( 11%) — translation of business needs into technical specifications. Read more.', 'Metadata Management (11%) — information about data collected. Read more.', 'Master and Reference Data Management (10%) — reference data is information used to categorize other data found in a database, or information that is solely for relating data in a database to information beyond the boundaries of the organization. Master reference data refers to information that is shared across a number of systems within the organization. Read more.', 'Data Warehousing and Business Intelligence (10%) — a data warehouse stores information from operational systems (as well as other data resources, potentially) in a way that is optimized to support decision-making processes. Business intelligence refers to the use of technology to gather and analyze data, then translate it into useful information. Read more.', 'Document and Content Management (6%) — technologies, methods, and tools used to organize and store an organization’s documents. Read more.', 'Data Integration and Interoperability ( 6%) — use of technical and business processes to merge data from different sources, with the goal of readily and efficiently providing access to valuable information. Read more.', 'Data Architecture (6%) — specifications to describe existing state, define data requirements, guide data integration, and control data assets, according to the organization’s data strategy. Read more.', 'Data Security ( 6%) — implementation of policies and procedures to ensure people and things take the right actions with data and information assets, even in the presence of malicious inputs. Read more.', 'Data Storage and Operations ( 6%) — characterization of hardware or software that holds, deletes, backs up, organizes, and secures an organization’s information. Read more.', 'Data Management Process ( 2%) — end-to-end management of data, including collection, control, protection, delivery, and enhancement. Read more.', 'Big Data ( 2%) — extremely large datasets, often composed of various structured, unstructured, and semi-structured data types. Read more.', 'Data Ethics ( 2%) — code of conduct encompassing data handling, algorithms, and other practices to ensure that data is used appropriately in a moral context. Read more.', 'Out of curiosity, I’d love to hear your thoughts about the most important aspect of data management. After you make your selection in the poll below, you’ll see what the community thinks as well.', 'What considerations drove your choice? Do you think studying for CDMP is an effective way to learn these topics? Let’s talk in the comments.', 'Still not convinced why data strategy is important? Let’s take a look from the perspective of a data scientist aiming to increase their knowledge and earning potential.', 'It’s been said that a data scientist sits at the nexus of statistics, computer science, and domain knowledge. Why would you want to add one more thing to your plate?', 'Successwise, you’re better off being good at two complementary skills than being excellent at one', 'Scott Adams, author and creator of the Dilbert comics, offers the idea that “every skill you acquire doubles your odds of success.” He acknowledges this may be somewhat of an oversimplification — “obviously some skills are more valuable than others, and the twelfth skill you acquire might have less value than each of the first eleven” — but the point is that sometimes it’s better to go wide than to go deep.', 'Setting aside the relative magnitude of the benefit (because I seriously doubt it’s 2x per skill… thank you, law of diminishing marginal returns), it seems unquestionable that broadening your skillset can lead to more significant gains relative to toiling away at learning one specific skill. In a nutshell, this is why I think it’s important for a data scientist to learn data strategy.', 'Generally speaking, having diversity in your skillset allows you to:', 'Understanding data strategy transforms you from being a data consumer into an empowered data advocate at your organization. It’s worth putting up with all the tongue twister acronyms (DMBOK — really? Couldn’t they have just called it The Data Management Book?) in order to deepen your appreciation for the end-to-end knowledge generating process.', 'If you enjoyed reading this article, follow me on Medium, LinkedIn, and Twitter for more ideas to advance your data science skills. Join the study group for the CDMP Exam. Buy the DMBOK.']"
08/2020,You are telling people that you are a Python beginner if you ask this question.,-,2.7K,16,https://towardsdatascience.com/@wyfok,https://towardsdatascience.com/you-are-telling-people-that-you-are-a-python-beginner-if-you-ask-this-question-fe35514ca091?source=collection_archive---------5-----------------------,5,1,['You are telling people that you are a Python beginner if you ask this question.'],21,"['A few days ago when I browsed the “learnpython” sub on Reddit, I saw a Redditor asking this question again. Although there are too many answers and explanations about this question on the Internet, many beginners still do not know about it and make mistakes. Here is the question', 'What is the difference between “==” and “is”?', 'Both “==” and “is” are operators in Python(Link to operator page in Python). For beginners, they may interpret “a == b” as “a is equal to b” and “a is b” as, well, “a is b”. Probably this is the reason why beginners confuse “==” and “is” in Python.', 'I want to show some examples of using “==” and “is” first before the in-depth discussion.', 'Simple, right? a == b and a is b both return True. Then go to the next example.', 'WTF ?!? The only change from the first example to the second is the values of a and b from 5 to 1000. But the results already differ between “==” and “is”. Go to the next one.', 'Here is the last example if your mind is still not blown.', 'The official operation for “==” is equality while the operation for “is” is identity. You use “==” for comparing the values of two objects. “a == b” should be interpreted as “The value of a is whether equal to the value of b”. In all examples above, the value of a is always equal to the value of b (even for the empty list example). Therefore “a == b” is always true.', 'Before explaining identity, I need to first introduce id function. You can get the identity of an object with idfunction. This identity is unique and constant for this object throughout the time. You can think of this as an address for this object. If two objects have the same identity, their values must be also the same.', 'The operator “is” is to compare whether the identities of two objects are the same. “a is b” means “The identity of a is the same as the identity of b”.', 'Once you know the actual meanings of “==” and “is”, we can start going deep on those examples above.', 'First is the different results in the first and second examples. The reason for showing different results is that Python stores an array list of integers from -5 to 256 with a fixed identity for each integer. When you assign a variable of an integer within this range, Python will assign the identity of this variable as the one for the integer inside the array list. As a result, for the first example, since the identities of a and b are both obtained from the array list, their identities are of course the same and therefore a is bis True.', 'But once the value of this variable falls outside this range, since Python inside does not have an object with that value, therefore Python will create a new identity for this variable and assign the value to this variable. As said before, the identity is unique for each creation, therefore even the values of two variables are the same, their identities are never equal. That’s why a is bin the second example is False', '(Extra: if you open two consoles, you will get the same identity if the value is still within the range. But of course, this is not the case if the value falls outside the range.)', 'Once you understand the difference between the first and second examples, it is easy to understand the result for the third example. Because Python does not store the “empty list” object, so Python creates one new object and assign the value “empty list”. The result will be the same no matter the two lists are empty or with identical elements.', 'Finally, we move on to the last example. The only difference between the second and the last example is that there is one more line of code a = b.However this line of code changes the destiny of the variable a. The below result tells you why.', 'As you can see, after a = b, the identity of a changes to the identity of b. a = bassigns the identity of bto a. So both aand b have the same identity, and thus the value of a now is the same as the value of b, which is 2000.', 'The last example tells you an important message that you may accidentally change the value of an object without notice, especially when the object is a list.', 'From the above example, because both a and bhave the same identity, their values must be the same. And thus after appending a new element to a, the value of bwill be also impacted. To prevent this situation, if you want to copy the value from one object to another object without referring to the same identity, the one for all method is to use deepcopyin the module copy (Link to Python document). For list, you can also perform by b = a[:] .', 'Using [:]for copying elements to a new variable', 'I hope now you can understand the difference between the two and you will not be a beginner again for asking this question.']"
08/2020,HTTP 3 is Out and About!,Evolution of HTTP from HTTP 1 to HTTP 3,2.5K,10,https://towardsdatascience.com/@anuradhawick,https://towardsdatascience.com/http-3-is-out-and-about-7c903f9aab9e?source=collection_archive---------6-----------------------,4,4,"['HTTP 3 is Out and About!', 'What is HTTP?', 'HTTP 2', 'HTTP 3 the New Member']",17,"['HTTP stands for Hypertext Transfer Protocol. This is the set of rules used in the delivery of web pages from servers to your browser. HTTPS simple means the same protocols are used over an encryption layer for better privacy.', 'For those who are familiar with the OSI (Open Systems Interconnections) Specification of the Internetworking/Communications, HTTP is an application layer protocol. Which means it has nothing to do with the underlying hardware structures or media. This makes it feasible to improve and upgrade the HTTP specification with increasing bandwidth and etc. You’ll see how that happens soon!', '“HTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it). Most of the time the user-agent is a Web browser, but it can be anything, for example a robot that crawls the Web to populate and maintain a search engine index.”Quoted from Mozilla', 'Client in this definition is the user agent. This could be a browser, program or anything that makes a call to a server using the HTTP protocol. The web server does the delivery or the serving of the requested content. The proxies coordinate the communication between the web servers and user agents.', 'The functionality of the proxies as follows;', 'In HTTP 1 or (1.1) simply the above tasks take place peacefully. Yet in 2009 a newer protocol HTTP 2 began to raise.', 'HTTP 2 was put forward to address a few limitations that HTTP 1 met with the advancements of networking technologies and bandwidth.', 'HTTP 2 gives a faster and secure (HTTP encryption is enforced in HTTP 2) web experience. You can have a look at this demo by the akamai cloud provider.', 'This is a protocol that is yet to come (sort of). However, I noted that YouTube already uses HTTP 3 in the chrome browser (Not sure about others though!). How do I know? Because I use a plugin to see the HTTP protocol (HTTP/2 and SPDY indicator).', 'The new specification also promises a faster, better, and secure web experience. But how?', 'HTTP 3 intends to simplify the TCP handshake (it is a 3-way handshake, hi, hi again, ok you got my hi, alike.). The bottom line, the connections are established faster using the QUIC protocol, which is something new as well.', 'The enhanced connection establishment promises better connection migration that occurs between wifi connections and mobile networks and etc. Note that this isn’t the connection migration you see between routers in the same network. Much complicated and sophisticated.', 'Lastly, QUIC also tries to upgrade the TCP slow start thus enabling high speeds to be achieved faster. QUIC tries to increase the congestion window exponential for this. This will be an important upgrade for gameplay in cases like Stadia and video streaming in UHD and above. Read more here.', 'This might likely be the case of YouTube being super fast and plays HD from start-up without buffering. But don’t quote me on that. The perfect specification might come out close to the end of this year.', 'I hope you enjoyed reading my article on HTTP 3. Though I didn’t go deep into the tech terms and the nitty-gritty details, now you know what might our future internet look like. For an in-depth comparison have a look at the following article.', 'Few more articles you might like,', 'Cheers!']"
08/2020,Deep Reinforcement Learning for Automated Stock Trading,Trade multiple stocks using DRL | ICAIF 2020,870,16,https://towardsdatascience.com/@ai4finance,https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02?source=collection_archive---------7-----------------------,15,5,"['Deep Reinforcement Learning for Automated Stock Trading', 'Overview', 'Part 1. Why do you want to use Deep Reinforcement Learning (DRL) for stock trading?', 'Part 2: What is Reinforcement Learning? What is Deep Reinforcement Learning? What are some of the related works to use Reinforcement Learning for stock trading?', 'Part 3: How to use DRL to trade stocks?']",53,"['Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details.', 'This blog is based on our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, presented at ICAIF 2020: ACM International Conference on AI in Finance.', 'Our codes are available on Github.', 'Our paper is available on SSRN.', 'If you want to cite our paper, the reference format is as follows:', 'Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA.', 'A most recent DRL library for Automated Trading-FinRL can be found here:', 'FinRL for Quantitative Finance: Tutorial for Single Stock Trading', 'FinRL for Quantitative Finance: Tutorial for Multiple Stock Trading', 'FinRL for Quantitative Finance: Tutorial for Portfolio Allocation', 'One can hardly overestimate the crucial role stock trading strategies play in investment.', 'Profitable automated stock trading strategy is vital to investment companies and hedge funds. It is applied to optimize capital allocation and maximize investment performance, such as expected return. Return maximization can be based on the estimates of potential return and risk. However, it is challenging to design a profitable strategy in a complex and dynamic stock market.', 'Every player wants a winning strategy. Needless to say, a profitable strategy in such a complex and dynamic stock market is not easy to design.', 'Yet, we are to reveal a deep reinforcement learning scheme that automatically learns a stock trading strategy by maximizing investment return.', 'Our Solution: Ensemble Deep Reinforcement Learning Trading StrategyThis strategy includes three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG).It combines the best features of the three algorithms, thereby robustly adjusting to different market conditions.', 'The performance of the trading agent with different reinforcement learning algorithms is evaluated using Sharpe ratio and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy.', 'Existing works are not satisfactory. Deep Reinforcement Learning approach has many advantages.', 'Reinforcement Learning is one of three approaches of machine learning techniques, and it trains an agent to interact with the environment by sequentially receiving states and rewards from the environment and taking actions to reach better rewards.', 'Deep Reinforcement Learning approximates the Q value with a neural network. Using a neural network as a function approximator would allow reinforcement learning to be applied to large data.', 'Bellman Equation is the guiding principle to design reinforcement learning algorithms.', 'Markov Decision Process (MDP) is used to model the environment.', 'Recent applications of deep reinforcement learning in financial markets consider discrete or continuous state and action spaces, and employ one of these learning approaches: critic-only approach, actor-only approach, or and actor-critic approach.', '1. Critic-only approach: the critic-only learning approach, which is the most common, solves a discrete action space problem using, for example, Q-learning, Deep Q-learning (DQN) and its improvements, and trains an agent on a single stock or asset. The idea of the critic-only approach is to use a Q-value function to learn the optimal action-selection policy that maximizes the expected future reward given the current state. Instead of calculating a state-action value table, DQN minimizes the mean squared error between the target Q-values, and uses a neural network to perform function approximation. The major limitation of the critic-only approach is that it only works with discrete and finite state and action spaces, which is not practical for a large portfolio of stocks, since the prices are of course continuous.', '2. Actor-only approach: The idea here is that the agent directly learns the optimal policy itself. Instead of having a neural network to learn the Q-value, the neural network learns the policy. The policy is a probability distribution that is essentially a strategy for a given state, namely the likelihood to take an allowed action. The actor-only approach can handle the continuous action space environments.', '3. Actor-Critic approach: The actor-critic approach has been recently applied in finance. The idea is to simultaneously update the actor network that represents the policy, and the critic network that represents the value function. The critic estimates the value function, while the actor updates the policy probability distribution guided by the critic with policy gradients. Over time, the actor learns to take better actions and the critic gets better at evaluating those actions. The actor-critic approach has proven to be able to learn and adapt to large and complex environments, and has been used to play popular video games, such as Doom. Thus, the actor-critic approach fits well in trading with a large stock portfolio.', 'We track and select the Dow Jones 30 stocks (at 2016/01/01) and use historical daily data from 01/01/2009 to 05/08/2020 to train the agent and test the performance. The dataset is downloaded from Compustat database accessed through Wharton Research Data Services (WRDS).', 'The whole dataset is split in the following figure. Data from 01/01/2009 to 12/31/2014 is used for training, and the data from 10/01/2015 to 12/31/2015 is used for validation and tuning of parameters. Finally, we test our agent’s performance on trading data, which is the unseen out-of-sample data from 01/01/2016 to 05/08/2020. To better exploit the trading data, we continue training our agent while in the trading stage, since this will help the agent to better adapt to the market dynamics.', '• State 𝒔 = [𝒑, 𝒉, 𝑏]: a vector that includes stock prices 𝒑 ∈ R+^D, the stock shares 𝒉 ∈ Z+^D, and the remaining balance 𝑏 ∈ R+, where 𝐷 denotes the number of stocks and Z+ denotes non-negative integers.', '• Action 𝒂: a vector of actions over 𝐷 stocks. The allowed actions on each stock include selling, buying, or holding, which result in decreasing, increasing, and no change of the stock shares 𝒉, respectively.', '• Reward 𝑟(𝑠,𝑎,𝑠′):the direct reward of taking action 𝑎 at state 𝑠 and arriving at the new state 𝑠′.', '• Policy 𝜋 (𝑠): the trading strategy at state 𝑠, which is the probability distribution of actions at state 𝑠.', '• Q-value 𝑄𝜋 (𝑠, 𝑎): the expected reward of taking action 𝑎 at state 𝑠 following policy 𝜋 .', 'The state transition of our stock trading process is shown in the following figure. At each state, one of three possible actions is taken on stock 𝑑 (𝑑 = 1, …, 𝐷) in the portfolio.', 'At time 𝑡 an action is taken and the stock prices update at 𝑡+1, accordingly the portfolio values may change from “portfolio value 0” to “portfolio value 1”, “portfolio value 2”, or “portfolio value 3”, respectively, as illustrated in Figure 2. Note that the portfolio value is 𝒑𝑻 𝒉 + 𝑏.', 'We define our reward function as the change of the portfolio value when action 𝑎 is taken at state 𝑠 and arriving at new state 𝑠 + 1.', 'The goal is to design a trading strategy that maximizes the change of the portfolio value 𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1) in the dynamic environment, and we employ the deep reinforcement learning method to solve this problem.', 'State Space: We use a 181-dimensional vector (30 stocks * 6 + 1) consists of seven parts of information to represent the state space of multiple stocks trading environment', 'Action Space:', 'A2C is a typical actor-critic algorithm which we use as a component in the ensemble method. A2C is introduced to improve the policy gradient updates. A2C utilizes an advantage function to reduce the variance of the policy gradient. Instead of only estimates the value function, the critic network estimates the advantage function. Thus, the evaluation of an action not only depends on how good the action is, but also considers how much better it can be. So that it reduces the high variance of the policy networks and makes the model more robust.', 'A2C uses copies of the same agent working in parallel to update gradients with different data samples. Each agent works independently to interact with the same environment. After all of the parallel agents finish calculating their gradients, A2C uses a coordinator to pass the average gradients over all the agents to a global network. So that the global network can update the actor and the critic network. The presence of a global network increases the diversity of training data. The synchronized gradient update is more cost-effective, faster and works better with large batch sizes. A2C is a great model for stock trading because of its stability.', 'DDPG is an actor-critic based algorithm which we use as a component in the ensemble strategy to maximize the investment return. DDPG combines the frameworks of both Q-learning and policy gradient, and uses neural networks as function approximators. In contrast with DQN that learns indirectly through Q-values tables and suffers the curse of dimensionality problem, DDPG learns directly from the observations through policy gradient. It is proposed to deterministically map states to actions to better fit the continuous action space environment.', 'We explore and use PPO as a component in the ensemble method. PPO is introduced to control the policy gradient update and ensure that the new policy will not be too different from the older one. PPO tries to simplify the objective of Trust Region Policy Optimization (TRPO) by introducing a clipping term to the objective function.', 'The objective function of PPO takes the minimum of the clipped and normal objective. PPO discourages large policy change move outside of the clipped interval. Therefore, PPO improves the stability of the policy networks training by restricting the policy update at each training step. We select PPO for stock trading because it is stable, fast, and simpler to implement and tune.', 'Our purpose is to create a highly robust trading strategy. So we use an ensemble method to automatically select the best performing agent among PPO, A2C, and DDPG to trade based on the Sharpe ratio. The ensemble process is described as follows:', 'Step 1. We use a growing window of 𝑛 months to retrain our three agents concurrently. In this paper, we retrain our three agents at every three months.', 'Step 2. We validate all three agents by using a 3-month validation rolling window followed by training to pick the best performing agent which has the highest Sharpe ratio. We also adjust risk-aversion by using turbulence index in our validation stage.', 'Step 3. After validation, we only use the best model with the highest Sharpe ratio to predict and trade for the next quarter.', 'We use Quantopian’s pyfolio to do the backtesting. The charts look pretty good, and it takes literally one line of code to implement it. You just need to convert everything into daily returns.', 'References:', 'A2C:Volodymyr Mnih, Adrià Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement learning. The 33rd International Conference on Machine Learning (02 2016). https://arxiv.org/abs/1602.01783', 'DDPG:Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. International Conference on Learning Representations (ICLR) 2016 (09 2015). https://arxiv.org/abs/1509.02971', 'PPO:John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. 2015. Trust region policy optimization. In The 31st International Conference on Machine Learning. https://arxiv.org/abs/1502.05477', 'John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv:1707.06347 (07 2017). https://arxiv.org/abs/1707.06347']"
08/2020,You’re living in 1985 if you don’t use Docker for your Data Science Projects,What is it Docker and How…,2.1K,23,https://towardsdatascience.com/@ahmasoh,https://towardsdatascience.com/youre-living-in-1985-if-you-don-t-use-docker-for-your-data-science-projects-858264db0082?source=collection_archive---------8-----------------------,4,5,"['You’re living in 1985 if you don’t use Docker for your Data Science Projects', 'Requirements', 'Containerise a Python service', 'Dockerfile', 'Analysis of a Dockerfile']",22,"['One of the hardest problems that new programmers face is understanding the concept of an ‘environment’. An environment is what you could say, the system that you code within. In principal it sounds easy, but later on in your career you begin to understand just how difficult it is to maintain.', 'The reason being is that libraries and IDE’s and even the Python Code itself goes through updates and version changes, then sometimes, you’ll update one library, and a separate piece of code will fail, so you’ll need to go back and fix it.', 'Moreover, if we have multiple projects being developed at the same time, there can be dependency conflicts, which is when things really get ugly as code fails directly because of another piece of code.', 'Also, say you want to share a project to a team mate working on a different OS, or even ship your project that you’ve built on your Mac to a production server on a different OS, would you have to reconfigure your code? Yes, you probably will have to.', 'So to mitigate any of these issues, containers were proposed as a method to separate projects and the environments that they exist within. A container is basically a place where an environment can run, separate to everything else on the system. Once you define what’s in your container, it becomes so much easier to recreate the environment, and even share the project with teammates.', 'To get started, we need to install a few things to get set up:', 'Let’s imagine we’re creating a Flask service called server.py and let’s say the contents of the file are as follows:', 'Now as I said above, we need to keep a record of the dependencies for our code so for this, we can create a requirements.txt file that can contain the following requirement:', 'So our package has the following structure:', 'The structure is pretty logical (source kept is kept in a separate directory). To execute our Python program, all is left to do is to install a Python interpreter and run it.', 'Now to run the program, we could run it locally but suppose we have 15 projects we’re working through — it makes sense to run it in a container to avoid any conflicts with any other projects.', 'Let’s move onto containerisation.', 'To run Python code, we pack the container as a Docker image and then run a container based on it. So as follows:', 'A Dockerfile is a file that contains instructions for assembling a Docker image (saved as myimage):', 'A Dockerfile is compiled line by line so the builder generates an image layer and stacks it upon previous images.', 'We can also observe in the output of the build command the Dockerfile instructions being executed as steps.', 'Then, we can see that the image is in the local image store:', 'During development, we may need to rebuild the image for our Python service multiple times and we want this to take as little time as possible.', 'Note: Docker and virtualenv are quite similar but different. Virtualenv only allows you to switch between Python Dependencies but you’re stuck with your host OS. However with Docker, you can swap out the entire OS — install and run Python on any OS (think Ubuntu, Debian, Alpine, even Windows Server Core). Therefore if you work in a team and want to future proof your technology, use Docker. If you don’t care about it — venv is fine, but remember it’s not future proof. Please reference this if you still want more information.', 'There you have it! We’ve shown how to containerise a Python service. Hopefully, this process will make it a lot easier and gives your project a longer shelf life as it’ll be less likely to come down with code-bugs as dependencies change.', 'Thanks for reading, and please let me know if you have any questions!', 'Keep up to date with my latest articles here!']"
08/2020,"Farewell RNNs, Welcome TCNs",How Temporal Convolutional Networks are moving in favor of Sequence…,1.93K,7,https://towardsdatascience.com/@BryanJr,https://towardsdatascience.com/farewell-rnns-welcome-tcns-dd76674707c8?source=collection_archive---------9-----------------------,16,8,"['Farewell RNNs, Welcome TCNs', 'Overview', '1. Background', '2. Noteworthy Data Preprocessing Practices for FTS', '3. Temporal Convolutional Network', '4. Knowledge-Driven Stock Trend Prediction and Explanation via TCN', '5. Conclusion', 'Citations and References']",96,"['Disclaimer: this article assumes that readers possess preliminary knowledge behind the model intuition and architecture of LSTM neural networks.', 'Financial Time Series (FTS) modelling is a practice with a long history which first revolutionised algorithmic trading in the early 1970s. The analysis of FTS was divided into two categories: fundamental analysis and technical analysis. Both these practices were put into question by the Efficient Market Hypothesis (EMH). The EMH, highly disputed since its initial publication in 1970, hypothesizes that stock prices are ultimately unpredictable. This has not constrained research attempting to model FTS through the use of linear, non-linear and ML-based models, as mentioned hereafter.', 'Due to the nonstationary, nonlinear, high-noise characteristics of financial time series, traditional statistical models have difficulty predicting them with high precision. Hence, increased attempts in recent years are being made to apply deep learning to stock market forecasts, though far from perfection. To list a mere few:', '2013', 'Lin et al. proposed a method to predict stocks using a support vector machine to establish a two-part feature selection and prediction model and proved that the method has better generalization than conventional methods.', '2014', 'Wanjawa et al. proposed an artificial neural network using a feed-forward multilayer perceptron with error backpropagation to predict stock prices. The results show that the model can predict a typical stock market.', '2017', '\u200e\u200e\u200e\u200eEnter LSTM — a surge in studies concerning application of LSTM neural networks to the time series data.', 'Zhao et al., a time-weighted function was added to an LSTM neural network, and the results surpassed those of other models.', '2018', 'Zhang et al. later combined convolutional neural network (CNN) and recurrent neural network (RNN) to propose a new architecture, the deep and wide area neural network (DWNN). The results show that the DWNN model can reduce the predicted mean square error by 30% compared to the general RNN model.', 'Ha et al., CNN was used to develop a quantitative stock selection strategy to determine stock trends and then predict stock prices using LSTM to promote a hybrid neural network model for quantitative timing strategies to increase profits.', 'Jiang et al. used an LSTM neural network and RNN to construct models and found that LSTM could be better applied to stock forecasting.', '2019', 'Jin et al. added investor sentiment tendency in model analysis and introduced empirical modal decomposition (EMD) combined with LSTM to obtain more accurate stock forecasts. The LSTM model based on the attention mechanism is common in speech and image recognition but is rarely used in finance.', 'Radford et al. Precursor to the now hot-stock, GPT-3, GPT-2’s goal is to design a multitask learner, and it utilizes a combination of pretraining and supervised finetuning to achieve more flexible forms of transfer. Therefore it has 1542M parameters, much bigger than other comparative models.', 'Shumin et al. A knowledge-driven approach using Temporal Convolutional Network (KDTCN) for stock trend prediction and explanation. They first extracted structured events from financial news and utilize knowledge graphs to obtain event embeddings. Then, combine event embeddings and price values together to forecast stock trend. Experiments demonstrate that this can (i) react to abrupt changes much faster and outperform state-of-the-art methods on stock datasets. (This will be the focal point of this article.)', '2020', 'Jiayu et al. and Thomas et al. proposed hybrid attention networks to predict stock trend based on the sequence of recent news. LSTMs with attention mechanisms outperforms conventional LSTMS as it prevents long-term dependencies due to its unique storage unit structure.', 'Hongyan et al. proposed an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts the shallow layer’s important information and transfers to deep layers.', 'The timeline above is merely meant to provide a glimpse into the historical context of FTS in deep learning but not downplay on the significant work contributed to by the rest of the sequence model academia during similar time periods.', 'However, a word of caution is worth mentioning here. It might be the case that academic publications in the field of FTS forecasting are often misleading. Many FTS forecasting papers tend to inflate their performance for recognition and overfit their models due to the heavy use of simulators. Many of the performances claimed in these papers are difficult to replicate as they fail to generalise for future changes in the particular FTS being forecast.', 'Financial time series data — especially stock prices, constantly fluctuate with seasonality, noise and autocorrection. Traditional methods of forecasting use moving averages and differencing to reduce the noise for forecasting. However, FTS is conventionally non-stationary and exhibits the overlapping of useful signals and noise, which makes traditional denoising ineffective.', 'Wavelet analysis has led to remarkable achievements in areas such as image and signal processing. With its ability to compensate for the shortcomings of Fourier analysis, it has gradually been introduced in the economic and financial fields. The wavelet transform has unique advantages in solving traditional time series analysis problems as it can decompose and reconstruct financial time series data from a different time and frequency domain scales.', 'Wavelet transform essentially uses multi-scale characteristics to denoise the dataset, effectively separating the useful signal from the noise. In a paper by Jiayu Qiu, Bin Wang, Changjun Zhou, they used the coif3 wavelet function with three decomposition layers, and evaluate the effect of the wavelet transform by its signal-to-noise ratio (SNR) and root mean square error (RMSE). The higher the SNR and the smaller the RMSE, the better the denoising effect of the wavelet transform:', 'In FTS, the choice of which piece of data to use as the validation set is not trivial. Indeed, there exist a myriad of ways of doing this which must be carefully considered for stock indices of varying volatility.', 'The fixed origin method is the most naive and common method used. Given a certain split size, the start of the data is the training set and the end is the validation set. However, this is a particularly rudimentary method to choose, especially for a high-growth stock like Amazon. The reason why this is the case is that Amazon’s stock price starts off with low volatility and, as the stock grows, experiences increasingly volatile behaviour.', 'We would therefore be training a model on low volatility dynamics and expect it to deal with unseen high volatility dynamics for its predictions. This has indeed shown itself to be difficult and come at a cost in performance for these types of stocks. Therefore our benchmark for validation loss and performance may be misleading if we only consider this. However, for stocks like Intel that are more constant in their volatility (pre-COVID crisis), this method is reasonable.', 'The rolling origin recalibration method is slightly less vulnerable than fixed origin as it allows the validation loss to be computed by taking the average of various different splits of the data to avoid running into unrepresentative issues with high volatility timeframes.', 'Finally, the rolling window method is usually one of the most useful methods as it is particularly used for FTS algorithms being run for long timeframes. Indeed, this model outputs the average validation error of multiple rolling windows of data. This means the final values we get are more representative of recent model performance, as we are less biased by strong or poor performance in the distant past.', 'A study done by Thomas Hollis, Antoine Viscardi, Seung Eun Yi indicated that both rolling window (RW) and rolling origin recalibration (ROR) describe very slightly better performances (58% and 60%) than that of the simple fixed origin method. This suggests that for volatile stocks like Amazon, using these shuffling methods would be inevitable.', 'Temporal Convolutional Networks, or simply TCN, is a variation of Convolutional Neural Networks for sequence modelling tasks, by combining aspects of RNN and CNN architectures. Preliminary empirical evaluations of TCNs have shown that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets while demonstrating longer effective memory.', 'The distinguishing characteristics of TCNs are:', '3.1.1 Causal Convolutions', 'As mentioned above, the TCN is based upon two principles: the fact that the network produces an output of the same length as the input, and the fact that there can be no leakage from the future into the past. To accomplish the first point, the TCN uses a 1D fully-convolutional network (FCN) architecture, where each hidden layer is the same length as the input layer, and zero padding of length (kernel size − 1) is added to keep subsequent layers the same length as previous ones. To achieve the second point, the TCN uses causal convolutions, convolutions where output at time t is convolved only with elements from time t and earlier in the previous layer.', 'To put it simply: TCN = 1D FCN + causal convolutions.', '3.1.2 Dilated Convolutions', 'A simple causal convolution is only able to look back at a history with size linear in the depth of the network. This makes it challenging to apply the aforementioned causal convolution on sequence tasks, especially those requiring a longer history. The solution implemented by Bai, Kolter and Koltun (2020), was to employ dilated convolutions that enable an exponentially large receptive field. More formally, for a 1-D sequence input x ∈ Rⁿ and a filter f:{0,…,k−1}→R, the dilated convolution operation F on element s of the sequence is defined as:', 'where d is the dilation factor, k is the filter size, and s − d · i accounts for the direction of the past. Dilation is thus equivalent to introducing a fixed step between every two adjacent filter taps. When d = 1, a dilated convolution reduces to a regular convolution. Using larger dilation enables an output at the top level to represent a wider range of inputs, thus effectively expanding the receptive field of a ConvNet.', '3.1.3 Residual Connections', 'Residual blocks effectively allow layers to learn modifications to the identity mapping rather than the entire transformation, which has repeatedly been shown to benefit very deep networks.', 'Since a TCN’s receptive field depends on the network depth n as well as filter size k and dilation factor d, stabilization of deeper and larger TCNs becomes important.', 'Several advantages of using TCNs for sequence modelling:', 'Parallelism. Unlike in RNNs where the predictions for later timesteps must wait for their predecessors to complete, convolutions can be done in parallel since the same filter is used in each layer. Therefore, in both training and evaluation, a long input sequence can be processed as a whole in TCN, instead of sequentially as in RNN.', 'Flexible receptive field size. A TCN can change its receptive field size in multiple ways. For instance, stacking more dilated (causal) convolutional layers, using larger dilation factors, or increasing the filter size are all viable options. TCNs thus afford better control of the model’s memory size and are easy to adapt to different domains.', 'Stable gradients. Unlike recurrent architectures, TCN has a backpropagation path different from the temporal direction of the sequence. TCN thus avoids the problem of exploding/vanishing gradients, which is a major issue for RNNs (and which led to the development of LSTM and GRU).', 'Low memory requirement for training. Especially in the case of a long input sequence, LSTMs and GRUs can easily use up a lot of memory to store the partial results for their multiple cell gates. However, in a TCN the filters are shared across a layer, with the backpropagation path depending only on network depth. Therefore in practice, it was found that gated RNNs are likely to use up to a multiplicative factor more memory than TCNs.', 'Variable length inputs. Just like RNNs, which model inputs with variable lengths in a recurrent way, TCNs can also take in inputs of arbitrary lengths by sliding the 1D convolutional kernels. This means that TCNs can be adopted as drop-in replacements for RNNs for sequential data of arbitrary length.', 'Two notable disadvantages to using TCNs:', 'Data storage during evaluation. TCNs need to take in the raw sequence up to the effective history length, thus possibly requiring more memory during evaluation.', 'Potential parameter change for a transfer of domain. Different domains can have different requirements on the amount of history the model needs in order to predict. Therefore, when transferring a model from a domain where only little memory is needed (i.e., small k and d) to a domain where much longer memory is required (i.e., much larger k and d), TCN may perform poorly for not having a sufficiently large receptive field.', 'Executive summary:', 'The results strongly suggest that the generic TCN architecture with minimal tuning outperforms canonical recurrent architectures across a broad variety of sequence modelling tasks that are commonly used to benchmark the performance of recurrent architectures themselves.', 'Most of the deep neural networks in stock trend prediction have two common drawbacks: (i) current methods are not sensitive enough to abrupt changes of stock trend, and (ii) forecasting results are not interpretable for humans. To address these two problems, Deng et al., 2019 proposed a novel Knowledge-Driven Temporal Convolutional Network (KDTCN) for stock trend prediction and explanation, by incorporating background knowledge, news events and price data into deep prediction models, to tackle the problem of stock trend prediction and explanation with abrupt changes.', 'To address the problem of prediction with abrupt changes, events from financial news are extracted and structurized into event tuples, e.g., “Britain exiting from EU ” is represented as (Britain, exiting from, EU ). Then entities and relations in event tuples are linked to KGs, such as Freebase and Wikidata. Secondly, structured knowledge, textual news, as well as price values are vectorized respectively, and then concatenated together. Finally, feed these embeddings into a TCN-based model.', 'Experiments demonstrate that KDTCN can (i) react to abrupt changes much faster and outperform state-of-the-art methods on stock datasets, as well as (ii) facilitate the explanation of prediction particularly with abrupt changes.', 'Furthermore, based on prediction results with abrupt changes, to address the problem of making explanations, the effect of events are visualized by presenting the linkage among events with the use of Knowledge Graphs (KG). By doing so, we can make explanations of (i) how knowledge-driven events influence the stock market fluctuation in different levels, and (ii) how knowledge helps to associate events with abrupt changes in stock trend prediction.', 'Note the section below merely summarizes into a broad overview of the paper Shumin et al., if you could refer to the paper if you seek further technical details.', 'The fundamental TCN model architecture mentioned here is derived from Section 3 above —a generic TCN architecture consisting of causal convolutions, residual connections and dilated convolutions.', 'The overview of KDTCN architecture is shown below:', 'Original model inputs are price values X, news corpus N , and knowledge graph G. The price values are normalized and mapped into the price vector, denoted by', 'where each vector pt represents a real-time price vector on a stock trading day t, and T is the time span.', 'As for news corpus, pieces of news are represented as event sets, ε; then, structured into event tuple e = (s, p, o), where p is the action/predicate, s is the actor/subject and o is the object on which the action is performed; then, each item in the event tuples is linked to KG, correspond to entities and relations in KG; lastly, event embeddings V are obtained by training both event tuples and KG triples. A more detailed of this process is documented in Shumin et al.', 'Finally, event embeddings, combined with price vectors are input into a TCN-based model.', 'Datasets:', 'Baselines:', 'Performance of KDTCN was benchmarked in three progressive aspects: (i) evaluation of basic TCN architecture, (ii) influence of different model inputs with TCN, and (iii) TCN-based model performance for abrupt changes.', 'Basic TCN Architecture:', 'Note that all experiments reported in this part are only input with price values.', 'TCN greatly outperforms baseline models on the stock trend prediction task. TCN achieves much better performance than either traditional ML models (ARIMA), or deep neural networks (such as LSTM and CNN), indicating that TCN has more obvious advantages in sequence modeling and classification problems.', 'Different Model Inputs with TCN:', 'As seen, WB-TCN and EB-TCN both get better performance than TCN, indicating textual information helps to improve forecasting.', 'KDTCN gets both the highest accuracy and F1 scores, and such a result demonstrates the validity of model input integration with structured knowledge, financial news, and price values.', 'Model Performance for Abrupt Changes:', 'It was observed that models with knowledge-driven event embedding input, such as KDEB-TCN and KDTCN, can greatly outperform numerical-data-based and textual-data-based models. These comparison results indicate that knowledge-driven models have advantages in reacting to abrupt changes in the stock market swiftly.', 'Additional note on how the degree of stock fluctuation is quantified below.', 'First, get time intervals of abrupt changes by figuring out the difference of stock fluctuation degree D(fluctuation) between two adjacent stock trading days', 'where x at time t denotes the stock price value on the stock trading day t. Then the difference of fluctuation degree C is defined by:', 'If |Ci | exceeds a certain threshold, it can be considered that the stock price abruptly changes at the ith day.', 'Explanations for why knowledge-driven events are common sources of abrupt changes to human without ML expertise are accomplished in two aspects: (i) visualizing effects of knowledge-driven events on prediction results with abrupt changes, and (ii) retrieving background facts of knowledge-driven events by linking the events to external KG.', 'Effect Visualization of Events:', 'The prediction result in the figure below is that trend of DJIA index will drop. Note that the bars of the same colour have the same event effect, the height of bars reflects the degree of effects, and the event popularity declines from left to right. Intuitively, events with higher popularity should have greater effects on stock trend prediction with abrupt changes, but not always.', 'Nearly all other events with negative effect are related to these two events, e.g., (British Pound, drops, nearly 5%) and (Northern Ireland, calls for poll on United Ireland).', 'Although there are also some events have positive effects of predicting stock trend to rise, and have high popularity, i.e., (Rich, Getting, Richer), the total effect is negative. Therefore, abrupt changes of the stock index fluctuation can be viewed as the combined result of effects and popularity of events.', 'Visualization of Event Tuples Linked to KG:', 'First, the event tuples with great effects or high popularity in stock trend movements were searched. Then, backtrack to the news texts containing these events. Finally, retrieve associated KG triples linked to event tuples by entity linking. In the above figure, each event tuple is marked in blue, and entities in it are linked to KG.', 'These listed event tuples, such as (Britain, exiting from, EU ), (United Kingdom, votes to leave, European Union), (British Pound, drops, nearly 5%), (J. K. Rowling, leads the charge for, Scottish independence), and (Northern Ireland, calls for poll on United Ireland), are not strongly relevant literally. However, with the linkage to KG, they can establish an association with each other, and strongly related to events of Brexit and EU Referendum. By incorporating explanations of event effects, it could be justified that knowledge-driven events are common sources of abrupt changes.', 'The preeminence enjoyed by recurrent networks in sequence modelling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. The recent academic research has indicated that with these elements, a simple convolutional architecture is more effective across diverse sequence modelling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, it was proposed in Bai, S., Kolter, J. and Koltun, V., 2020, that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modelling.', 'Furthermore, as seen in the application of TCNs in stock trend prediction above, by incorporating news event and knowledge graphs, TCNs could significantly outperform canonical RNNs.', '[1] Hollis, T., Viscardi, A. and Yi, S. (2020). “A Comparison Of Lstms And Attention Mechanisms For Forecasting Financial Time Series”.', '[2] Qiu J, Wang B, Zhou C. (2020). “Forecasting stock prices with long-short term memory neural network based on attention mechanism”.', '[3] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. (2020). “Neural Machine Translation By Jointly Learning To Align And Translate”.', '[4] Bai, S., Kolter, J. and Koltun, V., 2020. “An Empirical Evaluation Of Generic Convolutional And Recurrent Networks For Sequence Modeling”.', '[6] Deng, S., Zhang, N., Zhang, W., Chen, J., Pan, J. and Chen, H., 2019. “Knowledge-Driven Stock Trend Prediction and Explanation via Temporal Convolutional Network”.', '[5] Hao, H., Wang, Y., Xia, Y., Zhao, J. and Shen, F., 2020. “Temporal Convolutional Attention-Based Network For Sequence Modeling”.']"
09/2020,Who is the Least Biased News Source? Simplifying the News Bias Chart,I examined 102 news sites on the…,1.2K,30,https://towardsdatascience.com/@jjpryor,https://towardsdatascience.com/how-statistically-biased-is-our-news-f28f0fab3cb3?source=collection_archive---------0-----------------------,19,21,"['Who is the Least Biased News Source? Simplifying the News Bias Chart', 'Table of contents', 'Methods & sources', 'Legend', 'Interesting data', 'Which websites can be considered ‘Fake News’?', 'Which news sources should we be reading?', 'Who are the 10 most biased news sources?', 'Who are the 10 most neutral news sources?', 'Who are the 10 most liberally biased news sources?', 'Who are the 10 most conservatively biased news sources?', 'What news websites get the most traffic in the USA?', 'Which sites can be considered contributors to echo-chambers?', 'How many news sources use paywalls?', 'Graphical data and correlations', 'Website visits vs News media bias', 'Reliability vs Unique American visitors in July', 'Bias vs Reliability', 'Monthly visits per person vs Reliability', 'Commentary', 'Full list of news websites analyzed:']",112,"['“I challenge anybody to show me an example of bias in Fox News Channel.”', 'Challenge accepted.', 'Rupert Murdoch, the founder and current chairman of Fox Corp (owner of Fox News) first said the above quote in an article on the Salon website in 2001.', 'And even though it’s from almost 20 years ago, I can’t think of a more telling quote about the current environment of news in America.', 'Was there actually any iota of truth to that statement?', '“Fake news!”', '“Mainstream media!”', '“Hoax!”', 'All of these phrases seem to be shouted from the rooftops everywhere we look these days.', 'On TV, on the internet, social media — even in private conversations with family and friends.', 'This has weighed on me (and probably you too) for a while now.', 'Not only have normally friendly conversations turned vitriolic, but it seems a candid discussion about actual facts is getting more and more impossible every day.', 'My curiosity finally got the best of me and I spent a huge amount of time over the last couple of weeks collecting and analyzing related data.', 'I explored the subject of what truly can be considered fake news today.', 'I explore these questions and many more in the following analysis.', 'Enjoy!', 'I’ll start off by sharing how and where I got the information to create this analysis. I feel, now more than ever, that there’s already far too much bias in the information on the web.', 'As such, I tried my best to remain neutral in the following analysis.', 'If you spot any errors or corrections needed, please comment and I’ll take a look.', 'The initial data set I went to find was a statistically backed source of media bias and truthfulness.', 'This is a seemingly impossible task to accurately track.', 'Consider the thousands of news stories and opinion pieces that are published every single day.', 'Now multiply this by all of the blogs, opinion sites, and media organizations putting their own version of each story out into the web.', 'Then imagine that happens every single day, and multiply the numbers by decades.', 'It’s easy to see the insurmountability of such a project.', 'And yet — in steps Ad Fontes Media.', 'If you haven’t heard of them, they’re responsible for the Media Bias Chart:', 'The Media Bias Chart is a project that aims to evaluate as many major news sources in the U.S. as possible (within their budget limitations).', 'For every article they analyze, their panel of reviewers consists of 1 person leaning left, central, and right. They also have a meticulous methodology that needs to be followed.', 'You can read more about this project here. Currently, they are following and evaluating over 100 news organizations, while adding more when they can.', 'Unfortunately for my cookie history, part of my process involved visiting every single news site at least 5 times to evaluate their use of paywalls and revenue sources.', 'In the end, I settled on a list of 102 organizations that I was able to extract all of the same data from, for a proper comparison across the board.', 'I also wanted to analyze news media website traffic. Most of this information is kept strictly private, likely due to competition and other factors.', 'Fortunately for this article, I was able to use two different SEO (Search Engine Optimization) tools to examine information such as:', 'For this analysis, I used two different SEO tools, SEMrush and SimilarWeb.', 'These tools are known to not be 100% accurate due to how they calculate their figures.', 'That being said, they are a good estimate of website traffic, amongst other things.', 'More importantly, they are great to use as a benchmark, as their methodology is applied equally to any website you query.', 'Because of the above aspects, I used both tools to average out the results to present a more accurate viewpoint.', 'Now let’s get to the meat of the analysis — and the findings.', 'I need to explain a few things before showing the results.', 'Firstly, we need to look at how Ad Fontes Media measures bias and reliability of news sources.', 'This was one of the biggest questions that stuck out in my mind when I sat down to do this analysis.', 'The below list is any news source rated with a reliability factor of lower than 24 points (37.4%) according to Ad Fontes Media. News sources falling within these metrics can be considered to produce a significant amount of purposefully non-factual articles.', 'Be extremely skeptical when reading these news sources:', 'Amongst all the clutter of thousands of news articles being pumped out every day, how are we to know if it’s reliable, factual, and not biased?', 'This is an important question, and according to the following data, these are the sites we should trust the most.', 'To calculate this, I used both metrics of reliability and bias, measured by Ad Fontes Media. I did a basic multiplication of the two data sets and weighted them equally.', 'The aim was to rank the publications by the least amount of bias with the most amount of reliability in their reporting.', 'The following 15 news sources ranked the highest under this metric.', 'News organizations used to be a bastion of reliability. But somewhere in the past few decades, the idea that opinions could be substituted for facts became acceptable.', 'In my humble opinion, bias should be kept out of the news, as much as possible.', 'And from that viewpoint — here are the 10 worst offenders for having the most bias according to Ad Fontes Media.', '(Remember, the bias rankings are on a scale of -42 to +42. The more negative the bias ranking, the more liberal. The more positive it is, the more conservative it’s considered.)', 'Avoid these sources if you value neutrality:', 'This list shows who is consistently rated to be the most unbiased site of the 102 websites in this analysis.', 'News media to consider the most neutral sources:', 'The following websites all received over 100 million total visits from people in the USA in July, 2020.', 'Measuring the echo-chamber effect isn’t inherently intuitive. To come up with this ranking, I matched the two metrics of reliability and site visits per month.', 'The line of thought is that ‘fake news’ websites are more likely to write strong opinions without the benefit of truth behind them.', 'If these websites also had a strong correlation of repeat visitors — I would say that is an echo chamber of strong believers in ideology, rather than facts.', 'Also remember, the visits per month is an average. Some people can be visiting far more — and likely to believe what they are reading.', 'Based on the data and in my opinion, the following websites are harming the news industry and our political discourse.', '21 of the 102 sources analyzed had some form of paywall from my discovery. This wasn’t a perfect method, so it’s possible I missed one or two news sources.', 'I went on every single news site and clicked/skimmed through 5 different articles. If a paywall prompt opened up at any point in that journey, I marked it down.', 'In the below list, anything over 62.5% is considered to be pretty reliable by Ad Fontes Media. I bolded the ones below that can be considered reliable sources.', 'Paywalled news sources:', 'I am not a data scientist although I have studied the subject as part of my two university degrees in the past.', 'To make sure I was on the right track, I ran this article by a friend of mine that is a professional quantitative analyst.', 'Based on his advice, I have left out any conclusions to the following data — I merely present my opinion.', 'Some correlations were shown to be statistically significant, while others showed very little numerical relationships.', 'I’ll leave you to be the judge of which ones may or may not be.', 'I was curious to see if the popularity of a news source affected its bias.', 'I don’t believe there is a strong relationship in the data, but it is interesting to note the most popular sites are all relatively unbiased — with the exception of Fox News, as denoted above.', 'It’s important to know if our most popular sources of news are reliable. I thought this would be an interesting graph to visualize because of this.', 'Fortunately, most of the most popular sources can be considered reliable, with Weather.com having the most visitors as well as being the most reliable.', 'On the other side of things, we can see two of the more unreliable but popular websites are outliers — Fox News and the Daily Mail.', 'On this chart, we can see measured bias vs measured reliability. The horizontal axis is divided by a line measuring reliability.', 'It’s nice to see a strong connection between highly reliable sites and their unbiasedness — a position I believe all proper news sites should strive for.', 'On the opposite side, it seems the more biased a website is — whether right or left — the more fake news they spew out into the world to absorb.', 'This chart also shows an interesting feature of the data set — 63% of all of the publications are left-leaning, even if a little bit.', 'Another attempt at trying to see evidence of an echo-chamber effect. Some websites such as the Palmer Report have a very high rate of repeated visits.', 'Unfortunately for neutrality, several of these are assessed to be very unreliable, if not extremist.', 'It also shows that most of the highly reliable news sources are not visited that frequently. The one exception to that is Weather.com, with 3.6 average visits per month per user.', 'A bit ironic since I can’t remember the last time my weather app accurately forecast the rain. (/s)', 'I spent an incredible amount of time on this article. By far the most I’ve ever done for a single piece.', 'The constant anger, arguments, and contempt we see in our everyday lives spurred me on to gather and analyze this dataset.', 'And yet, I find myself now with even more questions than I was able to answer in creating this article.', 'As you can see from some of the data above, there are many sites that are clearly spreading false information, opinion, and extremism.', 'This does not bring us together.', 'It leads to us doubting our neighbors, our friends, our parents, and other important people in our lives.', 'Eternal distrust.', 'You can’t believe what you hear.', 'Every man for himself.', 'It seems that many people these days, mistakenly in my opinion, search for sources based on what they already want to hear.', 'They look for articles to confirm their suspicions. Their thoughts and feelings.', 'Right or left, it doesn’t matter. If you search on Google for something to back up your feeling on a subject (regardless of truth) — you will find it.', 'There’s an article for everything now.', 'Opinions being added to the news cycle has corrupted the impartiality of it.', 'This is not how we come together as a world, as a nation.', 'We must be better than this.', 'It’s my belief that many of these websites, their owners, and their anchors are one of the largest absolute causes of anger in the world today.', 'Be better, people.', 'I’ll close off by stating my most nagging thought after conducting this extensive exercise — I couldn’t wait to clear my browser cookies fast enough.', 'Thank you for reading my analysis.', 'If you noticed any glaring errors please let me know in the comments section. I’ll try my best to respond to any other questions and comments as well.', 'J.J. Pryor', 'I also recently released a satirical adult children’s political satire book — for those with a bit of dark quirky humor, please feel free to check it out.', 'ABC’s for MAGA Kids — The Alphabet According to Trump', 'For anyone interested in the data from Ad Fontes Media, here are the basics I used from their available information.', 'Remember their metrics:', 'For more interesting deep dives, how to’s, guides, and the occasional humor piece, please follow my free newsletter.']"
09/2020,How to go from a Python newbie to a Google Certified TensorFlow Developer under two months,-,2.1K,8,https://towardsdatascience.com/@radvian,https://towardsdatascience.com/how-to-go-from-a-python-newbie-to-a-google-certified-tensorflow-developer-under-two-months-3d1535432284?source=collection_archive---------1-----------------------,11,10,"['How to go from a Python newbie to a Google Certified TensorFlow Developer under two months', 'My TensorFlow ‘Origin Story’', 'What is TensorFlow? Why should you learn it?', 'What is the TensorFlow Developer Certificate exam? How much does it cost?', 'My Learning Journey: The First Month', 'My Learning Journey: The Second Month', 'Alternative Materials', 'Taking The Exam', 'Exam Result', 'Conclusion…and then what’s next?']",43,"['I still remember the day I finalized my thesis submission to my university. I sighed in relief as my bachelor studies came to an end. However, boredom quickly overcame me. With nothing to do, and the world swallowed by a pandemic, I desperately seek for a new activity to fill my empty days.', 'In this post, I’m going to tell you how this pandemic boredom led me to becoming a Google Certified TensorFlow developer in under two months, despite having never coded in Python before. I’ve provided a list of linked study materials that I use to prepare for this exam.', 'As someone who enjoys learning, my curiosity has led me to read extensive news and articles about the pandemic when I stumbled upon an article about a group of researchers developing a new system which can distinguish pneumonia from COVID-19 in X-ray images.', 'The article mentioned that they use ‘artificial intelligence’ and ‘neural network’ to make the system. This immediately piqued my interest — how could they train a system which can differentiate X-rays scans? The AI doesn’t even have a medical degree to begin with, and yet it has more than 90% of accuracy! Thus begin my journey into the rabbit hole that is deep learning.', 'Long story short, a brief conversation with a friend opened my eyes to TensorFlow (and Keras). Another day of curious browsing led me to read a blog post by Daniel Bourke about how he got certified as a TensorFlow Developer.', 'I challenged myself to get certified too, but I was worried that I don’t have enough time as soon I was going to be employed and start my master’s degree classes at roughly the same time. Furthermore, I am blind to Python. Can I really accomplish this feat?', 'I studied applied mathematics for actuarial science for my bachelor’s degree, which means that I quite familiar with calculus, regression, time series, and statistics. However, my coding skills in Python were slim to none, as the only programming language I know is R. While I find that R is a very versatile language that compliments the needs of data-related jobs, unfortunately at this moment the language R is not supported for the TensorFlow Developer Certification exam.', 'Taking this certification would be a milestone in my journey as a self-proclaimed data and AI enthusiast. Okay, enough of my story. Let’s talk about TensorFlow.', 'TL;DR version: TensorFlow is a widely available software library for machine learning.', 'A slightly less TL;DR version: TensorFlow is a free and open source framework which enables users to develop end-to-end machine learning and deep learning projects, starting from pre-processing to model training and deployment. It is initially developed by the Google Brain team for internal use within Google, but now its usage has been widespread.', 'Now, why should you learn TensorFlow? Because it is capable of a lot of things, and it is more widespread than you think. Chances are, you are using services made using TensorFlow without knowing it.', 'Have you ever used Gmail’s Smartreply? It is AI-powered and suggests you three responses based on what’s written on your email. It is built using TensorFlow.', 'Your Twitter timeline’s sorting method? WPS Office’s OCR (image-to-text recognition)? VSCO’s preset suggestion for your photos? TensorFlow.', 'When this article was written, TensorFlow has been around for only 4 years old, and it has seen widespread usage in so many services and products that we use daily. While not explicitly written, there is a possibility that the researchers who develop an image recognition system to differentiate regular pneumonia from COVID-19 pneumonia use TensorFlow in their system.', 'In the future, as the field of deep learning and artificial intelligence improves, we may see more and more products, services, as well as scientific breakthroughs which are powered by TensorFlow to assist in their deep learning aspect.', 'Practitioners in these fields are benefited if they are familiar with this platform, and this line of thought is what made me interested in becoming a certified TensorFlow developer myself. Perhaps, you have similar thoughts prior to or during reading this article, or perhaps you have your own reasons too to study TensorFlow. Nevertheless, read on to the next part to know more about the exam.', 'The TensorFlow Developer Certificate exam is written and has to be completed in Python language. In the exam, we utilize Python’s TensorFlow library as well as its API. The exam costs $100 per trial. If you fail the first trial, you may pay $100 again and retake the exam after two weeks. Further details about the exam payments and regulations can be found in the handbook here.', 'The exam syllabus comprises of four main points: building and training neural network using TensorFlow, image classification, natural language processing, as well as time series. The exam has to be taken in the Pycharm IDE.', 'After reviewing the handbook, I begin to plan out my learning path, which starts from learning the Python language itself, then familiarizing myself with TensorFlow.', 'If you’re still with me, or if you skipped reading the article to get to this point, then let me refresh you briefly. A bored applied mathematics graduate with nothing to do and no prior experience in Python suddenly dreamed of becoming a TensorFlow developer under two months. Here’s a recap my journey to achieve that goal.', 'During the first month, I familiarize myself with the Python language. How do I do this quickly? The first thing I did is going to hackerrank and immediately practiced with a lot of Python problems. When I get to a problem that I can’t solve on my own, I try to look up solutions online on-the-go. If that too didn’t help, I viewed the solution and tried to understand the concepts I can grasp in this problem.', 'That’s all I do for two weeks, and by then, I am able to answer most questions, even the ones with higher difficulty without looking at any solutions.', 'What did I do for the remaining two weeks? I watch free YouTube Python tutorials. You heard it right.', 'Alright, disclaimer incoming. If you have the opportunity to take formal Python class in a more structured manner, by all means please do so. The three videos I list below are only my personal choice to accelerate my Python learning journey.', 'These videos are so underrated just because they are “free”, and you don’t get any certificates for completing them. Here are a few great choices:', 'While I would enroll in a ‘formal’ Python class in my next studies, these three YouTube videos suffice for now. Just make sure to take notes, write your own codes, as well as trying out different things while you watch along.', 'I spend the last month taking the DeepLearning.AI TensorFlow Developer Professional Certificate at Coursera. In this course, we are tutored by Laurence Moroney, Lead Artificial Intelligence Advocate at Google, and Andrew Ng, founder of deeplearning.ai.', 'There are four courses in the specialization covering the four key points of the exam syllabus mentioned earlier. Each course consists of four weeks of lessons, but I learned the lessons for one week in a day, as this has been on top of my priority list for that month.', 'After completing each course, I take a day off to rest my mind, and use that day to either toy around my practice codes, or to explore ideas related to the course in a leisure manner.', 'To recap, I take five days to complete each course. Four days to view the lesson materials, and the fifth day to rest and review. Thus, I am able to finish the whole courses in 20 days.', 'Each courses have its own coding projects, and I really explore the codes provided. I often find myself spending hours toying around the hyperparameters of the neural network (you’ll know what I’m talking about when you start learning it) in order to try and gain the best validation accuracy. By doing so, you’ll gain an ‘instinct’ on the trial and error processes of creating deep neural network models.', 'Sometimes, the lessons referenced an external source such as datasets, articles, and ungraded materials. These are not mandatory to pass the course, but in my curiosity I explore a lot of these external sources. As the lessons are mostly practical, often than not we are also given links to videos made by Andrew Ng in which he explains a more intuitive and theoretical approach of particular subjects.', 'You don’t necessarily have to follow my study path and my learning materials in order to succeed in this exam. There are other alternatives to the coursera course if you don’t want to spend $49 per month for the course, and I’ll list them here:', 'I take four days to review my lessons and reread the handbook after finishing all the courses. On the 25th day of the second month, I started the exam.', 'Alright, here comes the D-Day. Okay so here is the answer key of the first questio — just kidding. While I can’t go to the exam details for obvious reasons, here are a few points regarding the exam that I’ve compiled about preparing and taking the exam:', 'If you’ve studied well, and made sure that you’ve learned all things listed in the exam syllabus, you should pass the exam. I can tell you that the syllabus written in the handbook is not misleading, and you can really use the list provided there as a benchmark for your exam readiness.', 'Personally, I do the exam on my AMD notebook with no dedicated GPU, and yet I only need to use Google Colab once in a problem with large data set. Train a few practice models on your device and you’ll know for yourself if your device is capable enough. I’d be more concerned on the internet speed and stability as you need to upload these models to complete the exam.', 'After ending the exam, I immediately received an email saying that I have passed the exam. Within 2 weeks, my official digital certificate is sent to my email, and I can link it to my LinkedIn profile.', 'It is only valid for three years, so I will have to take another certification exam in 2023. I could only wonder on the advancement of TensorFlow and the field of deep learning by then, and hopefully my journey in taking that exam is smoother than this first one.', 'This is not the end, in fact this is just the beginning for me. Having this exam as my first milestone really supercharged me and is my door to the world of data science, which is weird — as usually deep learning is the cherry on top for aspiring data scientists.', 'I’m glad that I managed to finish this certificate exam as well as writing this article merely days before my actuarial job and my master degree started. Through these two months, I have a newfound interest in the world of data science and artificial intelligence. The possibilities that this field might bring to solve real world problems are seemingly endless.', 'I have to write a reminder that I believe my learning path is not the best, and there are still a lot of room for improvements. To those who are not time-constrained, perhaps taking it slower as well as making projects along the way would be a better learning path. As for now, even when I am a certified TensorFlow developer, I have yet to make a single project on my GitHub account. This is what I aim to focus on after this article is published — improving my skills furthermore by making real personal projects and putting them on my GitHub.', 'As a quickly growing field filled with innovations, discoveries, and breakthroughs, I’m sure that the world of artificial intelligence, data science, machine learning, and deep learning is a new frontier waiting to be explored. Are you excited to venture into this world? Because I am. And for me, it all begins from a boredom during the pandemic.']"
09/2020,How I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid…,Data Science Interview,6.4K,43,https://towardsdatascience.com/@emmading,https://towardsdatascience.com/how-i-got-4-data-science-offers-and-doubled-my-income-2-months-after-being-laid-off-b3b6d2de6938?source=collection_archive---------2-----------------------,14,19,"['How I Got 4 Data Science Offers and Doubled my Income 2 Months after being Laid Off', 'Table of Contents', 'Getting Laid off', 'Preparing for the Search', 'The Job Search Begins', 'The Interview: Overview', 'Before the Interview', 'Preparation for Specific Subjects', 'Product Sense', 'SQL', 'Statistics and Probability', 'Machine Learning', 'Presentation', 'Behavioral Question', 'The Secret to Getting 100% Onsite-to-Offer Rate', 'Negotiation', 'Takeaways', 'Overview', 'Update (10/1/2020)']",41,"['During this unprecedented time with the pandemic, many are finding their careers affected. This includes some of the most talented data scientists with which I have ever worked. Having shared my personal experience with some close friends to help them find a new job after being laid off, I thought it worth sharing publicly. After all, this touches more than me and my friends. Any data scientist who was laid off due to the pandemic or who is actively looking for a data science position can find something here to which they can relate, and which I hope will ultimately offer hope in your job search.', 'So if you’ve ever been stuck - in getting interviews, in interview preparation, in negotiation, anything - I’ve been there, and I want to help. You can reach out to me here if you think I might be able to make your journey easier in any way! Here’s my story. I hope you find some useful tips and encouragement within it.', 'In December of 2018, I was informed by my manager that I was to be laid off in January 2019. Three months before, the VP of Engineering of my then startup company had written a letter to our head of People Success. This letter explained why I was one of the top performers in the company and advocated for an increase in my salary. This helped me get a 33% increase in my salary. I was naturally feeling motivated and eager to crack the next milestone on an important project. The company’s future and my own looked bright. It was during this moment of success that I was told that I was impacted by the company-wise cost-cutting initiative. I was let go on January 15th.', 'To be forced to start looking for a new job was daunting, to say the least. After browsing the data science job openings on the market, I soon realized my knowledge gap. What I was doing at the B2B startup (a mix of entry-level data engineering and machine learning) was simply irrelevant to many of the job requirements out there, such as product sense, SQL, stats, and more. I knew the basics but was unsure how to fill the gap towards more advanced skills. However, even that issue seemed secondary to more pressing questions, such as how do I even get an interview? I had a mere 1.5 years of work experience with a startup, and I lacked any statistics or computer science-related degree. More questions soon followed. What if I cannot find a job before I lose my visa status? What if the economy takes a downturn before I can find a new job? Despite my fears, there was little choice. I had to find a new job.', 'In the face of what felt like an overwhelming task, I needed some information to decide my next steps. After doing some research, I realized that more than half of the data science positions on the market were product-driven positions (‘product analytics’), and the rest were either modeling or data engineering oriented positions. I also noted that positions other than product analytics tended to have higher requirements. For example, most modeling positions required a PhD degree, and engineering positions required a computer science background. Clearly, the requirements for different tracks varied widely, so it followed that preparation for each would differ as well.', 'With this knowledge in hand, I made an important decision: preparing for all tracks would be both overwhelming and most likely less effective. I would need to focus on one. I choose product analytics because, based on my background and experience, there was a higher chance that I could get interviews on this track. Of course, not everyone in data science has my exact background and experience, so below I have summarized the general requirements for three categories of data science positions at big companies. Understanding this basic breakdown saved me a lot of time, and I trust it will prove useful for others looking for a job in data science. I will add, however, that for small startups it’s possible that the interview will be less structured and require more of a mixture of all three.', 'Product Analytics (~70% on the market)', 'Modeling (~20% on the market)', 'Data Engineering (~10% on the market)', 'In light of my own experience, the rest of this post is strongly tailored towards those preparing for positions in product analytics. Come back later to check out my post on preparation for a data engineering position.', 'The very first thing I did once I knew I was going to be laid off was to apply widely and aggressively to other jobs. I used all the job boards I knew including GlassDoor, Indeed and LinkedIn. I also asked everyone I knew for referrals. However, since it was almost at the end of the year, I did not receive any responses until January 2019.', 'Asking for referrals proved to be much more effective than applying by myself. Out of about 50 raw applications, I only got 3 interviews, but out of 18 referrals, I got 7 interviews. Overall, it was becoming obvious that I was not considered a strong candidate in this market.', 'While the structure of interviews was different for each company, there was a general outline that most companies followed:', 'Around half of the companies (4/10) that I’ve interviewed with had a take-home assignment before or instead of a TPS. Take-home assignments consumed a lot of energy. Typically, an 8-hour take-home assignment caused me to need at least half a day to rest after submission. Because of this, I did my best to schedule the interview accordingly. There were no interviews the morning after my take-home assignment. Simply being aware of the basic structure can go a long way in making you feel more at ease and able to cope with the process of finding a new job.', 'Going into my interviews, every opportunity was critical to me. Although I was aware that some people learn by interviewing, becoming better after many interviews, and typically obtaining offers for the last few companies with which they interview, I did not feel I could take this approach. When I graduated in 2017, I only received 4 interviews out of 500 raw applications. I was not expecting to get many more in 2019. Thus, my plan was to be fully prepared for each interview I got. I would let no opportunity go to waste.', 'One benefit of being laid off was that I could study full time for the interview. Each day I structured what I studied, focusing on two or three things per day. No more. From previous interviews, I had learned that a deep understanding allows you to give more thorough answers during interviews. It especially helps to have a depth of knowledge in an interview situation when you tend to be more nervous and anxious than usual. That is not the time when you want to try faking things.', 'As I describe my own experience, I can’t help thinking of a common misconception I often hear: it’s not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. After all, this is the same way we were taught things in school. Actually, as I get to know more senior data scientists I continue to learn that this method is common, even for people with years of experience. What you will be interviewed on may not be related to what you were doing at all, but you can gain the knowledge you need in ways other than job experience.', 'Here are the basics of what you can expect. Typically, product and SQL questions were asked during a TPS. Onsite interviews included a few rounds of questions, including product sense, SQL, stats, modeling, behavior, and maybe a presentation. The next few subsections summarize the most useful resources (all freely available) I used when preparing for interviews. In general, GlassDoor was a good source to get a sense of company-specific problems. Once I saw those problems, I understood both what the company needed and where my gaps were in fulfilling those needs. I was then able to develop a plan to fill those gaps.', 'The following six subsections are how I prepared for the specific content that comes up in interviews for the product analytics track. In explaining my own preparation, I hope to make the path smoother for those who come after me.', 'Working as a data scientist at a startup, I was mainly responsible for developing and deploying machine learning models and writing spark jobs. Thus, I barely gained any product knowledge. When I saw some real interview questions on GlassDoor, such as “how to measure success?” or “how to validate the new feature by current users’ behaviors?”, I had utterly no idea how to approach such questions. At the time, they seemed far too abstract and open-ended.', 'To learn product sense I resorted to the basic read and summarize strategy, using the resources listed below. All this reading helped me build up my product knowledge. As a result, I came up with a structured way (my own ‘framework’) to answer any type of product questions. I then put my knowledge and framework to the test with that all essential to learning any skill: practice. I wrote out answers to questions involving product sense. I said my answers out loud (even recording myself with my phone), and used the recordings to finetune my answers. Soon I could not only fake it for an interview, I actually knew my stuff.', 'Resources:', 'The first time I took a SQL TPS I failed, and it was with a company in which I was very interested. Clearly, something needed to change. I needed to, once again, practice, and so I spent time grinding SQL questions. Eventually, I was able to complete in a day, questions that had previously taken me an entire week. Practice makes perfect!', 'Resources:', 'To prepare for these kinds of questions, I brushed up on elementary statistics and probability and did some coding exercises. While this may seem overwhelming (there is a lot of content for both topics), the interview questions for a product data scientist were never hard. The resources below are a great way to review.', 'Resources:', 'Without a CS degree, I went into the job search with limited machine knowledge. I had taken some courses during my previous job, and I reviewed my notes from these to prep for interviews. However, even though modeling questions are getting more and more frequent nowadays, the interview questions for a product data scientist mainly geared toward how to apply those models rather than the underlying math and theories. Still here are some helpful resources to bump up your machine learning skills before the interview time.', 'Resources:', 'Some companies required candidates to either present the take-home assignment or a project of which they are most proud. Still, other companies asked about the most impactful project during behavioral interviews. However, no matter what the form the key is to make your presentation interesting and challenging.', 'That sounds great, but how do you do that? My main recommendation is to think through all the details, such as high-level goals and success metrics to ETL to modeling implementation details, to deployment, monitoring, and improvement. The little things add up to make a great presentation rather than one big idea. Here are a few questions worth rethinking to help reach your ideal presentation:', 'When presenting a project, you want to engage the audience. To make my presentations interesting, I often share interesting findings and the biggest challenges of the project. But the best way to make sure you are engaging is practice. Practice and practice out loud. I practiced presenting to my family to ensure my grasp of the material and ease of communication. If you can engage the people you know, an interviewer, who is required to listen, doesn’t stand a chance.', 'While it is easy to get caught up in preparing for the technical interview questions, don’t forget that the behavioral questions are equally important. All companies I’ve interviewed with had at least 1 round of behavior interviews during the onsite portions. These questions typically fall into these three categories:', 'Behavioral questions are very important for data scientists. So be prepared! Understanding a company’s mission and core values helps answer questions in the first group. Questions like 2 and 3 can be answered by telling a story — 3 stories were enough to answer all behavioral questions. Make sure you’ve got a few good stories on hand when you walk in for an interview. Similar to product questions, I practiced a lot by saying it out loud, recording, and listening to then fine-tune my answers. Hearing a story is the best way to make sure it works.', 'The night before an onsite interview was typically a stressful, hectic night. I always tried to cram in more technical knowledge while simultaneously reviewing my statistics notes and thinking of my framework to answer a product question. Of course, as we all learned in school, none of that was incredibly useful. The results were largely determined due to the amount of preparation before not a single night of cramming. So preparation is important, but there are some rules you can follow the day of to make sure your interview is a success.', 'Using these rules, this was the feedback I got from onsite interviews:', 'After receiving verbal offers, the next step was to work with recruiters to finalize the numbers. There’s only one rule here that I stick with - ALWAYS negotiate. But how?', 'Haseeb Qureshi has a very helpful guide on negotiating a job offer (with scripts!) which I followed religiously during my offer negotiation phase. Every single rule was so true. I negotiated with all companies that gave me an offer. The average increase for offers was 15%, and the highest offer was, in total value, increased by 25%. Negotiating works, so don’t be afraid to try it!', 'After losing 11 pounds and lots of cries and screaming (job hunting is stressful and it is okay to admit that), I finally got 4 offers within 2 months of being laid off. 3 of those offers were from companies that I have never dreamed of joining: Twitter, Lyft, and Airbnb (where I ultimately joined) and another offer from a healthcare startup. By the end of two frenzied months, I had received a total of 10 interviews, 4 onsite interviews, and 4 job offers, giving me a 40% TPS-to-onsite rate and 100% onsite-to-offer rate.', 'I was so lucky that I got lots of support and help from family and friends after being laid off, which was critical to landing a job at my dream company. It was difficult. Ironically looking for a job is also a lot of work, but everything was worth it.', 'I wrote this blog because I know how overwhelmed I was. There is so much to prepare for interviews. I hope this post has made things clearer for other data specialists out there in need of work, and if you want more advice feel free to contact me here. I am grateful to now be working in a great job, and I would be happy to help you get there too!', 'Since I published this post three weeks ago, I got hundreds of questions on data science interviews. So I decided to make a series of videos to help you land your dream data science job. Check my YouTube channel if you are interested!']"
09/2020,10 Cool Python Project Ideas for Python Developers,A list of interesting ideas and projects you can…,2.2K,10,https://towardsdatascience.com/@harish_6956,https://towardsdatascience.com/10-cool-python-project-ideas-for-python-developers-7953047e203?source=collection_archive---------3-----------------------,10,16,"['10 Cool Python Project Ideas for Python Developers', 'Choosing a Project Platform', 'Python Project Ideas for Python Developers', '1. Content Aggregator', '2. URL Shortener', '3. File Renaming Tool', '4. Directory Tree Generator', '5. MP3 Player', '6. Tic Tac Toe', '7. Quiz Application', '8. Calculator', '9. Build a Virtual Assistant', '10. Currency Converter', 'More Python Project Ideas to Build—', 'Conclusion', 'More Interesting Readings']",44,"['The joy of coding Python should be in seeing short, concise, readable classes that express a lot of action in a small amount of clear code — not in reams of trivial code that bores the reader to death.', '- Guido van Rossum', 'Python is one of the most used programming languages in the world, and that can be contributed to its general-purpose nature, which makes it a suitable candidate for various domains in the industry. With Python, you can develop programs not just for the web, but also for desktop and command-line. Python can be suitable for programmers of varying skill levels, right from the students to intermediate developers, to experts and professionals. But every programming language requires constant learning, and its the same case with Python.', 'If you truly want to get in-depth practical knowledge, there is no better way to get your hands dirty with Python than to undertake some cool projects that will not only keep you occupied in your free time but will also teach you how to get more out of Python.', 'According to Stackoverflow, Python is the most preferred language which means that the majority of developers use python.', 'Python can be a very versatile programming language in the right hands, and you can build many nifty programs with it to strengthen your command over the language. It is of utmost importance to have more exposure to practical knowledge than theoretical, especially when it comes to learning programming languages, such as Python.', 'But before we dive into the fun projects we have store for you, you must decide which platform you’re going to be working on. The platforms for the projects mentioned in this article can be categorized into three categories listed below:', 'Building a web application allows you and everyone else to access it from anywhere via the internet. For that, you would need to work on the front-end, the visual part, and the back-end of the application, where the business logic is implemented. Tools & Frameworks such as Django, Flask, and Web2Py are some of the many options you can use for this.', 'Desktop applications are also very commonly used and cater to a sizable chunk of users. When it comes to building desktop applications, Python makes it very easy for you to develop one using its PySimpleGUI package, which allows you to build all the necessary elements using Python. The framework PyQt5 also offers advanced GUI building elements but has a steeper learning curve.', 'Command-line programs work only in console windows and have no GUI at all. The user interaction takes place via commands and it is the oldest method of interacting with programs but don’t mistake its lack of GUI for its lack of usefulness. Hundreds of top enterprises depend on command-line programs to perform their daily business activities. To build command-line programs, you can use tools such as docopt, Python Fire, plac, and cliff.', '“Python has been an important part of Google since the beginning, and remains so as the system grows and evolves. Today dozens of Google engineers use Python, and we’re looking for more people with skills in this language.”', '- Peter Norvig, director of search quality at Google, Inc.', 'If you have made up your mind about the platform you’re going to use, let’s jump straight into the projects. Mentioned below are some fun projects addressed towards developers of all skill levels that will play a crucial role in taking their skills and confidence with Python to the next level.', 'The internet is a prime source of information for millions of people who are always looking for something online. For those looking for bulk information about a specific topic can save time using a content aggregator.', 'A content aggregator is a tool that gathers and provides information about a topic from a bulk of websites in one place. To make one, you can take the help of the requests library for handling the HTTP requests and BeautifulSoup for parsing and scraping the required information, along with a database to save the collected information.', 'Examples of Content Aggregators:', 'URLs are the primary source of navigation to any resource on the internet, be it a webpage or a file, and, sometimes, some of these URLs can be quite large with weird characters. URL shorteners play an important role in reducing the characters in these URLs and making them easier to remember and work with.', 'The idea behind making a URL shortener is to use the random and string modules for generating a new short URL from the entered long URL. Once you’ve done that, you would need to map the long URLs and short URLs and store them in a database to allow users to use them in the future.', 'Examples of URL Shortener —', 'Here is the link to join the course for FREE: —', 'If your job requires you to manage a large number of files frequently, then using a file renaming tool can save you a major chunk of your time. What it essentially does is that it renames hundreds of files using a defined initial identifier, which could be defined in the code or asked from the user.', 'To make this happen, you could use the libraries such as sys, shutil, and os in Python to rename the files instantaneously. To implement the option to add a custom initial identifier to the files, you can use the regex library to match the naming patterns of the files.', 'Examples of Bulk File Rename Tools —', 'A directory tree generator is a tool that you would use in conditions where you’d like to visualize all the directories in your system and identify the relationship between them. What a directory tree essentially indicates is which directory is the parent directory and which ones are its sub-directories. A tool like this would be helpful if you work with a lot of directories, and you want to analyze their positioning. To build this, you can use the os library to list the files and directories along with the docopt framework.', 'Examples of Directory Tree Generators —', 'If you love listening to music, you’d be surprised to know that you can build a music player with Python. You can build an mp3 player with the graphical interface with a basic set of controls for playback, and even display the integrated media information such as artist, media length, album name, and more.', 'You can also have the option to navigate to folders and search for mp3 files for your music player. To make working with media files in Python easier, you can use the simpleaudio, pymedia, and pygame libraries.', 'Examples of MP3 Players—', 'Tic Tac Toe is a classic game we’re sure each of you is familiar with. It’s a simple and fun game and requires only two players. The goal is to create an uninterrupted horizontal, vertical, or diagonal line of either three Xs or Os on a 3x3 grid, and whoever does it first is the winner of the game. A project like this can use Python’s pygame library, which comes with all the required graphics and the audio to get you started with building something like this.', 'Here are a few tutorials you can try:', 'More Fun Python projects for game dev:', 'Another popular and fun project you can build using Python is a quiz application. A popular example of this is Kahoot, which is famous for making learning a fun activity among the students. The application presents a series of questions with multiple options and asks the user to select an option and later on, the application reveals the correct options.', 'As the developer, you can also create the functionality to add any desired question with the answers to be used in the quiz. To make a quiz application, you would need to use a database to store all the questions, options, the correct answers, and the user scores.', 'Examples of Quiz Applications—', 'Read about the Best Python IDEs and Code Editors —', 'Of course, no one should miss the age-old idea of developing a calculator while learning a new programming language, even if it is just for fun. We’re sure all of you know what a calculator is, and if you have already given it a shot, you can try to enhance it with a better GUI that brings it closer to the modern versions that come with operating systems today. To make that happen, you can use the tkinter package to add GUI elements to your project.', 'Almost every smartphone nowadays comes with its own variant of a smart assistant that takes commands from you either via voice or by text and manages your calls, notes, books a cab, and much more. Some examples of this are Google Assistant, Alexa, Cortana, and Siri. If you’re wondering what goes into making something like this, you can use packages such as pyaudio, SpeechRecognition, gTTS, and Wikipedia. The goal here is to record the audio, convert the audio to text, process the command, and make the program act according to the command.', 'Here is the link to join the course for FREE —', 'As the name suggests, this project includes building a currency converter that allows you to input the desired value in the base currency and returns the converted value in the target currency. A good practice is to code the ability to get updated conversion rates from the internet for more accurate conversions. For this too, you can use the tkinter package to build the GUI.', 'Concluding our list of a handful of interesting ideas and projects you can build using Python, we can say that Python can be a very useful programming language to develop applications of all sorts and scales. Furthermore, the packages provided by Python offer immense value to the developers in simplifying the development process to a great extent. To wrap things up, we would like to say that the potential with Python is limitless, and the only thing that you might be missing could be the right idea.', 'If you have more suggestions or ideas, we’d love to hear about them.', 'I hope you’ve found this article useful! Below are some interesting readings hope you like them too-', 'About Author', 'Claire D. is a Content Crafter and Marketer at Digitalogy — a tech sourcing and custom matchmaking marketplace that connects people with pre-screened & top-notch developers and designers based on their specific needs across the globe. Connect with Digitalogy on Linkedin, Twitter, Instagram.']"
09/2020,"Object-oriented programming is dead. Wait, really?","Functional programming evangelists, you’re pointing…",1.8K,39,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/object-oriented-programming-is-dead-wait-really-db1f1f05cc44?source=collection_archive---------4-----------------------,9,5,"['Object-oriented programming is dead. Wait, really?', 'Is coupling functions with data that stupid?', 'The five big problems in object-oriented programming', 'The danger of the single paradigm', 'The big question: are we on the cusp of a new revolution?']",48,"['Programming in the 1960s had a big problem: computers weren’t that powerful yet, and somehow they needed to split the capacities between data structures and procedures.', 'This meant that if you had a large set of data, you couldn’t do that much with it without pushing a computer to its limits. On the other hand, if you needed to do a lot of things, you couldn’t use too much data or the computer would take forever.', 'Then Alan Kay came around in 1966 or 1967 and theorized that one could use encapsulated mini-computers that didn’t share their data, but rather communicated through messaging. This way, compute resources could be used much more economically.', 'Despite the ingenuity of the idea, it would take until 1981 until object-oriented programming hit the mainstream. Since then, however, it hasn’t stopped attracting new and seasoned software developers alike. The market for object-oriented programmers is as busy as ever.', 'But in recent years, the decade-old paradigm has received more and more criticism. Could it be that, four decades after object-oriented programming hit the masses, technology is outgrowing this paradigm?', 'The main idea behind object-oriented programming is as simple as can be: you try to break a program in parts that are as powerful as the whole. It follows that you couple pieces of data and those functions that only get used on the data in question.', 'Note that this only covers the notion of encapsulation, that is, data and functions that sit inside an object are invisible to the outside. One can only interact with the contents of an object through messages, typically called getter and setter functions.', 'What is not contained in the initial idea, but is considered essential to object-oriented programming today, are inheritance and polymorphism. Inheritance basically means that developers can define subclasses that have all the properties that their parent class has. This wasn’t introduced to object-oriented programming until 1976, a decade after its conception.', 'Polymorphism came to object-oriented programming another decade later. In basic terms, it means that a method or an object can serve as a template for others. In a sense it’s a generalization of inheritance, because not all properties of the original method or object need to be transmitted to the new entity; instead, you can choose to override properties.', 'What’s special about polymorphism is that even if two entities depend on each other in the source code, a called entity works more like a plugin. This makes life easier for developers because they don’t have to worry about dependencies at runtime.', 'It’s worth mentioning that inheritance and polymorphism aren’t exclusive to object-oriented programming. The real differentiator is encapsulating pieces of data and the methods that belong to them. In a time where compute resources were a lot scarcer than today, this was a genius idea.', 'Once object-oriented programming hit the masses, it transformed the way developers see code. What prevailed before the 1980s, procedural programming, was very machine-oriented. Developers needed to know quite a bit about how computers work to write good code.', 'By encapsulating data and methods, object-oriented programming made software development more human-centered. It matches human intuition that the method drive() belongs to the data group car, but not to the group teddybear.', 'When inheritance came around, that was intuitive, too. It makes perfect sense that Hyundai is a subgroup of car and shares the same properties, but PooTheBear does not.', 'This sounds like a powerful machinery. The problem, however, is that programmers who only know object-oriented code will force this way of thinking on everything they do. It’s like when people see nails everywhere because all they have is a hammer. As we will see below, when your toolbox contains only a hammer, that can lead to fatal problems.', 'Imagine you’re setting up a new program, and you’re thinking about designing a new class. Then you think back to a neat little class that you’ve created for another project, and you realize that it would be perfect for what you’re currently trying to do.', 'No problem! You can reuse the class from the old project for your new one.', 'Except for the fact that this class may actually be a subclass of another class, so now you need to include the parent class too. Then you realize that the parent class depends on other classes as well, and you end up including heaps of code.', 'The creator of Erlang, Joe Armstrong, famously proclaimed:', 'The problem with object-oriented languages is they’ve got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.', 'That pretty much says it all. It’s fine to reuse classes; in fact, it can be a major virtue of object-oriented programming.', 'But don’t take it to the extreme. Sometimes you’re better off writing a new class instead of including masses of dependencies for the sake of DRY (don’t repeat yourself).', 'Imagine you’ve successfully reused a class from another project for your new code. What happens if the base class changes?', 'It can corrupt your entire code. You might not even have touched it. But one day your project works like a charm, the next day it doesn’t because somebody changed a minor detail in the base class that ends up being crucial for your project.', 'The more you use inheritance, the more maintenance you potentially have to do. So even though reusing code seems very efficient in the short term, it can get costly in the long run.', 'Inheritance is this cute little thing where we can take properties of one class and transfer it to others. But what if you want to mix the properties of two different classes?', 'Well, you can’t do it. At least not in an elegant way. Consider for example the class Copier. (I borrowed this example, as well as some info about the problems presented here, from Charles Scalfani’s viral story Goodbye, Object Oriented Programming.) A copier scans the content of a document and prints it on an empty sheet. So should it be the subclass of Scanner, or of Printer?', 'There simply is no good answer. And even though this problem isn’t going to break your code, it comes up often enough to be frustrating.', 'In the diamond problem, the question was which class Copier is a subclass of. But I lied to you — there is a neat solution. Let Copier be the parent class, and Scanner and Printer be subclasses that only inherit a subset of the properties. Problem fixed!', 'That’s neat. But what if your Copier is only black-and-white, and your Printer can handle color, too? Isn’t Printer in that sense a generalization of Copier? What if Printer is connected to WiFi, but Copier is not?', 'The more properties you heap on a class, the more difficult it becomes to establish proper hierarchies. Really, you’re dealing with clusters of properties, where Copier shares some, but not all properties of Printer, and vice versa. And if you try to stick that into hierarchies, and you have a big complex project, this might lead you to a messy disaster.', 'You might say, alright, then we’ll just do object-oriented programming without hierarchies. Instead, we could use clusters of properties, and inherit, extend, or override properties as needed. Sure, that would be a bit messy, but it would be an accurate representation of the problem at hand.', 'There’s just one problem. The whole point of encapsulation is to keep pieces of data safe from one another and thus make computing more efficient. This doesn’t work without strict hierarchies.', 'Consider what happens if an object A overrides the hierarchy by interacting with another object B. It doesn’t matter what relationship A has with B, except that B is not the direct parent class. Then A must contain a private reference to B, because otherwise, it couldn’t interact.', 'But if A contains the information that the children of B also have, then that information can be modified in multiple places. Therefore, the information about B isn’t safe anymore, and encapsulation is broken.', 'Although many object-oriented programmers build programs with this kind of architecture, this isn’t object-oriented programming. It’s just a mess.', 'What these five problems have in common is that they implement inheritance where it’s not the best solution. Since inheritance wasn’t even included in the original form of object-oriented programming, I wouldn’t call these problems inherent to object orientation. They’re just examples of a dogma taken too far.', 'Not only object-oriented programming can be overdone, though. In pure functional programming, it’s extremely difficult to process user input or print messages on a screen. Object-oriented or procedural programming is much better for these purposes.', 'Still, there are developers who try to implement these things as pure functions and blow their code up to dozens of lines that nobody can understand. Using another paradigm, they could have easily reduced their code to a couple of readable lines.', 'Paradigms are a bit like religions. They’re good in moderation — arguably, Jesus, Mohamed and Buddha said some pretty cool stuff. But if you follow them to the last little detail, you might end up making the lives of yourself and of people around you quite miserable.', 'The same goes for programming paradigms. There’s no doubt that functional programming is gaining traction, whereas object-oriented programming has attracted some harsh criticism in the last few years.', 'It makes sense to get informed about new programming paradigms and use them when appropriate. If object-oriented programming is the hammer that makes developers see nails wherever they go, is that a reason to throw the hammer out the window? No. You add a screwdriver to your toolbox, and maybe a knife or a pair of scissors, and you choose your tool based on the problem at hand.', 'Functional and object-oriented programmers alike, stop treating your paradigms like a religion. They’re tools, and they all have their use somewhere. What you use should only depend on what problems you are solving.', 'At the end of the day, the — admittedly rather heated — debate of functional versus object-oriented programming boils down to this: could we be reaching the end of the era of object-oriented programming?', 'More and more problems are coming up where functional programming is often the more efficient option. Think data analysis, machine learning, and parallel programming. The more you get into those fields, the more you’ll love functional programming.', 'But if you look at the status quo, there are a dozen offers for object-oriented programmers to one offer for functional coders. That doesn’t mean you won’t get a job if you prefer the latter; functional developers are still pretty scarce these days.', 'The most likely scenario is that object-oriented programming will stay around for another decade or so. Sure, the avant-garde is functional, but that doesn’t mean you should ditch object-oriented yet. It’s still incredibly good to have in your repertoire.', 'So don’t throw object-oriented programming out of your toolbox in the next few years. But make sure that it’s not the only tool you have.']"
09/2020,Why You Shouldn’t Go to Casinos (3 Statistical Concepts),The house always wins. We all know this…,478,10,https://towardsdatascience.com/@tomimester,https://towardsdatascience.com/why-you-shouldnt-go-to-casinos-3-statistical-concepts-a3b600086463?source=collection_archive---------5-----------------------,7,5,"['Why You Shouldn’t Go to Casinos (3 Statistical Concepts)', 'Survivorship bias', 'Expected value', 'Hot hand fallacy', 'Conclusion']",44,"['You are at the casino. The roulette wheel is spinning and the ball is bouncing. Bounce, bounce, bounce, you smile: “it’s red!” And then it bounces one more. No, it’s black! You lose everything again and go home with empty pockets.', 'Well, I hope you won’t — because you don’t go to casinos, you don’t buy scratch tickets, you don’t play the lottery or any gambling game in general.', 'Why? Because these games are designed to make you lose money.', 'And in this article I’ll tell you why. (Check out the podcast or video version of it, too!)', 'The house always wins. We all know this phrase. But this is more than a phrase. This is a simple, mathematically proven fact. And you’ll only have to know three statistical concepts to see why the house always wins.', 'These three statistical concepts come up often in data science projects, too. So if you are wondering why I’m talking about gambling on a data science channel, rest assured, you’ll be able to take advantage of this knowledge in your data science career, too.', 'Anyways, three statistical concepts.', 'These are:', 'Let’s start with the first one.', 'Everyone loves good stories! A good story sticks.', 'And I bet that you, too, have a friend — or a friend of a friend — who won big on a sports bet, or came home with 10,000 bucks from Vegas or won the dream trip to Malta on a scratch ticket… So won something big.', 'The trick is that in gambling the good stories are always the ones that end with winning big. It makes sense. My grandma never talks about how she played the family numbers on the lottery last week and won nothing, again, for the 200th time. But she never forgets to mention when she won $6000 on it in 2003.', 'Why is that?', 'Because losing is boring. It’s everyday. It happens with everyone. Winning is exciting, it’s a fun-to-tell story, even after years.', 'The story of winning big survives the filter of boredom.', 'This is why this statistical concept is called survivorship bias. In this case, the story of winning is the thing that survives. And why is it a bias?', 'Because what happens here?', 'My brain hears a winning story. That’s one datapoint. Then it hears another one, then another one, then another one. Sometimes it hears losing stories, too… but by far not as many as there are in reality. So my poor brain will have a disproportionately big sample size of winning stories and a relatively small number of losing stories. And it unconsciously creates false statistics from the skewed data — and so it thinks that I have a much bigger chance to win than I have for real.', 'This is how my silly brain works. Well, okay, the bad news is that it’s not just my brain, it’s yours, too. In fact, it’s everyone’s brain: this is how humans are created. We instinctively believe that we have a bigger chance to win in games than we do. Because of survivorship bias.', 'Oh, and of course, almost all casinos and online betting companies amplify this effect as much as they can.', 'Anyways, if there wasn’t survivorship bias, we’d see our chances at gambling more rationally and probably none of us would ever go to the casinos. So if you hear a good winning story, you should always remember that’s not the full picture… and that on the full scale, the house always wins.', 'I keep saying this, by the way: the house always wins. But I haven’t yet explained the math behind it. So let’s continue with that and head over to the second statistical concept.', 'Here, I won’t go into the details of the expected value calculation itself. But check out this article to learn more: Expected Value Formula.', 'But, let’s get back and let me talk a little bit about expected value.', 'Expected value shows what result you would get on average if you made the very same bet infinite times.', 'I know, this sounds a bit tricky, so let me give you a very simple example to bring this home.', 'Flipping a coin.', 'Flipping a coin is usually a fair game. When you flip a coin, there’s 50%-50% for tails or heads. Let’s say that you bet and when it’s tails you double your money, if it’s heads you lose your money. If you do this over and over again several times, let’s say for 1,000 rounds, your wins and losses will balance each other out. Your average profit will be 0 dollars. That means that the expected value of this game is exactly $0.', 'In roulette, there’s a pretty similar bet to flipping a coin. That’s betting red versus black. But in roulette your winning chances are a tiny bit lower compared to flipping a coin. When you put $10 on black, your expected value is not 0. It’s minus $0.27 per round. Again, I won’t go into the math here, check out the article I mentioned. But the point is that in every round you play, you lose an average of 27 cents. It seems like a very small amount of money. But over 1,000 rounds, it adds up and your losses will be around $270.', 'I mean sure, expected value is a theoretical value, but it always shows itself in the long term. In other words: the more you play, the more you lose.', 'The point is: roulette is a game where the expected value is negative — because the probabilities in it are designed in a way that you’ll lose in the long term. And it’s not that big of a secret, that every single game in a casino is designed with a negative expected value.', 'And that’s why the house always wins.', 'So that was the second statistical concept, expected value.', 'Let’s talk about the third statistical concept:', 'This is another bias and it explains why people don’t get out of a game when they are in a winning series.', 'First off, you have to know that probability is tricky. It works in a way that’s really hard to interpret for the human brain. There are events that are extremely unlikely to happen. Like the chance that you’ll get 10 heads in a row when flipping a coin. The probability of that is less than 0.1%. Still, in a big enough sample size, let’s say when you toss the coin 100,000 times, it’ll inevitably happen, even multiple times.', 'And the same thing can happen to you. If you play 1,000 rounds of roulette for instance, it’s actually pretty likely that you’ll have lucky runs.', 'And when one’s in the middle of a lucky run, it’s easy to feel that she has a hot hand. So she raises the bar, plays with bigger bets — in the hope of getting the most out of these winning series. But the thing is that these winning series are nothing else but blind luck, and statistically speaking it happens to everyone every now or then.', 'In gambling, there’s no such thing as a hot hand. In the casino, just as fast as you win something, that’s how fast you can lose it.”. Again, you can’t get out of the law of statistics — and the more you play, the bigger the chance you’ll lose. Remember, the house always wins. So don’t fall for the hot hand fallacy. If you go to the casino and play (against common sense) and you win (against the odds), the best thing you can do is get out immediately and be happy for being lucky!', 'So why shouldn’t you go to casinos?', 'Because of 3 simple statistical concepts:', 'And don’t get me wrong, it’s your choice whether you gamble or not. I get it. It’s fun to play sometimes, it’s fun to get lucky and it’s fun to win.', 'I just wanted you to understand the math behind gambling — and to give you a more realistic picture of your chances and about why the house always wins.', 'Tomi Mester, data36.com']"
09/2020,Software developers might be obsolete by 2030,Opinion,1.3K,19,https://towardsdatascience.com/@rheamoutafis,https://towardsdatascience.com/software-developers-might-be-obsolete-by-2030-cb5ddbfec291?source=collection_archive---------6-----------------------,9,5,"['Software developers might be obsolete by 2030', 'From designing logic to designing minds', 'The status quo', 'How developers and corporations can stay ahead of the curve', 'The bottom line: geeks are becoming leaders']",51,"['In 1930, John Maynard Keynes predicted that we’d be having 15-hour workweeks by the end of the century. But by the time it was 2013, it was clear that the great economist had gotten something wrong.', 'Welcome to the era of bullshit jobs, as anthropologist David Graeber coined it. Since the 1930s, whole new industries have sprung up, which don’t necessarily add much value to our lives. Graeber would probably call most jobs in software development bullshit.', 'I don’t share Graeber’s opinion, especially when it comes to software. But he does touch an interesting point: as more and more processes are automated, most jobs are obsolete at some point. According to one estimate, 45 percent of all jobs could be automated using current technology. And over time, they probably will.', 'In software development, where things move pretty fast anyway, you can see this happen in real-time: as soon as software testing became a hot topic, automation tools started springing up. And this is just one of the many areas where the bullshit-parts — the parts that are iterative and time-consuming — of software has been automated away.', 'This begs the question, though, whether developers are making themselves obsolete by building automation tools. If more and more machines can write code for themselves, what do we need humans for?', 'Software developers are builders at heart. They build logical links, algorithms, programs, projects, and more. The point is: they build logical stuff.', 'With the rise of artificial intelligence, we’re seeing a paradigm shift though. Developers aren’t designing logical links anymore. Instead, they’re training models on the heuristic of these logical links.', 'Many developers have gone from building logic to building minds. To put it differently, more and more software developers are taking on the activities of data scientists.', 'If you’ve ever used an IDE, then you know how amazing assisted software development can be. Once you’ve gotten used to features like autocomplete or semantic code search, you don’t want to go without them again.', 'This is the first area of automation in software development. As machines understand what you’re trying to implement, they can help you through the process.', 'The second area is that of closed systems. Consider a social media app: it consists of many different pages that are linked among each other. However, it’s closed insofar as it isn’t designed to directly communicate with another service.', 'Although the technology for building such an app is getting more and more easy to use, we can’t speak of real automation yet. As of now, you need to be able to code if you want to create dynamic pages, use variables, apply security rules, or integrate databases.', 'The third and last area is that of integrated systems. The API of a bank, for example, is such a system since it is built to communicate with other services. At this point in time, however, it’s pretty impossible to automate ATM integrations, communications, world models, deep security, and complex troubleshooting issues.', 'When asked whether they’ll be replaced by a robot in the future, human workers often don’t think so. This applies to software development as well as many other areas.', 'Their reason is clear: qualities like creativity, empathy, collaboration, or critical thinking are not what computers are good at.', 'But often, that’s not what matters to get a job done. Even the most complex projects consist of many small parts that can be automated. DeepMind scientist Richard S. Sutton puts it like this:', '“Researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.”', 'Don’t get me wrong; human qualities are amazing. But we’ve been overestimating the importance of these problems when it comes to regular tasks. For a long time, for example, even researchers believed that machines would never be able to recognize a cat on a photo.', 'Nowadays, a single machine can categorize billions of photos at a time, and with a greater accuracy than a human. While a machine might be unable to marvel at the cuteness of a little cat, it’s excellent at working with undefined states. That’s what a photo of a kitten is through a machine’s eyes: an undefined state.', 'In addition to working with undefined states, there are two other things that computers can do more efficiently than humans: firstly, doing things at a scale. Secondly, working on novel manifolds.', 'We’ve all experienced how well computers work at a scale. For example, if you ask a computer to print(""I am so stupid"") two-hundred times, it will do so without complaining, and complete the task in a fraction of a second. Ask a human, and you’ll need to wait for hours to get the job done…', 'Manifolds are basically a fancy, or mathematical, way of referring to subsets of space that share particular properties. For example, if you take a piece of paper, that’s a two-dimensional manifold in three-dimensional space. If you scrunch up the piece of paper or fold it to a plane, it’s still a manifold.', 'It turns out that computers are really good at working in manifolds that humans find hard to visualize, for example because they extend into twenty dimensions or have lots of complicated kinks and edges. Since many everyday problems, like human language or computer code, can be expressed as a mathematical manifold, there is a lot of potential to deploy really efficient products in the future.', 'It might seem like developers are already using a lot of automations. But we’re only at the cusp of software automation. Automating integrated systems is almost impossible as of today. But other areas are already being automated.', 'For one, code reviews and debugging might soon be a thing of the past. Swiss company DeepCode is working on a tool for automatic bug identification. Google’s DeepMind can already recommend more elegant solutions for existing code. And Facebook’s Aroma can autocomplete small programs on its own.', 'What’s more, the Machine Inferred Code Similarity System, short MISIM, claims to be able to understand computer code in the same way that Alexa or Siri can understand human language. This is exciting because such a system could allow developers to automate common and time-consuming tasks, such as pushing code to the cloud or implementing compliance processes.', 'So far, all these automations work great on small projects, but are quite useless on more complex ones. For example, bug identification software is still returning many false positives, and autocompletion doesn’t work if the project has a very novel goal.', 'Since MISIM hasn’t been around for a long time, the jury is still out on this automation. However, you’ll need to keep in mind that these are the very beginnings, and these tools are expected to become a lot more powerful in the future.', 'Some early applications of these new automations could include tracking human activity. This isn’t meant like a spy-software, of course; rather, things like scheduling the hours of a worker or individualizing the lessons for a student could be optimized this way.', 'This, in itself, presents huge economic opportunities because students could learn the important stuff faster, and workers could serve during the hours in which they happen to be more productive.', 'If MISIM is as good as it promises, it could also be used to rewrite legacy code. For example, lots of banking and government software is written in COBOL, which is hardly taught today. Translating this code into a newer language would make it easier to maintain.', 'All these new applications are exciting. But above them looms a big Damocles’ sword: what if the competition makes use of those automations before you catch on? What if they make developers totally obsolete?', 'These are certainly two buzzwords in the world of automation. But they’re important nevertheless.', 'If you don’t test your software before releases, you might be compromising the user experience or encounter security issues down the road. And experience shows that automated testing covers cases that human testers didn’t even think of although they might have been crucial.', 'Continuous delivery is a practise that more and more teams are picking up, and for good reason. When you bundle lots and lots of features and only release an update, say, once every three months, you often spend the next few months fixing everything that got broken in the process. Not only is this way of working a big hindrance for speedy development, it also compromises the user experience.', 'There’s plenty of automation software for testing, and there’s version control (and many other frameworks) for continuous delivery. In most cases, it seems better to pay for these automations than to build them yourself. After all, your developers were hired to build new projects, not to automate boring tasks.', 'If you’re a manager, consider these purchases an investment. By doing so, you’re supporting your developers the best you can because you’re capitalizing on what they’re really good at.', 'Oftentimes, projects get created somewhere in upper management or close to the R&D-team, and then get passed down until they reach the development team — which then has the task of making this project idea real.', 'However, since not every project manager is also a seasoned software engineer, some parts of the project might be implementable by the development team, while others would be costly or pretty much impossible.', 'That approach may have been legitimate in the past. But as lots of the monotonous parts of software development — yes, those parts exist! — are being automated, developers are getting a chance to get more and more creative.', 'This is an excellent chance to move developers left, i.e., involving them in the planning stages of a project. Not only to they know what can be implemented and what can’t. With their creativity, they might add value in ways that are not imaginable a priori.', 'It’s been a brief five years since Microsoft’s Satya Nadella proclaimed that “every business will be a software business”. He was right.', 'Not only should developers shift left in management. Software should shift up in priorities.', 'If the current pandemic taught you anything, then it is that much of life, and value creation, happens online these days.', 'Software is king. Paradoxically, this becomes more apparent the more of it gets automated.', 'When I was at school, people who liked computers were deemed unsociable kids, nerds, geeks, unlikeable creatures, and zombie-like beings devoid of human feelings and passions. I really wish I were exaggerating.', 'The more time is progressing, however, the more people are seeing the other sides of developers. People who code are not regarded as nerds any more, but rather as smart folks who can build cool stuff.', 'Software is gaining more power the more it’s being automated. In that sense, your fear of losing your developer job due to automation is largely unfounded.', 'Sure, in a decade — in a few months even — you’ll probably be doing things that you can’t even imagine right now. But that doesn’t mean that your job will go away. Rather, it will be upgraded.', 'The fear that you really need to conquer is not that you might lose your job. What you need to shake off is the fear of the unknown.', 'Developers, you won’t be obsolete. You just won’t be nerds that much longer. Rather, you’ll become leaders.']"
09/2020,Python Is About to Get the Squeeze,Python has ruled data science and machine learning for the last…,1.7K,39,https://towardsdatascience.com/@anupamchugh,https://towardsdatascience.com/python-is-about-to-get-the-squeeze-5ab15fce55bd?source=collection_archive---------7-----------------------,6,5,"['Python Is About to Get the Squeeze', 'Python lacks type-safety and is super slow', 'Python has limitations in parallelism', 'Swift and Julia have Python’s interoperability and strong support in their favor', 'Conclusion']",26,"['Python was released in the 1990s as a general-purpose programming language.', 'Despite its clean syntax, the exposure Python got in its first decade wasn’t encouraging, and it didn’t really find inroads into the developer’s workspace. Perl was the first choice scripting language and Java had established itself as the go-to in the object-oriented programming arena. Of course, any language takes time to mature and only gets adopted when it’s better suited to a task than the existing tools.', 'For Python, that time first arrived during the early 2000s when people started realizing it has an easier learning curve than Perl and offers interoperability with other languages.', 'This realization led to a larger number of developers incorporating Python into their applications. The emergence of Django eventually led to the doom of Perl, and Python started gaining more momentum. Still, it wasn’t even close in popularity to Java and JavaScript, both of which were newer than Python.', 'Fast forward to the present, and Python has trumped Java to become the second-most-popular language according to the StackOverflow Developer Survey 2019.', 'It was also the fastest-growing programming language of the previous decade. Python’s rise in popularity has a lot to do with the emergence of big data in the 2010s as well as developments in machine learning and artificial intelligence. Businesses urgently required a language for quick development with low barriers of entry that could help manage large-scale data and scientific computing tasks. Python was well-suited to all these challenges.', 'Besides having those factors in its favor, Python was an interpreted language with dynamic typing support. More importantly, it had the backing of Google, who’d invested in Python for Tensorflow, which led to its emergence as the preferred language for data analysis, visualization, and machine learning.', 'Yet, despite the growing demand for machine learning and AI at the turn of this decade, Python won’t stay around for long. Like every programming language, it has its own set of weaknesses. Those weaknesses make it vulnerable to replacement by languages more suited to the common tasks businesses ask of them. Despite the presence of R, the emergence of newer languages such as Swift, Julia, and Rust actually poses a bigger threat to the current king of data science.', 'Rust is still trying to catch up with the machine learning community, and so I believe Swift and Julia are the languages that will dethrone Python and eventually rule data science. Let’s see why odds are against Python.', 'All good things come at a cost, and Python’s dynamically typed nature is no exception. It hampers developers, especially when running the code in production. Dynamic typing that makes it easy to write code quickly without defining types increases the risk of running into runtime issues, especially when the codebase size increases. Bugs that a compiler would easily figure out could go unidentified in Python, causing hindrances in production and ultimately slowing the development process in large-scale applications.', 'Worse, unlike compiled code, Python’s interpreter analyzes every line of code at execution time. This leads to an overhead that causes a significantly slower performance when compared to other languages.', 'Julia allows you to avoid some of these problems. Despite being dynamically typed, it has a just-in-time compiler. The JIT compiler either generates the machine code right before it’s executed or uses previously stored, cached compilations, which makes it as performant as statically typed languages. More importantly, it has a key feature known as multiple dispatch that is like function overloading of OOPs, albeit at runtime. The power of multiple dispatch lies in its ability to handle different argument types without the need to create separate function names or nested if statements. This helps in writing compact code, which is a big win in numeric computations since unlike Python, you can easily scale solutions to deal with all types of arguments.', 'Even better, Swift is a statically typed language and is highly optimized due to its LLVM (Low-Level Virtual Machine) compiler. The LLVM makes it possible to quickly compile into assembly code, making Swift super-efficient and almost as fast as C. Also, Swift boasts better memory safety and management tools known as Automatic Reference Counting. Unlike garbage collectors, ARC is a lot more deterministic as it reclaims memory whenever the reference count hits zero.', 'As compiled languages that offer type annotations, Swift and Julia are a lot faster and robust for development than Python. That alone might be enough to recommend them over the older language, but there are other factors to consider as well.', 'If slowness was not the most obvious drawback of Python, the language also has limitations with parallel computing.', 'In short, Python uses GIL (Global Interpreter Lock), which prevents multiple threads from executing at the same time in order to boost the performance of single threads. This process is a big hindrance because it means that developers cannot use multiple CPU cores for intensive computing.', 'I agree with the commonplace notion that we’re currently doing fine when leveraging Python’s interoperability with C/C++ libraries like Tensorflow and PyTorch. But a Python wrapper doesn’t solve all debugging issues. Ultimately, when inspecting the underlying low-level code, we’re falling back on C and C++. Essentially, we can’t leverage the strengths of Python at the low level, which puts it out of the picture.', 'This factor will soon play a decisive role in the fall of Python and rise of Julia and Swift. Julia is a language exclusively designed to address the shortcomings of Python. It primarily offers three features: coroutines (asynchronous tasks), multi-threading, and distributed computing — all of which only show the immense possibilities for concurrent and parallel programming. This structure makes Julia capable of performing scientific computations and solving big data problems at a far greater speed than Python.', 'On the other hand, Swift possesses all the tools required for developing mobile apps and has no problems with parallel computing.', 'Despite the disadvantages it has with respect to speed, multi-threading, and type-safety, Python still has a huge ecosystem that boasts an enormous set of libraries and packages. Understandably, Swift, and Julia are still infants in the field of machine learning and possess only a limited number of libraries. Yet, their interoperability with Python more than compensates for the lack of library support in Julia and Swift.', 'Julia not only lets programmers use Python code (and vice-versa), but also supports interoperability with C, R, Java, and almost every major programming language. This versatility would certainly give the language a good boost and increase its chances of a quick adoption among data scientists.', 'Swift, on the other hand, provides interoperability with Python with the PythonKit library. The biggest selling point for Swift (which has an Apple origin) is a strong support it’s been getting from Google, who fully backed Python decades ago. See how the tables have turned!', 'Also, the fact that the creator of Swift, Chris Lattner, is now working on Google’s AI brain team just shows that Swift is being seriously groomed as Python’s replacement in the machine learning field. The Tensorflow team investing in Swift with their S4TF project only further proves that the language isn’t merely regarded as a wrapper over Python. Instead Swift, thanks to its differential programming support and ability to work at a low level like C, will potentially be used to replace the underlying deep learning tools.', 'As the size of data continues to increase, Python’s Achilles heel will be soon found out. Gone are the days when ease of use and ability to write code quickly mattered. Speed and parallel computing are the names of the game and Python, which is a more general-purpose language, will no longer solve that problem. Inevitably, it will fade away while Julia and Swift seem like the candidates to take over the reins.', 'It’s important to note that Python as a programming language won’t disappear any time soon. Instead, it’ll only take a backseat in data science as the languages that are more specifically designed for deep learning will rule.', 'This article was originally published at Built In.']"
09/2020,How I’d Learn Data Science if I Could Start Over (2 years in),My guide to learning data science as…,3.2K,14,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/how-id-learn-data-science-if-i-could-start-over-2-years-in-b821d8a4876c?source=collection_archive---------8-----------------------,6,9,"['How I’d Learn Data Science if I Could Start Over (2 years in)', 'Table of Content', 'Preface', 'Introduction', '1. Mathematics and Statistics', '2. Programming Fundamentals', '3. Machine Learning Algorithms and Concepts', '4. Data Science Projects', 'Thanks for Reading!']",31,"['Coming from a non-technical background, I was more-or-less on my own.', 'When I first started my data science journey, I spent a chunk of time figuring out where to even begin, what I should learn first, and what resources I should use.', 'Over the past two years, I’ve learned several things that I wish someone could have told me, like whether to focus on programming or statistics first, what resources I should use to learn new skills, how I should approach learning new skills, etc…', 'Therefore, this article aims to provide some direction and insights for those who are learning data science.', 'My assumption is that as an aspiring Data Scientist, you’ll want to fully understand the concepts and details of various machine learning algorithms, data science concepts, and so forth.', 'Therefore, I recommend that you start with the building blocks before you even look at machine learning algorithms or data science applications. If you don’t have a basic understanding of calculus & integrals, linear algebra, and statistics, you’ll have a hard time understanding the mechanics behind various algorithms. Likewise, if you don’t have a basic understanding of Python, you’ll have a hard time implementing your knowledge in real-life applications.', 'Below is the order of topics that I recommend you go through:', 'Like anything else, you have to learn the fundamentals before you get to the fun stuff. TRUST ME, I would’ve had a much easier time if I started with learning mathematics and statistics before getting into any machine learning algorithms.', 'The three general topics that I recommend you review are calculus/integrals, statistics, and linear algebra (in no particular order).', 'Integrals are essential when it comes to probability distributions and hypothesis testing. While you don’t need to be an expert, it’s in your best interest to learn the fundamentals of integrals.', 'The first two articles are for those who want to get an idea of what integrals is all about or for those who simply need a refresher. If you know absolutely nothing about integrals, I recommend that you complete Khan Academy’s course. Lastly, I’ve provided a link to a number of practice problems to hone your skills.', 'If there was one topic that you should focus the majority of your time on, it’s statistics. After all, a data scientist is really a modern statistician and machine learning is a modern term for statistics.', 'If you have the time, I recommend that you go through Georgia Tech’s course called “Statistical Methods”, which covers probability fundamentals, random variables, probability distributions, hypothesis testing, and more.', 'If you don’t have the time to commit to the course above, I definitely recommend going through Khan Academy’s video on Statistics.', 'Linear Algebra is especially important if you want to get into deep learning, but even then, it’s good to know for other fundamental machine learning concepts, like principal component analysis and recommendation systems.', 'For Linear Algebra, I also recommend Khan Academy!', 'Just as having a fundamental understanding of math and stats is important, having a fundamental understanding of programming will make your life much easier, especially when it comes to implementation. Therefore, I recommend that you take the time to learn basic SQL and Python before diving into machine learning algorithms.', 'It’s entirely up to you whether you want to learn Python or SQL first, but if you were to ask me, I’d start with SQL. Why? It’s easier to learn and it’s useful to know if you work for a company that works with data, EVEN if you’re not a data scientist.', 'If you’re completely new to SQL, I recommend going through Mode’s SQL tutorials, as it’s very succinct and thorough. If you want to learn more advanced concepts, I would check out my list of resources where you can learn advanced SQL.', 'More importantly, below are a handful of resources that you can use to practice SQL.', 'I started with Python, and I’ll probably stick with Python for the rest of my life. It’s so far ahead in terms of open source contributions, and it’s straightforward to learn. Feel free to go with R if you want, but I have no opinions or advice to provide regarding R.', 'Personally, I found that learning Python by ‘doing’ is much more helpful. That being said, after going through several Python crash courses, I found this one to be the most comprehensive (and it’s free!).', 'Arguably the most important library to know in Python is Pandas, which is specifically meant for data manipulation and analysis.', 'Below are two resources that should ramp you up pretty quickly. The first link is a tutorial on how to use Pandas and the second link provides dozens and dozens of practice problems that you can use to solidify your learnings!', 'If you’ve gotten to this point, that means that you’ve built your foundation and you’re ready to learn the fun stuff. This part is split into two parts: machine learning algorithms and machine learning concepts.', 'The next step is to learn about the various machine learning algorithms, how they work, and when to use them. Below is a non-exhaustive list of the various machine learning algorithms and resources that you can use to learn about each one.', 'Similarly, there are several fundamental machine learning concepts that you’ll want to go over as well. Below is a (non-exhaustive) list of concepts that I strongly recommend you go through. A lot of interview questions are based on these topics!', 'By this point, you’ll not only have a strong foundation built, but also a solid understanding of machine learning fundamentals. Now it’s time to work on some personal side projects, the same way coders have their own side projects too.', 'If you want to look at some simple data science project examples, check out some of my projects below:', 'Here’s a list of data science projects that you can look at to generate ideas and come up with an interesting side project of your own.', 'I hope that this provides some direction and helps you in your data science careers. There’s no cookie-cutter way of approaching this, so feel free to take this with a grain of salt, but I truly believe that learning the fundamentals will pay dividends in the future.']"
09/2020,Develop and sell a Python API — from start to end tutorial,The article paints a picture for developing…,2K,7,https://towardsdatascience.com/@createdd,https://towardsdatascience.com/develop-and-sell-a-python-api-from-start-to-end-tutorial-9a038e433966?source=collection_archive---------9-----------------------,13,26,"['Develop and sell a Python API — from start to end tutorial', 'Table of Contents', 'About this article', 'Disclaimer', 'Stack used', '1. Create project formalities', '2. Create a solution for a problem', 'Install packages', 'Develop a solution to a problem', 'Download data', 'Create functionality', 'Build server to execute a function with REST', '3. Deploy to AWS', 'Set up zappa', 'Set up AWS', 'AWS credentials', 'AWS API Gateway', '4. Set up Rapidapi', 'Create API on Rapidapi', 'Test your own API', 'Create a private plan for testing', 'Test endpoint with Rapidapi', 'Create code to consume API', 'End result', 'Inspiration', 'About']",92,"['I recently read a blog post about setting up your own API and selling it.', 'I was quite inspired and wanted to test if it works. In just 5 days I was able to create an API from start to end. So I thought I share issues I came across, elaborate on concepts that the article was introducing, and provide a quick checklist to build something yourself. All of this by developing another API.', 'This article can be considered as a tutorial and comprehension of other articles (listed in my “Inspiration” section).', 'It paints a picture for developing a Python API from start to finish and provides help in more difficult areas like the setup with AWS and Rapidapi.', 'I thought it will be useful for other people trying to do the same. I had some issues on the way, so I thought I share my approach. It is also a great way to build side projects and maybe even make some money.', 'As the Table of content shows, it consists of 4 major parts, namely:', 'You will find all my code open sourced on Github:', 'You will find the end result here on Rapidapi:', 'If you found this article helpful let me know and/or buy the functionality on Rapidapi to show support.', 'I am not associated with any of the services I use in this article.', 'I do not consider myself an expert. If you have the feeling that I am missing important steps or neglected something, consider pointing it out in the comment section or get in touch with me. Also, always make sure to monitor your AWS costs to not pay for things you do not know about.', 'I am always happy for constructive input and how to improve.', 'We will use', 'It’s always the same but necessary. I do it along with these steps:', 'Now we have:', 'Then we need to create a solution to some problem. For the sake of demonstration, I will show how to convert an excel csv file into other formats. The basic functionality will be coded and tested in a Jupyter Notebook first.', 'Install jupyter notebook and jupytext:', 'set a hook in .git/hooks/pre-commit for tracking the notebook changes in git properly:', 'copy this in the file', 'afterwards for making the hook executable (on mac)', 'Add a .gitignore file and add the data folder (data/) to not upload the data to the hosting.', 'Download an example dataset (titanic dataset) and save it into a data folder:', 'Transform format', 'After developing the functionality in jupyter notebook we want to actually provide the functionality in a python app.', 'There are ways to use parts of the jupyter notebook, but for the sake of simplicity we create it again now.', 'Add an app.py file.', 'We want the user to upload an excel file and return the file converted into JSON for example.', ""Browsing through the internet we can see that there are already packages that work with flask and excel formats. So let's use them."", 'Start Flask server with', 'Tipp: Test your backend functionality with Postman. It is easy to set up and allows us to test the backend functionality quickly. Uploading an excel is done in the “form-data” tab:', 'Here you can see the uploaded titanic csv file and the returned column names of the dataset.', 'Now we simply write the function to transform the excel into json, like:', '(Check out my repository for the full code.)', 'Now we have the functionality to transform csv files into json for example.', 'After developing it locally we want to get it in the cloud.', 'After we created the app locally we need to start setting up the hosting on a real server. We will use zappa.', 'Zappa makes it super easy to build and deploy server-less, event-driven Python applications (including, but not limited to, WSGI web apps) on AWS Lambda + API Gateway. Think of it as “serverless” web hosting for your Python apps. That means infinite scaling, zero downtime, zero maintenance — and at a fraction of the cost of your current deployments!', 'As we are using a conda environment we need to specify it:', 'will give you /Users/XXX/opt/anaconda3/envs/XXXX/bin/python (for Mac)', 'remove the bin/python/ and export', 'Now we can do', 'to set up the config.', 'Just click through everything and you will have a zappa_settings.json like', 'Note that we are not yet ready to deploy. First, we need to get some AWS credentials.', 'First, you need te get an AWS access key id and access key', 'You might think it is as easy as:', 'To get the credentials you need to', 'But no. There is more to permissions in AWS!', 'I found this article from Peter Kazarinoff to be very helpful. He explains the next section in great detail. My following bullet point approach is a quick summary and I often quote his steps. Please check out his article for more details if you are stuck somewhere.', 'I break it down as simple as possible:', 'My Custom policy:', 'NOTE: Replace XXXXXXXXXXX in the inline policy by your AWS Account Number.', 'Your AWS Account Number can be found by clicking “Support → “Support Center. Your Account Number is listed in the Support Center on the upper left-hand side. The json above is what worked for me. But, I expect this set of security permissions may be too open. To increase security, you could slowly pare down the permissions and see if Zappa still deploys. The settings above are the ones that finally worked for me. You can dig through this discussion on GitHub if you want to learn more about specific AWS permissions needed to run Zappa: https://github.com/Miserlou/Zappa/issues/244.', 'Create a .aws/credentials folder in your root with', 'and paste your credentials from AWS', 'Same with the config', 'Note that code is for opening a folder with vscode, my editor of choice.', 'Save the AWS access key id and secret access key assigned to the user you created in the file ~/.aws/credentials. Note the .aws/ directory needs to be in your home directory and the credentials file has no file extension.', 'Now you can do deploy your API with', 'There shouldn’t be any errors anymore. However, if there are still some, you can debug with:', 'The most common errors are permission related (then check your permission policy) or about python libraries that are incompatible. Either way, zappa will provide good enough error messages for debugging.', 'If you update your code don’t forget to update the deployment as well with', 'To set up the API on a market we need to first restrict its usage with an API-key and then set it up on the market platform.', 'I found this article from Nagesh Bansal to be helpful. He explains the next section in great detail. My following bullet point approach is a quick summary and I often quote his steps. Please check out his article for more details if you are stuck somewhere.', 'Again, I break it down:', 'it looks like this', 'Now you have restricted access to your API.', '5. In the security tab you can check everything', '6. Then go to “endpoints” to add the routes from you Python app by clicking “create REST endpoint”', '7. Add an image for your API', '8. Set a pricing plan. Rapidapi published an own article on pricing options and strategies. As they conclude, it is up to your preferences and product on how to price it.', '9. I created a freemium pricing plan. The reason for that is that I want to give the chance to test it without cost, but add a price for using it regularly. Also, I want to create a plan for supporting my work. For example:', '10. Create some docs and a tutorial. This is pretty self-explaining. It is encouraged to do so as it is easier for people to use your API if it is documented properly.', '11. The last step is to make your API publicly available. But before you do that it is useful to test it for yourself.', 'Having set up everything, you of course should test it with the provided snippets. This step is not trivial and I had to contact the support to understand it. Now I am simplifying it here.', 'Create a private plan for yourself, by setting no limits.', 'The go to the “Users” section of your API, then to “Users on free plans”, select yourself and “invite” you to the private plan.', 'Now you are subscribed to your own private plan and can test the functionality with the provided snippets.', 'Upload an example excel file and click on “test endpoint”. Then you will get a 200 ok response.', 'To consume the API now you can simply copy the snippet that Rapidapi provides. For example with Python and the requests library:', 'The article “API as a product. How to sell your work when all you know is a back-end” by Artem provided a great idea, namely to', 'Make an API that solves a problem', 'Deploy it with a serverless architecture', 'Distribute through an API Marketplace', 'For the setting everything I found the articles from Nagesh Bansal very helpful:', 'Also this article from Peter Kazarinoff: https://pythonforundergradengineers.com/deploy-serverless-web-app-aws-lambda-zappa.html', 'I encourage you to have a look at those articles as well.', 'You can also read my article directly on Github (for better code formatting)', 'Daniel is an entrepreneur, software developer, and lawyer. He has worked at various IT companies, tax advisory, management consulting, and at the Austrian court.', 'His knowledge and interests currently revolve around programming machine learning applications and all its related aspects. To the core, he considers himself a problem solver of complex environments, which is reflected in his various projects.', 'Don’t hesitate to get in touch if you have ideas, projects, or problems.', 'Connect on:']"
10/2020,You Don’t Have to Use Docker Anymore,Opinion,2.9K,26,https://towardsdatascience.com/@martin.heinz,https://towardsdatascience.com/its-time-to-say-goodbye-to-docker-5cfec8eff833?source=collection_archive---------0-----------------------,10,7,"['You Don’t Have to Use Docker Anymore', 'Why Not Use Docker, Though?', 'Container Engines', 'Building Images', 'Container Runtime', 'Image Inspection and Distribution', 'Conclusion']",31,"['In the ancient times of containers (really more like 4 years ago) Docker was the only player in the container game. That’s not the case anymore though and Docker is not the only, but rather just another container engine on the landscape. Docker allows us to build, run, pull, push or inspect container images, but for each of these tasks there are other alternative tools, which might just do better job at it than Docker. So, let’s explore the landscape and (just maybe) uninstall and forget about Docker altogether…', 'If you’ve been a docker user for long time, I think it will take some persuading for you to even consider to switch to different tooling. So, here goes:', 'First of all, Docker is a monolithic tool. It’s a tool that tries to do everything, which generally is not the best approach. Most of the time it’s better to choose a specialized tool that does just one thing, but does it really well.', 'If you are scared of switching to different set of tools, because you would have to learn to work with different CLI, different API or in general different concepts, then that won’t be a problem. Choosing any of the tools shown in this article can be completely seamless as they all (including Docker) adhere to same specification under OCI, which is short for Open Container Initiative. This initiative contains specifications for container runtime, container distribution and container images, which covers all the features needed for working with containers.', 'Thanks to the OCI you can choose a set of tools that best suit your needs and at the same time you can still enjoy using the same APIs and same CLI commands as with Docker.', 'So, if you’re open to trying out new tools, then let’s compare the advantages, disadvantages and features of Docker and it’s competitors to see whether it actually makes sense to even consider ditching Docker for some new shiny tool.', 'When comparing Docker with any other tool we need to break it down by its components and first thing we should talk about are container engines. Container engine is a tool that provides user interface for working with images and containers so that you don’t have to mess with things like SECCOMP rules or SELinux policies. Its job is also to pull images from remote repositories and expand them to your disk. It also seemingly runs the containers, but in reality its job is to create container manifest and directory with image layers. It then passes them to container runtime like runc or crun (which we will talk about little later).', 'There are many container engines available, but the most prominent competitor to Docker is Podman, developed by Red Hat. Unlike Docker, Podman doesn’t need daemon to run and also doesn’t need root privileges which has been long-standing concern with Docker. Based on the name, Podman can not only run containers, but also pods. In case you are not familiar with concept of pods, then pod is the smallest compute unit for Kubernetes. It consists of one or more containers — the main one and so-called sidecars — that perform supporting tasks. This makes it easier for Podman users to later migrate their workloads to Kubernetes. So, as a simple demonstration, this is how you would run 2 containers in a single pod:', 'Finally, Podman provides the exact same CLI commands as Docker so you can just do alias docker=podman and pretend that nothing changed.', 'There are other container engines besides Docker and Podman, but I would consider all of them a dead-end tech or not a suitable option for local development and usage. But to have a complete picture, let’s at least mention what’s out there:', 'With container engines there was really only one alternative to Docker. When it comes to building images though, we have many more options to choose from.', 'First, let me introduce Buildah. Buildah is another tool developed by Red Hat and it plays very nicely with Podman. If you already installed Podman, you might have even noticed the podman build subcommand, which is really just Buildah in disguise, as its binary is included in Podman.', 'As for its features, it follows same route as Podman — it’s daemonless and rootless and produces OCI compliant images, so it’s guaranteed that your images will run the same way as the ones built with Docker. It’s also able to build images from Dockerfile or (more suitably named) Containerfile which is the same thing with different name. Apart from that, Buildah also provides finer control over image layers, allowing you to commit many changes into single layer. One unexpected but (in my opinion) nice difference from Docker is that images built by Buildah are user specific, so you will be able to list only images you built yourself.', 'Now, considering that Buildah is already included in Podman CLI, you might be asking why even use the separate buildah CLI? Well, the buildah CLI is superset of commands included in podman build, so you might not need to ever touch the buildah CLI, but by using it you might also discover some extra useful features (For specifics about differences between podman build and buildah see following article).', 'With that said, let’s see a little demonstration:', 'From the above script you can see that you can build images simply using buildah bud, where bud stands for build using Dockerfile, but you can also use more scripted approach using Buildahs from, run and copy, which are equivalent commands to the commands in Dockerfile ( FROM image, RUN ..., COPY ...).', 'Next up is Google’s Kaniko. Kaniko also builds container images from Dockerfile and similarly to Buildah, it also doesn’t need a daemon. The major difference from Buildah is that Kaniko is more focused on building images in Kubernetes.', ""Kaniko is meant to be run as an image, using gcr.io/kaniko-project/executor, which makes sense for Kubernetes, but isn't very convenient for local builds and kind of defeats the purpose as you would need to use Docker to run Kaniko image to build your images. That being said, if you are looking for tool for building images in your Kubernetes cluster (e.g. in CI/CD pipeline), then Kaniko might be a good option, considering that it's daemonless and (maybe) more secure."", 'From my personal experience though — I used both Kaniko and Buildah to build images in Kubernetes/OpenShift clusters and I think both will do the job just fine, but with Kaniko I’ve seen some random build crashes and fails when pushing images to registry.', ""The third contender here is buildkit, which could be also called the next-generation docker build. It's part of Moby project (as is Docker) and can be enabled with Docker as an experimental feature using DOCKER_BUILDKIT=1 docker build .... Well, but what exactly will this bring to you? It introduces bunch of improvements and cool features including parallel build steps, skipping unused stages, better incremental builds and rootless builds. On the other hand however, it still requires daemon to run ( buildkitd). So, if you don't want to get rid of Docker, but want some new features and nice improvements, then using buildkit might be the way to go."", 'As in the previous section, here we also have a few “honorable mentions” which fill some very specific use cases and wouldn’t be one of my top choices:', 'Last big piece of a puzzle is container runtime which is responsible for, well, running containers. Container runtime is one part of the whole container lifecycle/stack, which you will most likely not going to mess with, unless you have some very specific requirement for speed, security, etc. So, if you’re tired of me already, then you might want skip this one section. If on the other hand, you just want to know what are the options, then here goes:', 'runc is the most popular container runtime created based on OCI container runtime specification. It’s used by Docker (through containerd), Podman and CRI-O, so pretty much everything expect for LXD (which uses LXC). There’s not much else I can add. It’s default for (almost) everything, so even if you ditch Docker after reading this article, you will most likely still use runc.', 'One alternative to runc is similarly (and confusingly) named crun. This is tool developed by Red Hat and fully written in C (runc is written in Go). This makes it much faster and more memory efficient than runc. Considering that it’s also OCI compliant runtime, you should be able switch to it easily enough, if you want to check for yourself. Even though it’s not very popular right now, it will be in tech preview as an alternative OCI runtime as of the RHEL 8.3 release and considering that it’s Red Hat product we might eventually see as default for Podman or CRI-O.', 'Speaking of CRI-O. Earlier I said that CRI-O isn’t really a container engine, but rather container runtime. That’s because CRI-O doesn’t include features like pushing images, which is what you would expect from container engine. CRI-O as a runtime uses runc internally to run containers. This runtime is not the one you should try using on your machine, as it’s built to be used as runtime on Kubernetes nodes and you can see it described as “all the runtime Kubernetes needs and nothing more”. So, unless you are setting up Kubernetes cluster (or OpenShift cluster — CRI-O is default there already), then you probably should not touch this one.', 'Last one for this section is containerd, which is a CNCF graduating project. It’s a daemon that acts as an API facade for various container runtimes and OS. In the background it relies on runc and it’s the default runtime for Docker engine. It’s also used by Google Kubernetes Engine (GKE) and IBM Kubernetes Service (IKS). It’s an implementation of Kubernetes Container Runtime Interface (same as CRI-O), therefore it’s a good candidate for runtime of your Kubernetes cluster.', 'Last part of container stack is image inspection and distribution. This effectively replaces docker inspect and also (optionally) adds ability to copy/mirror images between remote registries.', 'The only tool which I will mention here that can do these tasks is Skopeo. It’s made by Red Hat and it’s an accompanying tool for Buildah, Podman and CRI-O. Apart from the basic skopeo inspect which we all know from Docker, Skopeo is also able to copy images using skopeo copy which allows you to mirror images between remote registries without first pulling them to local registry. This feature can also act as pull/push if you use local registry.', 'As a little bonus, I want to also mention Dive, which is a tool for inspecting, exploring and analyzing images. It’s little more user friendly, provides more readable output and can dig (or dive, I guess) a bit deeper into your image and analyze and measure its efficiency. It’s also suitable for use in CI pipelines, where it can measure whether your image is “efficient enough” or in other words — whether it wastes too much space or not.', 'This article wasn’t meant to persuade you to completely ditch Docker, rather its goal was to show you the whole landscape and all the options for building, running, managing and distributing containers and their images. Each of these tools including Docker, has its pros and cons and it’s important to evaluate what set of tools suits your workflow and use case the best and I hope this article will help you with that.', 'This article was originally posted at martinheinz.dev']"
10/2020,The Roadmap of Mathematics for Deep Learning,Understanding the inner workings of neural networks from…,3.4K,14,https://towardsdatascience.com/@tivadar.danka,https://towardsdatascience.com/the-roadmap-of-mathematics-for-deep-learning-357b3db8569b?source=collection_archive---------1-----------------------,19,7,"['The Roadmap of Mathematics for Deep Learning', 'The fundamentals', 'Calculus', 'Linear algebra', 'Multivariable calculus', 'Probability theory', 'Beyond the mathematical foundations']",131,"['Knowing the mathematics behind machine learning algorithms is a superpower. If you have ever built a model for a real-life problem, you probably experienced that being familiar with the details can go a long way if you want to move beyond baseline performance. This is especially true when you want to push the boundaries of state of the art.', 'However, most of this knowledge is hidden behind layers of advanced mathematics. Understanding methods like stochastic gradient descent might seem difficult since it is built on top of multivariable calculus and probability theory.', 'With proper foundations, though, most ideas can be seen as quite natural. If you are a beginner and don’t necessarily have formal education in higher mathematics, creating a curriculum for yourself is hard. In this post, my goal is to present a roadmap, taking you from absolute zero to a deep understanding of how neural networks work.', 'To keep things simple, the aim is not to cover everything. Instead, we will focus on getting our directions. This way, you will be able to study other topics without difficulties, if need be.', 'Instead of reading through in one sitting, I recommend using this article as a reference point through your studies. Go deep into a concept that is introduced, then check the roadmap and move on. I firmly believe that this is the best way to study: I will show you the road, but you must walk it.', 'Most of machine learning is built upon three pillars: linear algebra, calculus, and probability theory. Since the last one builds on the first two, we should start with them. Calculus and linear algebra can be studied independently, as is usually the case in a standard curriculum.', 'Calculus is the study of differentiation and integration of functions. Essentially, a neural network is a differentiable function, so calculus will be a fundamental tool to train neural networks, as we will see.', 'To familiarize yourself with the concepts, you should make things simple and study functions of a single variable for the first time. By definition, the derivative of a function is defined by', 'where the ratio for a given h is the slope of the line between the points (x, f(x)) and (x+h, f(x+h)).', 'In the limit, this is essentially the slope of the tangent line at the point x. The figure below illustrates the concept.', 'Differentiation can be used to optimize functions: the derivative is zero at local maxima or minima. (However, this is not true in the other direction; see f(x) = x³ at 0.) Points where the derivative is zero, are called critical points. Whether a critical point is minima or maxima can be decided by looking at the second derivative:', 'There are several essential rules regarding differentiation, but probably the most important is the so-called chain rule:', 'which tells us how to calculate the derivative of composed functions.', 'Integration is often called the inverse of differentiation. This is true because', 'which holds for any integrable function f(x). The integral of a function can also be thought of as the signed area under the curve. For instance,', 'because when the function is negative, the area there also has a negative sign.', 'Integration itself plays a role in understanding the concept of expected value. For instance, quantities like entropy and Kullback-Leibler divergence are defined in terms of integrals.', 'I would recommend the Single Variable Calculus course from MIT. (In general, online courses from MIT are always excellent learning resources.) If you are more of a book person, there are many textbooks available. The Calculus book by Gilbert Strang accompanying the previously mentioned course is again a great resource, completely free of charge.', 'As I mentioned, neural networks are essentially functions, which are trained using the tools of calculus. However, they are described with linear algebraic concepts like matrix multiplication.', 'Linear algebra is a vast subject with many essential aspects of machine learning, so this will be a significant segment.', 'To have a good understanding of linear algebra, I would suggest starting with vector spaces. It is better if we talk about a special case first. You can think of each point in the plane as a tuple', 'These are essentially vectors pointing from zero to (x₁, x₂). You can add these vectors together and multiply them with scalars:', 'This is the prototypical model of a vector space. In general, a set of vectors V is a vector space over the real numbers if you can add vectors together and multiply a vector with a real number, such that the following properties hold:', 'Don’t panic! I know that this looks very scary (at least it looked that to me when I was a freshman student with a math major), but it really isn’t. These just guarantee that vectors can be added together and scaled just as you would expect. When thinking about vector spaces, it helps if you mentally model them as', 'If you feel like you have a good understanding of vector spaces, the next step would be to understand how to measure a vector’s magnitude. By default, a vector space in itself gives no tools for this. How would you do this on the plane? You have probably learned that there, we have', 'This is a special case of a norm. In general, a vector space V is normed if there is a function', 'called norm such that', 'Again, this might be scary, but this is a simple and essential concept. There are a bunch of norms out there, but the most important is the p-norm family', '(with p = 2 we obtain the special case mentioned above) and the supremum norm', 'Sometimes, like for p = 2, the norm comes from a so-called inner product, which is a bilinear function', 'such that', 'A vector space with an inner product is called inner product space. An example is the classical Euclidean product', 'Every inner product can be turned into a norm by', 'When the inner product for two vectors is zero, we say that the vectors are orthogonal to each other. (Try to come up with some concrete examples on the plane to understand the concept deeper.)', 'Although vector spaces are infinite (in our case), you can find a finite set of vectors that can be used to express all vectors in the space. For example, on the plane, we have', 'where', 'This is a special case of basis and orthonormal basis.', 'In general, basis is a minimal set of vectors', 'such that their linear combinations span the vector space:', 'A basis always exists for any vector space. (It may not be a finite set, but that shouldn’t concern us now.) Without a doubt, a basis simplifies things greatly when talking about linear spaces.', 'When the vectors in a basis are orthogonal to each other, we call it an orthogonal basis. If each basis vector’s norm is 1 for an orthogonal basis, we say it is orthonormal.', 'One of the key objects related to vector spaces are linear transformations. If you have seen a neural network before, you know that one of the fundamental building blocks are layers of the form', 'where A is a matrix, b and x are vectors, and σ is the sigmoid function. (Or any activation function, really.) Well, the part Ax is a linear transformation. In general, the function', 'is a linear transformation between vector spaces V and W if', 'holds for all x, y in V, and all a real number.', 'To give a concrete example, rotations around the origin in the plane are linear transformations.', 'Undoubtedly, the most crucial fact about linear transformations is that they can be represented with matrices, as you’ll see next in your studies.', 'If linear transformations are clear, you can turn to the study of matrices. (Linear algebra courses often start with matrices, but I would recommend it this way for reasons to be explained later.)', 'The most important operation for matrices is the matrix product. In general, if A and B are matrices defined by', 'then their product can be obtained by', 'This might seem difficult to comprehend, but it is actually quite straightforward. Take a look at the figure below, demonstrating how to calculate the element in the 2nd row, 1 st column of the product.', 'The reason why matrix multiplication is defined the way it is because matrices represent linear transformations between vector spaces. Matrix multiplication is the composition of linear transformations.', 'If you would like to read more about this, there is a great article right here at Towards Data Science, explaining things in detail.', 'In my opinion, determinants are hands down one of the most challenging concepts to grasp in linear algebra. Depending on your learning resource, it is usually defined by either a recursive definition or a sum that iterates through all permutations. None of them are tractable without significant experience in mathematics.', 'To understand this concept, watch this video below. Trust me, it is magic.', 'To summarize, the determinant of a matrix describes how the volume of an object scales under the corresponding linear transformation. If the transformation changes orientations, the sign of the determinant is negative.', 'You will eventually need to understand how to calculate the determinant, but I wouldn’t worry about it now.', 'A standard first linear algebra course usually ends with eigenvalues/eigenvectors and some special matrix decompositions like the Singular Value Decomposition.', 'Let’s suppose that we have a matrix A. The number λ is an eigenvalue of A if there is a vector x (called eigenvector) such that', 'holds. In other words, the linear transformation represented by A is a scaling by λ for the vector x. This concept plays an essential role in linear algebra. (And practically in every field which uses linear algebra extensively.)', 'At this point, you are ready to familiarize yourself with a few matrix decompositions. If you think about it for a second, what type of matrices are the best from a computational perspective? Diagonal matrices! If a linear transformation has a diagonal matrix, it is trivial to compute its value on an arbitrary vector.', 'Most special forms aim to decompose a matrix A to a product of matrices, where preferably at least one is diagonal. Singular Value Decomposition, or SVD in short, the most famous one, states that there are special matrices U, V, and a diagonal matrix Σ such that', 'holds. (U and V are so-called unitary matrices, which I don’t define here, suffice to know that it is a special family of matrices.)', 'SVD is also used to perform Principal Component Analysis, one of the simplest and most well-known dimensionality reduction methods.', 'Linear algebra can be taught in many ways. The path I outlined here was inspired by the textbook Linear Algebra Done Right by Sheldon Axler. For an online lecture, I would recommend the Linear Algebra course from MIT OpenCourseWare, an excellent resource.', 'If a course might be too much, there are great articles out there, for instance, the following.', 'This is the part where linear algebra and calculus come together, laying the foundations for the primary tool to train neural networks: gradient descent. Mathematically speaking, a neural network is simply a function of multiple variables. (Although, the number of variables can be in the millions.)', 'Similar to univariable calculus, the two main topics here are differentiation and integration. Let’s suppose that we have a function', 'mapping vectors to real numbers. In two dimensions (that is, for n = 2), you can imagine its plot as a surface. (Since humans don’t see higher than three dimensions, it is hard to visualize functions with more than two real variables.)', 'In a single variable, the derivative was the slope of the tangent line. How would you define tangents here? A point on the surface has several tangents, not just one. However, there are two special tangents: one is parallel to the x-z plane, while the other is to the y-z plane. Their slope is determined by the partial derivatives, defined by', 'That is, you take the derivative of the functions obtained by fixing all but one variable. (The formal definition is identical for ≥ 3 variables, just more complicated notation.)', 'Tangents in these special directions span the tangent plane.', 'There is another special direction: the gradient, which is the vector defined by', 'The gradient always points to the direction of the largest increase! So, if you would take a tiny step in this direction, your elevation would be the maximal among all the other directions you could have chosen. This is the basic idea of gradient descent, which is an algorithm to maximize functions. Its steps are the following.', 'Of course, there are several flaws in this basic algorithm, which was improved several times over the years. Modern gradient descent based optimizers employ many tricks like adaptive step size, momentum, and other methods, which we are not going to detail here.', 'Calculating the gradient in practice is difficult. Functions are often described by the composition of other functions, for instance, the familiar linear layer', 'where A is a matrix, b and x are vectors, and σ is the sigmoid function. (Of course, there can be are other activations, but we shall stick to this for simplicity.) How would you calculate this gradient? At this point, it is not even clear how to define the gradient for vector-vector functions such as this, so let’s discuss! A function', 'can always be written in terms of vector-scalar functions like', 'The gradient of g is defined by the matrix whose k-th row is the k-th component’s gradient. That is,', 'This matrix is called the total derivative of g.', 'In our example', 'things become a bit more complicated because it is the composition of two functions:', 'and', 'defined by applying the univariate sigmoid componentwise. The function l can be decomposed further to m functions mapping from the n-dimensional vector space to the space of real numbers:', 'where', 'If you calculate the total derivative, you’ll see that', 'This is the chain rule for multivariate functions in its full generality. Without it, there would be no easy way to calculate the gradient of a neural network, which is ultimately a composition of many functions.', 'Similarly to the univariate case, the gradient and the derivatives play a role in determining whether a given point in space is local minima or maxima. (Or neither.) To provide a concrete example, training a neural network is equivalent to minimizing the loss function on the parameters’ training data. It is all about finding the optimal parameter configuration w for which the minimum is attained:', 'where', 'are the neural network and the loss function, respectively.', 'For a general differentiable vector-scalar function of say n variables, there are n² second derivatives, forming the Hessian matrix', 'In multiple variables, the determinant of the Hessian takes the role of the second derivative. Similarly, it can be used to tell whether a critical point (that is, where all of the derivatives are zero) is minima, maxima, or just a saddle point.', 'There are a lot of awesome online courses on multivariable calculus. I have two specific recommendations:', 'Now we are ready to take on the final subject: probability theory!', 'Probability theory is the mathematically rigorous study of chance, fundamental to all fields of science.', 'Setting exact definitions aside, let’s ponder a bit about what probability represents. Let’s say I toss a coin, with a 50% chance (or 0.5 probability) of it being heads. After repeating the experiment 10 times, how many heads did I get?', 'If you have answered 5, you are wrong. Heads being 0.5 probability doesn’t guarantee that every second throw is heads. Instead, what it means that if you repeat the experiment n times where n is a really large number, the number of heads will be very close to n/2.', 'To get a good grip on probability, I would recommend my post below, which I wrote a while ago, to offer a concise yet mathematically correct presentation of the concept.', 'Besides the basics, there are some advanced things you need to understand, first and foremost, expected value and entropy.', 'Supposed that you play a game with your friend. You toss a classical six-sided dice, and if the outcome is 1 or 2, you win 300 bucks. Otherwise, you lose 200. What are your average earnings per round if you play this game long enough? Should you be even playing this game?', 'Well, you win 100 bucks with probability 1/3, and you lose 200 with probability 2/3. That is if X is the random variable encoding the result of the dice throw, then', 'This is the expected value, that is, the average amount of money you will receive per round in the long run. Since this is negative, you will lose money, so you should never play this game.', 'Generally speaking, the expected value is defined by', 'for discrete random variables and', 'for real-valued continuous random variables.', 'In machine learning, loss functions for training neural networks are expected values in one way or another.', 'People often falsely attribute certain phenomena to the law of large numbers. For instance, gamblers who are on a losing streak believe that they should soon win because of the law of large numbers. This is totally wrong. Let’s see what this is really!', 'Suppose that', 'are random variables representing the independent repetitions of the same experiment. (Say, rolling a dice or tossing a coin.)', 'Essentially, the law of large numbers states that', 'That is the average of the outcomes in the long run equal to the expected value.', 'An interpretation is that if a random event is repeated enough times, individual results might not matter. So, if you are playing in a casino with a game that has negative expected value (as they all do), it doesn’t matter that you win occasionally. The law of large numbers implies that you will lose money.', 'To get a little bit ahead, LLN is going to be essential for stochastic gradient descent.', 'Let’s play a game. I have thought of a number between 1 and 1024, and you have to guess it. You can ask questions, but your goal is to use as few questions as possible. How much do you need?', 'If you play it smart, you will perform a binary search with your questions. First you may ask: is the number between 1 and 512? With this, you have cut the search space in half. Using this strategy, you can figure out the answer in', 'questions.', 'But what if I didn’t use the uniform distribution when picking the number? For instance, I could have used a Poisson distribution.', 'Here, you would probably need fewer questions because you know that the distribution tends to concentrate around specific points. (Which depends on the parameter.)', 'In the extreme case, when the distribution is concentrated on a single number, you need zero questions to guess it correctly. Generally, the number of questions depends on the information carried by the distribution. The uniform distribution contains the least amount of information, while singular ones are pure information.', 'Entropy is a way to quantify this. It is defined by', 'for discrete random variables and', 'for continuous, real-valued ones. (The base of the logarithm is usually 2, e, or 10, but it doesn’t really matter.)', 'If you have ever worked with classification models before, you probably encountered the cross-entropy loss, defined by', 'where P is the ground truth (a distribution concentrated to a single class), while the hatted version represents the class predictions. This measures how much “information” the predictions have compared to the ground truth. When the predictions match, the cross-entropy loss is zero.', 'Another frequently used quantity is the Kullback-Leibler divergence, defined by', 'where P and Q are two probability distributions. This is essentially cross-entropy minus the entropy, which can be thought of as quantifying how different the two distributions are. This is useful, for instance, when training generative adversarial networks. Minimizing the Kullback-Leibler divergence guarantees that the two distributions are similar.', 'Here, I would recommend two books for you:', 'These are the two fundamental textbooks, and they teach you much more than probability theory. Both of them go way beyond the basics, but the corresponding chapters provide an excellent introduction.', 'With this, we reviewed the necessary mathematics for understanding neural networks. Now, you are ready for the fun part: machine learning!', 'To really understand how neural networks work, you still have to learn some optimization and mathematical statistics. These subjects build upon the foundations we set. I won’t go into details, since this is beyond the scope of the article, but I have prepared a study roadmap to guide you.', 'If you would like to read more about some of these topics, check out some of my articles below!']"
10/2020,10 Awesome Python 3.9 Features,The Must-Know Python 3.9 Features,1.6K,5,https://towardsdatascience.com/@farhadmalik,https://towardsdatascience.com/10-awesome-python-3-9-features-b8c27f5eba5c?source=collection_archive---------2-----------------------,11,12,"['10 Awesome Python 3.9 Features', 'In A Nutshell', '1. Feature: Dictionary Update And Merge Operators', '2. Feature: New Flexible High Performant PEG-Based Parser', '3. Feature: New String Functions To Remove Prefix and Suffix', '4. Feature: Type Hinting For Built-in Generic Types', '5. Feature: Support For IANA timezone In DateTime', '6. Feature: Ability To Cancel Concurrent Futures', '7. Feature: AsyncIO and multiprocessing Improvements', '8. Feature: Consistent Package Import Errors', '9. Feature: Random Bytes Generation', '10. Feature: String Replace Function Fix']",80,"['The latest Python 3.9.0 final version is out on the Monday, 2020–10–05', 'Just like most of the Python fans, I am super excited to explore and use the latest features. This article will provide an overview of the must-know features of Python 3.9.', 'This is again an exciting time for the Python programmers.', 'I read through the Python 3.9 release notes and the associated discussions. Based on the information, I wanted to write a comprehensive guide so everyone can get a glimpse of the features along with their detailed workings.', 'Before I begin, I have to say, I am very excited to explore version 3.9 as some of the features are definitely going to be used in my applications.', 'From the dictionary update/merge to the addition of new string methods to the introduction of zoneinfo library, a number of new features have been added. Furthermore, a new stable and high-performant parser has been introduced.', 'The standard library is updated with numerous new features along with the addition of new modules, zoneinfo and graphlib. A number of modules have been improved too, such as ast, asyncio, concurrent.futures, multiprocessing, xml amongst others.', 'This release has further stabilized the Python standard library.', 'Let’s explore Python 3.9 features now.', 'Two new operators, | and |= have been added to the built-in dict class.', 'The | operator is used to merge dictionaries whereas the |= operator can be used to update the dictionaries.', 'For Merge: |', 'For Update: |=', 'The key rule to remember is that if there are any key conflicts then the rightmost-value will be kept. It means that the last seen value always wins. This is the current behavior of other dict operations too.', 'As we can see above, the two new operators, | and |=, have been added to the built-in dict class.', 'The | operator is used to merge dictionaries whereas the |= operator can be used to update the dictionaries.', 'We can consider | as the + (concatenate) in lists and we can consider |= as the += operator (extend) in lists.', 'If we assess version 3.8, we will notice that there are few ways of merging and updating dictionaries.', 'As an instance, we can do first_dict.update(second_dict). The issue with this approach is that it modifies first_dict in place. One way to solve this issue is to copy the first_dict in a temporary variable and then perform an update. However, it adds extra unnecessary code just to get the update/merge to work.', 'We can also use {**first_dict, **second_dict}. The issue with this approach is that it is not easily discoverable and it’s harder to understand what the code is intended to perform. The other issue with this approach is that the mappings types are ignored and the type is always dict. As an instance, if the first_dict is a defaultdict and the second_dict is of type dict then it will fail.', 'Lastly, the collections library contains aChainMap function. It can take in two dictionaries such as ChainMap(first_dict, second_dict) and return a merged dictionary but again, this library is not commonly-known.', 'It also fails for subclasses of dict that have an incompatible __init__ method.', 'The Python 3.9 version is proposing to replace the current LL(1) based Python parser with a new PEG-based parser which is high-performant and stable.', 'The current CPython parser is LL(1) based. Subsequently, the grammar is LL(1) based to allow it to be parsed by the LL(1) parser. The LL(1) parser is a top-down parser. Furthermore, it parses the inputs from left to right. The current grammar is context-free grammar hence the context of the tokens is not taken into account.', 'The Python 3.9 version is proposing to replace it with a new PEG-based parser which means it will lift the current LL(1) grammar Python restrictions. Additionally, the current parser has been patched with a number of hacks that are going to be removed. As a result, it will reduce the maintenance cost\xa0in\xa0long\xa0run.', 'As an instance, although the LL(1) parsers and grammars are simple to implement, the restrictions do not allow them to express common constructs in a natural way to the language designer and the reader. The parser only looks at one token ahead to distinguish possibilities.', 'The choice operator | is ordered. For an instance, if the following rule is written:', 'The LL(1) parser, a context-free grammar parser, will generate constructions such that given an input string will deduce whether it needs to expand A or B or C. The PEG parser is different. It will check if the first alternative succeeds. If it fails only then it will continue with the second or the third.', 'The PEG parser generates exactly one valid-tree for a string. Hence it’s not ambiguous like the LL(1) parser.', 'The PEG parser also directly generates the AST nodes for a rule via grammar actions. This means it avoids the generation of the intermediate steps.', 'The key point to take is that the PEG parser has been extensively tested and validated. The PEG parser performance is fine-tuned. As a result, for most instructions, it comes within 10% of the memory and speed consumption of the current parser. This is mainly because the intermediate syntax tree is not constructed.', 'I am eliminating the mention of the low-level details for the sake of keeping the article\xa0simple\xa0and\xa0readable. The link is provided at the bottom\xa0if\xa0more\xa0information\xa0is\xa0required\xa0to\xa0be understood.', 'Two new functions have been added to the str object.', 'One of the common tasks in a data science application that involves manipulating text is to remove the prefix/suffix of the strings. Two new functions have been added to the str object. These functions can be used to remove unneeded prefix and suffix from a string.', 'The first function removes the prefix. It is str.removeprefix(prefix). The second function removes the suffix. It is str.removesuffix(suffix).', 'Remember string is a collection of characters and each character has an index in a string. We can use the indexes along with the colon : to return a subset of the string. This feature is known as slicing a string.', 'If we study the functions, internally they check if the string starts with a prefix (or ends with a suffix) and if it does then they return a string without a prefix (or after a suffix) using str[:] slicing feature.', 'With these functions being part of the standard library, we get an API that is consistent, less fragile, high performant and is more descriptive.', 'Annotating programs have been made simpler in this release by removing the parallel type hierarchy in Python.', 'The release has enabled support for the generics syntax in all standard collections currently available in the typing module.', 'We can use the list or dict built-in collection types as generic types instead of using the typing.List or typing.Dict in the signature of our function.', 'Therefore, the code now looks cleaner and it has made it easier to understand/explain the code.', 'Although Python is a dynamically typed language, the annotation of the types in the Python program enables introspection of the type. Subsequently, the annotation can be used for API generation of runtime type checking.', 'This release has enabled support for the generics syntax in all standard collections currently available in the typing module.', 'A generic type is usually a container e.g. list. It is a type that can be parameterized. Parameterized generic is an instance of a generic with the expected types for container elements e.g. list[str]', 'We can use the list or dict built-in collection types as generic types instead of using the typing.List or typing.Dict.', 'For example, we could guide the Python runtime type checking by annotating the code:', 'Over the past few releases, a number of static typing features have been built incrementally on top of the existing Python runtime. Some of these features were constrained by existing syntax and runtime behavior. As a consequence, there was a duplicated collection hierarchy in the typing module due to generics.', 'For instance, we will see typing.List, typing.Dictionary along with built-in list, dictionary, and so on. This enables us to write code:', 'The module zoneinfo has been created to support the IANA time zone database. This support for the IANA time zone database has been added to the standard library.', 'IANA time zones are often called tz or zone info. There are a large number of IANA time zones with different search paths to specify the IANA timezone to a date-time object. As an instance, we can pass in the name of the search path as the Continent/City to a datetime object to set its tzinfo.', 'If we pass in an invalid key then zoneinfo.ZoneInfoNotFoundError will be raised.', 'We use the datetime library to create a datetime object and specify its timezone\xa0by\xa0setting\xa0the\xa0tzinfo propety. However, we can end up creating complex timezone rules when using the datetime.tzinfo baseline.', 'Most of time, we only need to set the object and set its timezone to either UTC, system local time zone, or IANA time zone.', 'We can create a zoneinfo.ZoneInfo(key) object where the key is of type string indicating the search path of the zone file in the system time zone database. The zoneinfo.ZoneInfo(key) object can be created and set as the tzinfo property of the datetime object.', 'A new parameter cancel_futures have been added to the concurrent.futures.Executor.shutdown().', 'This parameter cancels all of the pending futures that have not started. Prior to version 3.9, the process would wait for them to complete before shutting down the executor.', 'The new parameter cancel_futures have been added to both ThreadPoolExecutor and ProcessPoolExecutor. The way it works is when the value of the parameter is True then all pending futures would be canceled\xa0when\xa0the\xa0shutdown()\xa0function\xa0is\xa0called.', 'In a nutshell, when the shutdown() is executed, the interpreter checks if the executor is not garbage collected. If it is still in memory then it gets all of the pending worker items and then cancels the futures.', 'Once there are no pending work items then it shuts down the worker.', 'A number of improvements have been made to the asyncio and multiprocessing library in this release.', 'As an instance,', 'With regards to the multiprocessing library improvements, a new method close() has been added to the multiprocessing.SimpleQueue class.', 'This method explicitly closes the queue. This will ensure that the queue is closed and does not stay around for longer than expected. The key to remember is that the methods get(), put(), empty() must not be called once the queue is closed.', 'The main issue with importing Python libraries prior to the 3.9 release was the inconsistent import behavior in Python when the relative import went past its top-level package.', 'The builtins.__import__() raises ValueError while importlib.__import__() raises ImportError.', 'It has been fixed now. The __import__() now raises ImportError instead of ValueError.', 'Another feature that has been added in the 3.9 release is the function random.Random.randbytes(). This function can be used to generate random bytes.', 'We can generate random numbers but what if we needed to generate random bytes? Prior to 3.9 version, the developers had to get creative to generate the random bytes. Although we can use os.getrandom(), os.urandom() or secrets.token_bytes() but we can’t generate pseudo-random patterns.', 'As an instance, to ensure the random numbers are generated with expected behaviour and the process is reproducible, we generally use the seed with random.Random module.', 'As a result, random.Random.randbytes() method has been introduced. It can generate random bytes\xa0in\xa0a\xa0controlled\xa0manner\xa0too.', 'Prior to the Python version 3.9, the “”.replace(“”,s,n) returned empty string instead of s for all non-zero n.', 'This bug confused the users and caused inconsistent behavior in applications.', 'The 3.9 release has fixed this issue and it is now consistent with """".replace("""", s).', 'The way replace function works is that for a given max replace occurrence argument, it replaces a set of characters from the string by a new set of characters.', 'To further explain the issue, prior to the version 3.9, the replace function had inconsistent behaviour', 'Therefore the change now is that if we pass in:', 'A number of redundant features have been eliminated in the Python 3.9 too, such as Py_UNICODE_MATCH.', 'I am very excited to explore 3.9 as some of the features are definitely going to be used in my applications.', 'If you want to read more about those improvements, please read the official guide here.']"
10/2020,Tiny Machine Learning: The Next AI Revolution,The bigger model is not always the better model,2.1K,8,https://towardsdatascience.com/@matthew_stewart,https://towardsdatascience.com/tiny-machine-learning-the-next-ai-revolution-495c26463868?source=collection_archive---------3-----------------------,16,6,"['Tiny Machine Learning: The Next AI Revolution', 'Introduction', 'Examples of TinyML', 'How TinyML Works', 'The Next AI Revolution', 'References']",71,"['Miniaturization of electronics started by NASA’s push became an entire consumer products industry. Now we’re carrying the complete works of Beethoven on a lapel pin listening to it in headphones. — Neil deGrasse Tyson, astrophysicist and science commentator', '[…] the pervasiveness of ultra-low-power embedded devices, coupled with the introduction of embedded machine learning frameworks like TensorFlow Lite for Microcontrollers will enable the mass proliferation of AI-powered IoT devices. — Vijay Janapa Reddi, Associate Professor at Harvard University', 'This is the first in a series of articles on tiny machine learning. The goal of this article is to introduce the reader to the idea of tiny machine learning and its future potential. In-depth discussion of specific applications, implementations, and tutorials will follow in subsequent articles in the series.', 'Over the past decade, we have witnessed the size of machine learning algorithms grow exponentially due to improvements in processor speeds and the advent of big data. Initially, models were small enough to run on local machines using one or more cores within the central processing unit (CPU).', 'Shortly after, computation using graphics processing units (GPUs) became necessary to handle larger datasets and became more readily available due to introduction of cloud-based services such as SaaS platforms (e.g., Google Colaboratory) and IaaS (e.g., Amazon EC2 Instances). At this time, algorithms could still be run on single machines.', 'More recently, we have seen the development of specialized application-specific integrated circuits (ASICs) and tensor processing units (TPUs), which can pack the power of ~8 GPUs. These devices have been augmented with the ability to distribute learning across multiple systems in an attempt to grow larger and larger models.', 'This came to a head recently with the release of the GPT-3 algorithm (released in May 2020), boasting a network architecture containing a staggering 175 billion neurons — more than double the number present in the human brain (~85 billion). This is more than 10x the number of neurons than the next-largest neural network ever created, Turing-NLG (released in February 2020, containing ~17.5 billion parameters). Some estimates claim that the model cost around $10 million dollars to train and used approximately 3 GWh of electricity (approximately the output of three nuclear power plants for an hour).', 'While the achievements of GPT-3 and Turing-NLG are laudable, naturally, this has led to some in the industry to criticize the increasingly large carbon footprint of the AI industry. However, it has also helped to stimulate interest within the AI community towards more energy-efficient computing. Such ideas, like more efficient algorithms, data representations, and computation have been the focus of a seemingly unrelated field for several years: tiny machine learning.', 'Tiny machine learning (tinyML) is the intersection of machine learning and embedded internet of things (IoT) devices. The field is an emerging engineering discipline that has the potential to revolutionize many industries.', 'The main industry beneficiaries of tinyML are in edge computing and energy-efficient computing. TinyML emerged from the concept of the internet of things (IoT). The traditional idea of IoT was that data would be sent from a local device to the cloud for processing. Some individuals raised certain concerns with this concept: privacy, latency, storage, and energy efficiency to name a few.', 'Energy Efficiency. Transmitting data (via wires or wirelessly) is very energy-intensive, around an order of magnitude more energy-intensive than onboard computations (specifically, multiply-accumulate units). Developing IoT systems that can perform their own data processing is the most energy-efficient method. AI pioneers have discussed this idea of “data-centric” computing (as opposed to the cloud model’s “compute-centric”) for some time and we are now beginning to see it play out.', 'Privacy. Transmitting data opens the potential for privacy violations. Such data could be intercepted by a malicious actor and becomes inherently less secure when warehoused in a singular location (such as the cloud). By keeping data primarily on the device and minimizing communications, this improves security and privacy.', 'Storage. For many IoT devices, the data they are obtaining is of no merit. Imagine a security camera recording the entrance to a building for 24 hours a day. For a large portion of the day, the camera footage is of no utility, because nothing is happening. By having a more intelligent system that only activates when necessary, lower storage capacity is necessary, and the amount of data necessary to transmit to the cloud is reduced.', 'Latency. For standard IoT devices, such as Amazon Alexa, these devices transmit data to the cloud for processing and then return a response based on the algorithm’s output. In this sense, the device is just a convenient gateway to a cloud model, like a carrier pigeon between yourself and Amazon’s servers. The device is pretty dumb and fully dependent on the speed of the internet to produce a result. If you have slow internet, Amazon Alexa will also become slow. For an intelligent IoT device with onboard automatic speech recognition, the latency is reduced because there is reduced (if not no) dependence on external communications.', 'These issues led to the development of edge computing, the idea of performing processing activities onboard of edge devices (devices at the “edge” of the cloud). These devices are highly resource-constrained in terms of memory, computation, and power, leading to the development of more efficient algorithms, data structures, and computational methods.', 'Such improvements are also applicable to larger models, which may lead to efficiency increases in machine learning models by orders of magnitude with no impact on model accuracy. As an example, the Bonsai algorithm developed by Microsoft can be as small as 2 KB but can have even better performance than a typical 40 MB kNN algorithm, or a 4 MB neural network. This result may not sound important, but the same accuracy on a model 1/10,000th of the size is quite impressive. A model this small can be run on an Arduino Uno, which has 2 KB RAM available — in short, you can now build such a machine learning model on a $5 microcontroller.', 'We are at an interesting crossroads where machine learning is bifurcating between two computing paradigms: compute-centric computing and data-centric computing. In the compute-centric paradigm, data is stockpiled and analyzed by instances in data centers, while in the data-centric paradigm, the processing is done locally at the origin of the data. Although we appear to be quickly moving towards a ceiling in the compute-centric paradigm, work in the data-centric paradigm has only just begun.', 'IoT devices and embedded machine learning models are becoming increasingly ubiquitous in the modern world (predicted more than 20 billion active devices by the end of 2020). Many of these you may not even have noticed. Smart doorbells, smart thermostats, a smartphone that “wakes up” when you say a couple of words, or even just pick up the phone. The remainder of this article will focus deeper on how tinyML works, and on current and future applications.', 'Previously, complex circuitry was necessary for a device to perform a wide range of actions. Now, machine learning is making it increasingly possible to abstract such hardware “intelligence” into software, making embedded devices increasingly simple, lightweight, and flexible.', 'The challenges that machine learning with embedded devices presents are considerable, but great progress has already been achieved in this area. The key challenges in deploying neural networks on microcontrollers are the low memory footprint, limited power, and limited computation.', 'Perhaps the most obvious example of TinyML is within smartphones. These devices perpetually listen actively for ‘wake words’, such as “Hey Google” for Android smartphones, or ‘Hey Siri” on iPhones. Running these activities through the main central processing unit (CPU) of a smartphone, which is 1.85 GHz for the modern iPhone, would deplete the battery in just a few hours. This level of degradation is not acceptable for something that most people would use a few times a day at most.', 'To combat this, developers created specialized low-power hardware that is able to be powered by a small battery (such as a circular CR2032 “coin” battery). These allow the circuits to remain active even when the CPU is not running, which is basically whenever the screen is not lit.', 'These circuits can consume as little as 1 mW and can be powered for up to a year using a standard CR2032 battery.', 'It may not seem like it, but this is a big deal. Energy is a limiting factor for many electronic devices. Any device that requires mains electricity is restricted to locations with wiring, which can quickly get overwhelming when a dozen devices are present in the same location. Mains electricity is also inefficient and expensive. Converting mains voltage (which operates around 120 V in the United States) to a typical circuit voltage range (often ~5 V) wastes large amounts of energy. Anyone with a laptop charger will probably know this when unplugging their charger. The heat from the transformer within the charger is wasted energy during the voltage conversion process.', 'Even devices with batteries suffer from limited battery life, which requires frequent docking. Many consumer devices are designed such that the battery lasts for a single workday. TinyML devices that can continue operating for a year on a battery the size of a coin mean they can be placed in remote environments, only communicating when necessary in order to conserve energy.', 'Wake words are not the only TinyML we see seamlessly embedded in smartphones. Accelerometer data is used to determine whether someone has just picked the phone up, which wakes the CPU and turns on the screen.', 'Clearly, these are not the only possible applications of TinyML. In fact, TinyML presents many exciting opportunities for businesses and hobbyists alike to produce more intelligent IoT devices. In a world where data is becoming more and more important, the ability to distribute machine learning resources to memory-constrained devices in remote locations could have huge benefits on data-intensive industries such as farming, weather prediction, or seismology.', 'It is without a doubt that empowering edge devices with the capability of performing data-driven processing will produce a paradigm shift for industrial processes. As an example, devices that are able to monitor crops and send a “help” message when it detects characteristics such as soil moisture, specific gases (for example, apples emit ethane when ripe), or particular atmospheric conditions (e.g., high winds, low temperatures, or high humidity), would provide massive boosts to crop growth and hence crop yield.', 'As another example, a smart doorbell might be fitted with a camera that can use facial recognition to determine who is present. This could be used for security purposes, or even just so that the camera feed from the doorbell is fed to televisions in the house when someone is present so that the residents know who is at the door.', 'Two of the main focus areas of tinyML currently are:', 'Keyword spotting. Most people are already familiar with this application. “Hey Siri” and “Hey Google” are examples of keywords (often used synonymously with hotword or wake word). Such devices listen continuously to audio input from a microphone and are trained to only respond to specific sequences of sounds, which correspond with the learned keywords. These devices are simpler than automatic speech recognition (ASR) applications and utilize correspondingly fewer resources. Some devices, such as Google smartphones, utilize a cascade architecture to also provide speaker verification for security.', 'Visual Wake Words. There is an image-based analog to the wake words known as visual wake words. Think of these as a binary classification of an image to say that something is either present or not present. For example, a smart lighting system may be designed such that it activates when it detects the presence of a person and turns off when they leave. Similarly, wildlife photographers could use this to take pictures when a specific animal is present, or security cameras when they detect the presence of a person.', 'A more broad overview of current machine learning use cases of TinyML is shown below.', 'TinyML algorithms work in much the same way as traditional machine learning models. Typically, the models are trained as usual on a user’s computer or in the cloud. Post-training is where the real tinyML work begins, in a process often referred to as deep compression.', 'Post-training, the model is then altered in such a way as to create a model with a more compact representation. Pruning and knowledge distillation are two such techniques for this purpose.', 'The idea underlying knowledge distillation is that larger networks have some sparsity or redundancy within them. While large networks have a high representational capacity, if the network capacity is not saturated it could be represented in a smaller network with a lower representation capacity (i.e., less neurons). Hinton et al. (2015) referred to the embedded information in the teacher model to be transferred to the student model as “dark knowledge”.', 'The below diagram illustrates the process of knowledge distillation.', 'In this diagram, the ‘teacher’ is a trained neural network model. The teacher is tasked with transferring its ‘knowledge’ to a smaller network model with fewer parameters, the ‘student’. This process is used to enshrine the same knowledge in a smaller network, providing a way of compressing the knowledge representation, and hence the size, of a neural network such that they can be used on more memory-constrained devices.', 'Similarly, pruning can help to make the model’s representation more compact. Pruning, broadly speaking, attempts to remove neurons that provide little utility to the output prediction. This is often associated with small neural weights, whereas larger weights are kept due to their greater importance during inference. The network is then retrained on the pruned architecture to fine-tune the output.', 'Following distillation, the model is then quantized post-training into a format that is compatible with the architecture of the embedded device.', 'Why is quantization necessary? Imagine an Arduino Uno using an ATmega328P microcontroller, which uses 8-bit arithmetic. To run a model on the Uno, the model weights would ideally have to be stored as 8-bit integer values (whereas many desktop computers and laptops use 32-bit or 64-bit floating-point representation). By quantizing the model, the storage size of weights is reduced by a factor of 4 (for a quantization from 32-bit to 8-bit values), and the accuracy is often negligibly impacted (often around 1–3%).', 'Some information may be lost during quantization due to quantization error (for example, a value that is 3.42 on a floating-point representation may be truncated to 3 on an integer-based platform). To combat this, quantization-aware (QA) training has also been proposed as an alternative. QA training essentially constrains the network during training to only use the values that will be available on the quantized device (see Tensorflow example).', 'Encoding is an optional step that is sometimes taken to further reduce the model size by storing the data in a maximally efficient way: often via the famed Huffman encoding.', 'Once the model has been quantized and encoded, it is converted to a format that can be interpreted by some form of light neural network interpreter, the most popular of which are probably TF Lite (~500 KB in size) and TF Lite Micro (~20 KB in size). The model is then compiled into C or C++ code (the languages most microcontrollers work in for efficient memory usage) and run by the interpreter on-device.', 'Most of the skill of tinyML comes in dealing with the complex world of microcontrollers. TF Lite and TF Lite Micro are so small because any unnecessary functionality has been removed. Unfortunately, this includes useful abilities such as debugging and visualization. This means that it can be difficult to discern what is going on if there is an error during deployment.', 'Additionally, while the model has to be stored on the device, the model also has to be able to perform inference. This means the microcontroller must have a memory large enough that it can run (1) its operating system and libraries, (2) a neural network interpreter such as TF Lite, (3) the stored neural weights and neural architecture, and (4) the intermediate results during inference. Thus, the peak memory usage of a quantized algorithm is often quoted in tinyML research papers, along with memory usage, the number of multiply-accumulate units (MACs), accuracy, etc.', 'Training on-device brings about additional complications. Due to reduced numerical precision, it becomes exceedingly difficult to guarantee the necessary level of accuracy to sufficiently train a network. Automatic differentiation methods on a standard desktop computer are approximately accurate to machine precision. Computing derivatives to the accuracy of 10^-16 is incredible, but utilizing automatic differentiation on 8-bit values will result in poor results. During backpropagation, these derivatives are compounded and eventually used to update neural parameters. With such a low numerical precision, the accuracy of such a model may be poor.', 'That being said, neural networks have been trained using 16-bit and 8-bit floating-point numbers.', 'The first paper looking at reducing numerical precision in deep learning was the 2015 paper “Deep Learning with Limited Numerical Precision” by Suyog Gupta and colleagues. The results of this paper were interesting, showing that the 32-bit floating-point representation could be reduced to a 16-bit fixed-point representation with essentially no degradation in accuracy. However, this is the only case when stochastic rounding is used because, on average, it produces an unbiased result.', 'In 2018, Naigang Wang and colleagues trained a neural network using 8-bit floating point numbers in their paper “Training Deep Neural Networks with 8-bit Floating Point Numbers”. Training a neural network using 8-bit numbers rather than inference is significantly more challenging to achieve because of a need to maintain fidelity of gradient computations during backpropagation (which is able to achieve machine precision when using automatic differentiation).', 'Models can also be tailored to make them more compute-efficient. Model architectures widely deployed on mobile devices such as MobileNetV1 and MobileNetV2 are good examples. These are essentially convolutional neural networks that have recast the convolution operation to make it more compute-efficient. This more efficient form of convolution is known as depthwise separable convolution. Architectures can also be optimized for latency using hardware-based profiling and neural architecture search, which are not covered in this article.', 'The ability to run machine learning models on resource-constrained devices opens up doors to many new possibilities. Developments may help to make standard machine learning more energy-efficient, which will help to quell concerns about the impact of data science on the environment. In addition, tinyML allows embedded devices to be endowed with new intelligence based on data-driven algorithms, which could be used for anything from preventative maintenance to detecting bird sounds in forests.', 'While some machine learning practitioners will undoubtedly continue to grow the size of models, a new trend is growing towards more memory-, compute-, and energy-efficient machine learning algorithms. TinyML is still in its nascent stages, and there are very few experts on the topic. I recommend the interested reader to examine some of the papers in the references, which are some of the important papers in the field of tinyML. This space is growing quickly and will become a new and important application of artificial intelligence in industry within the coming years. Watch this space.', '[1] Hinton, Geoffrey & Vinyals, Oriol & Dean, Jeff. (2015). Distilling the Knowledge in a Neural Network.', '[2] D. Bankman, L. Yang, B. Moons, M. Verhelst and B. Murmann, “An always-on 3.8μJ/86% CIFAR-10 mixed-signal binary CNN processor with all memory on chip in 28nm CMOS,” 2018 IEEE International Solid-State Circuits Conference — (ISSCC), San Francisco, CA, 2018, pp. 222–224, doi: 10.1109/ISSCC.2018.8310264.', '[3] Warden, P. (2018). Why the Future of Machine Learning is Tiny. Pete Warden’s Blog.', '[4] Ward-Foxton, S. (2020). AI Sound Recognition on a Cortex-M0: Data is King. EE Times.', '[5] Levy, M. (2020). Deep Learning on MCUs is the Future of Edge Computing. EE Times.', '[6] Gruenstein, Alexander & Alvarez, Raziel & Thornton, Chris & Ghodrat, Mohammadali. (2017). A Cascade Architecture for Keyword Spotting on Mobile Devices.', '[7] Kumar, A., Saurabh Goyal, and M. Varma. (2017). Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things.', '[8] Zhang, Yundong & Suda, Naveen & Lai, Liangzhen & Chandra, Vikas. (2017). Hello Edge: Keyword Spotting on Microcontrollers.', '[9] Fedorov, Igor & Stamenovic, Marko & Jensen, Carl & Yang, Li-Chia & Mandell, Ari & Gan, Yiming & Mattina, Matthew & Whatmough, Paul. (2020). TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids.', '[10] Lin, Ji & Chen, Wei-Ming & Lin, Yujun & Cohn, John & Gan, Chuang & Han, Song. (2020). MCUNet: Tiny Deep Learning on IoT Devices.', '[11] Chen, Tianqi & Moreau, Thierry. (2020). TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.', '[12] Weber, Logan, and Reusch, Andrew (2020). TinyML — How TVM is Taming Tiny.', '[13] Krishnamoorthi, Raghuraman. (2018). Quantizing deep convolutional networks for efficient inference: A whitepaper.', '[14] Yosinski, Jason & Clune, Jeff & Bengio, Y. & Lipson, Hod. (2014). How transferable are features in deep neural networks?.', '[15] Lai, Liangzhen & Suda, Naveen & Chandra, Vikas. (2018). CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs.', '[16] Chowdhery, Aakanksha & Warden, Pete & Shlens, Jonathon & Howard, Andrew & Rhodes, Rocky. (2019). Visual Wake Words Dataset.', '[17] Warden, Pete. (2018). Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.', '[18] Zemlyanikin, Maxim & Smorkalov, Alexander & Khanova, Tatiana & Petrovicheva, Anna & Serebryakov, Grigory. (2019). 512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU. 2493–2500. 10.1109/ICCVW.2019.00305.']"
10/2020,Python 3.9,"What’s new, and what’s next?",2.6K,4,https://towardsdatascience.com/@jamescalam,https://towardsdatascience.com/python-3-9-9c2ce1332eb4?source=collection_archive---------4-----------------------,5,5,"['Python 3.9', 'Python’s New Path', 'New Features', 'Plenty to Look Forward to', 'Resources']",38,"['The full release of Python 3.9 is out!', 'It’s clear that this version marks a breaking point from the old route of Python’s evolution, onto a new path. We’ll cover:', 'Let’s explore these new features and understand where Python is heading.', 'There are two significant changes in this update, which we won’t see any immediate impact from — but we will begin to notice a slightly different evolution of Python as a language.', 'In short, this boils down to:', 'Around 30 years ago, Guido van Rossum wrote pgen. One of the first pieces of code ever written for Python — it is still in use as Python’s parser to this day [1].', 'Pgen uses a variant of LL(1)-based grammar. This means our parser reads the code top-down, left-to-right, with a lookahead of just one token.', 'This essentially means that Python development is limited, because:', 'These attributes of the LL(1)-based parser limit what is possible in Python.', 'Python 3.9 has broken from these limitations thanks to a shiny new PEG parser, outlined in PEP 617.', 'Immediately, we won’t notice this. No changes taking advantage of the new parser will be made before Python 3.10. But after that, the language will have been released from it’s LL(1) shackles.', 'Before 3.9, Python releases were scheduled in 18-month release schedules. Now, we’re seeing a move to 12-month release schedules [PEP 602].', 'Rather than seeing a new version of Python once every year and a half, we will now see them yearly. This means:', 'So in essence, what we are seeing here is a focus on smaller, incremental changes in a 12-month cycle — rather than bigger changes every 18-months. At the same time, development velocity is expected to stay the same.', 'Alongside these behind-the-scenes changes, we also get to see some new Python features!', 'Way back in 2008, Python 3 introduced function annotations — the precursor of type hinting. It wasn’t particularly robust, but it was a start.', 'Following this, more features were added over time. But now, 3.9 brings all of these different features together with a tidy new syntax to produce the newest development to Python type hinting.', 'We can easily specify the expected data types of our variables. If we then write something that doesn’t make sense (like we pass a string to an integer) then our editor will flag the issue.', 'No errors will be raised (unfortunately), but it’s incredibly useful when working with complex code bases. Let’s take a look at the new syntax.', 'In Python, adding two strings together with + is absolutely valid. So, in the case of this add_int function receiving two strings, no error would be raised.', 'With the new type hinting functionality, we simply add : int to our parameter in the function definition and our editor will immediately notice the error.', 'We can use the -> type syntax to determine the type of the value output by our function too.', 'And we’re not restricted to simple, predefined types either!', 'Maybe not as flashy as the other changes, but I see it being used a lot. We have two new methods for removing string prefixes and suffixes:', ""[Out]: 'o bar'"", ""[Out]: 'foo b'"", 'We now have two new operators for performing dictionary unions.', 'The first is the merge operator |:', ""[Out]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}"", 'And the update operator, which performs the merge in-place:', ""[Out]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}"", 'There’s plenty to look forward to in Python’s future as a language. It will be fascinating to see how the language evolves with the new release schedule and parser.', 'Head over here for the download link!', 'If you have any questions or suggestions, let me know on Twitter or in the comments below.', 'Thanks for reading!', '[1] Guido van Rossum, PEG Parsers (2019).', 'If you’re interested in Python — you may like my guide to deploying Python APIs onto Google Cloud with Docker:', '*All images have been produced by the author, except where stated otherwise.']"
10/2020,How to spot a data charlatan,Tips for identifying fakers and neutralizing their snake oil,2.1K,26,https://towardsdatascience.com/@kozyrkov,https://towardsdatascience.com/how-to-spot-a-data-charlatan-85785c991433?source=collection_archive---------5-----------------------,8,14,"['How to spot a data charlatan', 'Data charlatans are everywhere', 'Different disciplines', 'Peddlers of hindsight', 'True statisticians always call their shots', 'Analysts produce inspiration', 'Hiding behind fancy explanations', 'Charlatan-proofing your life', 'The same rule applies to ML/AI', 'Call your statistical shots or stay humble', 'Advice for data science professionals', 'Advice for leaders', 'More bad tricks', 'Summary']",47,"['You might have heard of analysts, ML/AI engineers, and statisticians, but have you heard of their overpaid cousin? Meet the data charlatan!', 'Attracted by the lure of lucrative jobs, these hucksters give legitimate data professionals a bad name.', '[In a hurry? Scroll down for a quick summary at the bottom.]', 'Chances are that your organization has been harboring these fakers for years, but the good news is that they’re easy to identify if you know what to look for.', 'Data charlatans are so good at hiding in plain sight that you might even be one without even realizing it. Uh-oh!', 'The first warning sign is a failure to understand that analytics and statistics are very different disciplines. I’ll give you a brief overview in the next section, but if you’d like to understand it more deeply, I’ve written a whole article here.', 'While statisticians are trained in inferring what’s beyond their data, analysts are trained in exploring the contents of their dataset. In other words, analysts make conclusions about what’s in their data while statisticians make conclusions about what isn’t.', 'Analysts help you come up with good questions (hypothesis generation) while statisticians help you get good answers (hypothesis testing).', 'There are also fancy hybrid roles who are able to wear both hats… but they don’t wear both hats in the same moment. Why not? A core principle of data science is that if you’re dealing with uncertainty, it’s not valid to use the same datapoint for both hypothesis generation and for testing. When you have limited data, uncertainty forces you to choose between statistics or analytics. (Find my explanation here.)', 'Without statistics, you’re stuck unable to know whether the opinion you just formed holds water.', 'Without analytics, you’re flying blind with little opportunity to tame your unknown unknowns.', 'That’s a tough choice! Do you open your eyes to inspiration (analytics) while vowing to forgo the satisfaction of knowing whether your newfound opinion holds water? Or do you break into a cold sweat praying that the question you’ve chosen to ask — by meditating alone in a broom closet without any data — is worth the rigorous answer (statistics) you’re about to get for it?', 'The charlatan’s way out of this bind is to ignore it, finding Elvis’s face in a potato chip and then pretending to be surprised that the same chip looks Elvis-like. (The logic of statistical hypothesis testing boils down to asking whether our data surprise us enough to change our minds. How could we be surprised by data if we’ve seen them already?)', 'Whenever charlatans find a pattern, get inspired, then test the same data for that same pattern to publish the result with a legitimizing p-value or two next to their theory, they’re effectively lying to you (and maybe to themselves too). That p-value has no meaning unless you committed to your hypothesis BEFORE you looked at your data.', 'Charlatans mimic the actions of analysts and statisticians without understanding the reasons for them, giving the entire field of data science a bad reputation.', 'Thanks to the statistics profession’s near-mystical reputation for rigorous reasoning, snake oil sales in data science are at an all-time high. It’s easy to cheat this way without getting caught, especially if your unsuspecting victims think that it’s all about the equations and data. A dataset is a dataset, right? Wrong. How you use it matters.', 'A dataset is a dataset, right? Wrong. How you use it matters.', 'Luckily for their would-be marks, you only need one clue to catch them: charlatans peddle hindsight.', 'A charlatan peddles hindsight — mathematically rediscovering phenomena that they already know to be in the data — while a statistician offers tests of foresight.', 'Unlike charlatans, good analysts are paragons of open-mindedness, always pairing inspirational insights with reminders that there could be many different explanations for the observed phenomena, while good statisticians are careful to call their shots before they take them.', 'Good analysts are paragons of open-mindedness . Unlike charlatans, they don’t make conclusions beyond their data.', 'Analysts are exempt from calling their shots… as long as they aren’t reaching beyond their data. If they’re tempted to make claims about things they haven’t seen, that’s a different job. They should take off their analyst hat and put on their statistician helmet. After all, whatever your official job title, there’s no rule that says you can’t learn both trades if you want to. Just don’t get them confused.', 'Being good at statistics does not mean you’re good at analytics and vice versa. If anyone tries to tell you otherwise, check your pockets. If that person tells you that you are allowed to do statistical inference on data you’ve already explored, check your pockets twice.', 'If you observe data charlatans in the wild, you’ll notice that they love to spin fancy stories to “explain” observed data. The more academic-sounding, the better. Nevermind that these stories only (over)fit the data in hindsight.', 'When charlatans do that — let me not mince words — they’re bullshitting. No amount of equations or pretty pontification can make up for the fact that they’ve offered exactly zero evidence that they knew what they were talking about beyond their data.', 'Don’t be impressed by how fancy their explanation is. For it to be statistical inference, they’d have to call their shots before they see the data.', 'It’s the equivalent of showing off their “psychic” powers by first peeking at the hand you’ve been dealt and then predicting that you’re holding… whatever you’re holding. Brace yourself for their novel on how it was your facial expression that gave it away. That’s hindsight bias and the data science profession is stuffed to the gills with it.', 'When there’s not a lot of data to go around, you’re forced to choose between statistics and analytics.', 'Data-splitting is the cultural quick fix everyone needs.', 'Luckily, if you have plenty of data, you have a beautiful opportunity to avail yourself of analytics and statistics without cheating. You also have the perfect protection against charlatans. It’s called data-splitting and in my opinion it’s the most powerful idea in data science.', 'Never take an untested opinion seriously. Instead, use a stash of test data to find out who knows what they’re talking about.', 'To protect yourself against charlatans, all you have to do is make sure you keep some test data out of reach of their prying eyes, then treat everything else as analytics (don’t take it seriously). When you’re faced with a theory you’re in danger of buying into, use it to call the shot, and then open your secret test data to see if the theory is nonsense. It’s as easy as that!', 'This is a big cultural shift from what people were used to in the era of “small data” where you have to explain how you know what you know in order to convince people — flimsily — that you might indeed know something.', 'Some charlatans posing as experts in ML/AI are easy to spot. You catch them the same way you’d catch any other bad engineer: the “solutions” they attempt to build repeatedly fail to deliver. (An earlier warning sign is lack of experience with industry-standard programming languages and libraries.)', 'But what about the folks who produce systems that seem to work? How do you know if there’s something fishy going on? The same rule applies! The charlatan is sinister character who shows you how well their model performed… on the same data they used to make the model. *facepalm*', 'If you’ve built a crazy-complicated machine learning system, how do you know if it’s any good? You don’t… until you show that it works on new data it hasn’t seen before.', 'It’s hardly a *pre*diction if you’ve seen the data before making it.', 'When you have enough data to split, you don’t need to hand-wave at the prettiness of your formulas to justify your project (which is still an old-fashioned habit I see everywhere, not just in science). You can say, “The reason I know it works is that I can take a dataset that I haven’t seen before and I can accurately predict what’s going to happen there… and be right. Over and over.”', 'Testing your model/theory in new data is the best basis for trust.', 'To paraphrase a quip by economist Paul Samuelson:', 'Charlatans have successfully predicted nine out of the last five recessions.', 'I have no patience for data charlatans. Think you “know” something involving Elvis-like potato chips? I couldn’t care less how well your opinion fits your old chips. I’m not impressed by how fancy your explanation is. Show me that your theory/model works (and keeps working) in a whole pile of new chips you’ve never seen before. That’s the true test of your opinion’s mettle.', 'Data science professionals, if you want to be taken seriously by anyone who understands the humor here, stop hiding behind fancy equations to prop up your human biases. Show us what you’ve got. If you want those who “get it” to treat your theory/model as more than a bit of inspiring poetry, have the guts to do the grand reveal of how well it works on a brand new dataset… in front of witnesses!', 'Leaders, refuse to take any data “insights” seriously until they’ve been tested on new data. Don’t feel like putting in the effort? Stick with analytics, but don’t lean on those insights — they’re flimsy and haven’t been checked for trustworthiness. Additionally, when your organization has data in abundance, there is no downside to making splitting a core part of your data science culture and even enforcing it at the infrastructure level by controlling access to test data earmarked for statistics. It’s a great way to nip snake oil sales attempts in the bud!', 'If you’d like to see more examples of charlatans up to no good, this Twitter thread is wonderful.', 'When data are too scarce to split, only a data charlatan tries to follow inspiration with rigor, peddling hindsight by mathematically rediscovering phenomena that they already know to be in the data and calling their surprise statistically significant. This distinguishes them from the open-minded analyst who deals in inspiration and the meticulous statistician who offers proof of foresight.', 'When data are plentiful, get in the habit of data-splitting so you can have the best of both worlds without cheating! Be sure to do analytics and statistics separately on separate subsets of your original pile of data.']"
10/2020,Go Programming Language for Artificial Intelligence and Data Science of the 20s,Opinion,1.5K,29,https://towardsdatascience.com/@skdasaradh,https://towardsdatascience.com/golang-ai-programming-language-for-the-20s-71890baa8c47?source=collection_archive---------6-----------------------,4,6,"['Go Programming Language for Artificial Intelligence and Data Science of the 20s', 'Reason for Python’s Popularity', 'Problems with Python', 'Why Go Language?', 'What makes Go suitable for AI Research', 'References:']",35,"['30 years ago, Python made its first appearance. But It took 20 years to gain appreciation from the developers. Fast-forward to 2019, it became the 2nd most loved language among developers.¹', 'Its growth over the past has been huge, especially over the past 5 years. Python became the machine learning and data science developers’ go-to language.', 'Python’s dominance in these fields will certainly be huge for the next few years. But it has got some serious disadvantages when compared to newer languages. This could be a roadblock for developers of the 20s.', 'This is the right time to examine the problems of Python and replacing it with a better alternative. In the case of AI development and Data Science, our next go-to language may be the Golang.', 'The major reason for Python’s popularity is — it’s easy to learn. Its syntax is simple compared to other languages and anyone can learn the basics of Python in a few hours or days.', 'Even after learning other languages such as C++ or Java, developers often prefer to stick to Python. That’s because there is a python library for almost everything one can ask for.', 'Libraries and simple syntax made developing software in Python, simple and productive. These advantages made Python the language of beginners.', 'Python is an interpreted language, which causes Python’s biggest problem — Slow execution. Execution in Python is slow, very slow compared to other compiled languages such as C++ and Go.', 'Python is a dynamically typed language. The data type of the variables is assigned automatically during the run-time. This makes the execution much slower.', 'To overcome Python’s slowness libraries such as Tensorflow, Numpy, and Pandas are partially written in C or C++. They help to increase the execution speed significantly.', 'Basically, Python needs the help of other languages to overcome its problem.', 'Python’s Global Interpreter Lock (GIL)² allows only one thread to execute at a time while improving single-threaded performance. Python’s multithreading doesn’t truly have multiple threads running at the same time.', 'In reality, Python cannot do true multithreading.', ""(Note: Non-CPython implementations of Python such as Jython and IronPython doesn't have GIL)"", 'Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.', '‘Go’ made its first appearance 10 years ago. It was developed at Google as a general-purpose language.', 'It has several advantages over Python and other programming languages, and that’s why we’re interested.', ""Go is statically typed and compiled language. This means the execution will be several times faster than Python. Unlike Python, Go doesn't need the help of other languages to be faster."", 'Here is a small benchmark game comparison between Go and Python. Go is nearly fast as C++ and Java. Also, Go compiles the code extremely faster than C++ and Java.', 'Go’s syntax is simple and similar to C. It is an easy-to-learn programming language, particularly if someone already knows the basics of C or Java language.', 'To learn the basics of Go, visit A Tour of Go for an interactive tour or visit official docs.', 'Go is the 2020’s most sought-after programming language according to this Hackearth survey of 16,000+ developers from across 76 countries.', ""It shouldn't be surprising that 32% of experienced developers and 29% of students say they want to learn Go."", 'Concurrency is one of Go’s main strengths. Go has Goroutines³ to achieve concurrency. Goroutines are functions that can run simultaneously and independently.', 'Goroutines are light-weight and take up only 2 kB of memory. As Goroutines are lightweight, it is possible to have thousands of them running at the same time.', 'There are lots of other advantages in Go. Check this in-depth article by Keval Patel.', 'There’s nothing really special about using Go specifically for AI Research or Data Science. Why should Go be used for AI and Data Science? I’ll tell you… But just a minute,', '30 years ago, Python was not developed to build Machine Learning or Deep Learning Algorithms or to make data visualizations.', 'It is what it is today because the developers and students loved to code in Python and the language supported to develop what the developers intended to.', 'Go aims to make programmers more productive. It has several advantages compared to Python. It can easily succeed Python as the most-loved and most-popular language in upcoming years.', 'So, it boils down to just 1 line,', 'Go can support ‘developers of the 20s’ to develop their ideas much better than any other language.', '[1] : Stackoverflow Developer Survey Results (2019), https://insights.stackoverflow.com/survey/2019#most-loved-dreaded-and-wanted', '[2] : Global Interpreter Lock (GIL), https://wiki.python.org/moin/GlobalInterpreterLock', '[3] : Goroutine, https://tour.golang.org/concurrency/1']"
10/2020,4 Reasons Why You Shouldn’t Be a Data Scientist,Why a data science job might not be the right fit for…,796,14,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/4-reasons-why-you-shouldnt-be-a-data-scientist-e3cc6c1d50e?source=collection_archive---------7-----------------------,4,6,"['4 Reasons Why You Shouldn’t Be a Data Scientist', '1. You don’t like the idea of having to constantly learn throughout your career.', '2. You don’t enjoy “dirty work”.', '3. You don’t like the idea of continuously negotiating, communicating, and educating with business stakeholders.', '4. You don’t consider yourself as a resourceful person.', 'Thanks for Reading!']",21,"['I’m sure most of you have seen HBR’s article, “Data Scientist: The Sexiest Job of the 21st Century,” and they’re not the only ones to have claimed that data science is the holy grail of all jobs.', 'A lot of people seem to have this false notion that a data scientist’s job consists only of building these revolutionary, intricate, and impactful machine learning models day and night, and this is simply not true.', 'As many of you seek to find a career that suits your interests, or for those who are looking to make a career switch, you might have pondered on the thought of becoming a data scientist. And since you’ve probably only heard about the good parts of data science, I’m going to give you 4 reasons why a data science job might not be for you.', 'Let’s dive into it!', 'Data science is an extremely broad term, which means that it means different things for different companies, and so, different companies desire different skillsets. For example, some companies desire machine learning knowledge others desire experimental design and A/B testing, some desire Python programmers and others desire R programmers, etc… Therefore, the skillset you develop at one company won’t necessarily carry you through your entire career.', 'On top of that, data science is a multidisciplinary job. You’re expected to have a certain level of knowledge in programming, statistics, mathematics, business understanding, etc… which means there you’ll ALWAYS have room to grow in one of these areas.', 'Lastly, like everything in tech, data science is constantly evolving. For example, it was only in 2015 that TensorFlow was created, and now, it’s one of the most in-demand skills. As new technologies are created and iterated to be better, you’ll be expected to learn new technologies as well.', 'Working on the fun stuff like building machine learning models makes up a small fraction of a data scientist’s job — I’d argue less than 25% of one’s time.', 'A lot of your time will most likely be spent on understanding the data, where it’s coming from and how it’s being manipulated), as well as preparing the data (EDA, data wrangling, feature engineering, etc.). And if you don’t have the data that you need to develop a good model, you’ll have to develop the pipelines to be able to get the data that you need.', 'And there’s a simple reason for this. The performance of your model is limited by the quality of data that was used to create it.', '“Garbage in, garbage out.”', 'Thus, if the “dirty work” isn’t something that you’re willing to do in order to work on machine learning models, then data science might not be the best route to take.', 'As a data scientist, you’re as much of a sales rep as you are a data scientist:', 'More often than not, you’ll be working with stakeholders who aren’t as tech-savvy and may not understand the projects that you are working on. Therefore, it’s your job to communicate and educate your co-workers and stakeholders in a manner that is digestible for them.', 'As well, you also will be required to break down your data science projects into steps. Business stakeholders care about progress and they want to see how a project is improving over time rather than waiting 5 months for the final result. Thus, you have a responsibility to communicate your progress as well as your outcome(s).', 'There isn’t much hand-holding when it comes to data science. It’s your responsibility to get the job done at the end of the day, and if you find yourself getting stuck, you need to be resourceful and find the answer(s) to your problem(s).', 'If you need to learn about how XGBoost works or how to build an API, you need to be resourceful and watch YouTube videos, read documentation, or work through a Bootcamp to figure it out.', 'As well, you also have to take the initiative to schedule meetings with relevant stakeholders to learn more about the business problem, the desired outcome, and relevant metrics.', 'If you’re not the type to be resourceful, take initiative, and persist, then data science might not be for you.', 'While this is an opinionated article, I hope that this sheds a bit of light into what data science is. At the end of the day, I still think the pros far outweigh the cons, but I think these are good pointers to think about and consider. As always, I wish you the best of luck in your endeavors!', 'Not sure what to read next? I’ve picked another article for you:']"
10/2020,A Learning Path To Becoming a Data Scientist,The 10 steps roadmap to kickstarting your data science…,1.2K,8,https://towardsdatascience.com/@saraametwalli,https://towardsdatascience.com/a-learning-path-to-becoming-a-data-scientist-56c5c2e8ae3f?source=collection_archive---------8-----------------------,8,11,"['A Learning Path To Becoming a Data Scientist', 'Step №1: Programming', 'Step №2: Databases', 'Step №3: Math', 'Step №4: Version Control', 'Step №5: Data Science Basics', 'Step №6: Machine Learning Basics', 'Step №7: Time Series and Model Validation', 'Step №8: Neural Networks', 'Step №9: Deep Learning', 'Step №10: Natural language Processing']",60,"['Data science is one of the rapidly growing fields that demand a data scientist growing up daily. As of October 2020, I can’t see this demand slowing down anytime soon. It is an interdisciplinary field that can help us analyze the data around us to make our life better and our future brighter.', 'Luckily, becoming a data scientist does not require a degree. As long as you are open to learning new things and willing to put in the effort and time, you can become a data scientist.', 'The question now is, where to start?', '“The beginning is perhaps more difficult than anything else, but keep heart, it will turn out all right.”', '― Vincent van Gogh', 'The internet is full of tutorials about all the details of every data science aspect, such as machine learning basics, natural language processing, speech recognition, and all kind of amazing data science magic.', 'But,', 'For a beginner, the amount of information can be overwhelming and lead someone to give up before they even start.', 'What could help is having a structured roadmap that clearly lays out what you need to learn and the order that you should learn to become a data scientist.', 'In this article, I will layout a 10 steps roadmap from start to finish of concepts you need to cover along your data science learning journey.', 'If you’re new to the technical field, then programming would be the best place to start. Currently, the two programming languages used most in data science are Python and R.', ""Because Python is a beginner-friendly programming language, I find it a great place to start with data science and maybe more fields in the future. Due to Python's popularity, there are many resources available to learn it independently of your goal application field."", 'Some of my favorite Python learning resources are CodeAcademy, Google Classes, Learn Python the Hard Way.', 'However, if you decide to go with R, both Coursera and edX have great courses that you can audit for free.', 'Some of you might already know how to program and might be transferred to data science from another technical field. In that case, you can skip this step and move forward to the next step of the journey.', 'The heart of data science is data. You can think of data science as the art of telling a story using data.', 'Whenever you work on a data science project, you will need to have data to analyze, visualize, and build a valid project. This data are often stored in some database.', 'An essential step to standing out as a data scientist is to interact and communicate with databases effectively. If you could design a simple database, then this will take you to the next level.', 'To communicate with a database, you will need to speak its language. That is SQL. SQL stands for Structured Query Language and is used to communicate with a database.', 'My favorite resources to learn SQL are CodeAcademy, Khan Academy, and interactive learning, SQLCourse.', 'The core of data science is math. To understand how the different concepts of data science function, you need to have a basic understanding of the math behind them.', 'I know math is one thing that could make some backup from pursuing a career in data science.', 'But,', 'You need to understand the basics of probability theory, statistics, and linear algebra to comprehend data science. However, most tools you would use in your career will eliminate implementing the math itself in your projects.', 'So, you need to understand how it works, how, and when to use it.', 'Don’t let math intimidate you from exploring the world of data science. I would say it’s well worth it. There are some helpful materials on Coursera that can help you tackle the math you need.', 'In software development in general and data science, one of the most important concepts to master — or try to — is version control.', 'Whenever you work on a data science project, you will need to write different code files, explore datasets, and collaborate with other data scientists. Manging, all changes in the code, is done via version control, namely, using Git.', 'Git is a version control system used to track changes in source code during the software development process. Git was built to coordinate work among a group of programmers, or to be used to track changes in any set of files by a single programmer.', 'Although Git is a system, some websites allow you to use Git easily without needing to interact much with the command line — you will eventually move to the command line eventually, though — such as GitHub or GitLab.', 'Luckily, there are many resources to help you understand the inner functionality of Git; my top choices are BitBucket Learn Git Tutorials and this lecture from the Harvard CS50 course.', 'Data science is an extensive term; it includes different concepts and technologies. But before you take a deep dive into the big sea of data science, you need to familiarize yourself with some basics first.', 'There are important skills you need to develop and work on to become a successful data scientist, for example:', 'So, you worked on your programming skills, brushed up your math, and dived in databases. You’re now ready to start the fun part, applying what you learned so far to build your first projects.', 'Machine learning basics is the place to start. Here is when you start learning and exploring basic machine learning algorithms and techniques, such as linear and logistic regression, decision trees, Naive Bayes, and support vector machines (SVM).', 'Here where you also start discovering the different Python or R packages to deal and implement your data. You will get to use Sciket-learn, Scipy, and Numpy.', 'You will learn how to clean up your data to have more accurate positions and results. This is part where you’ll get to experience what you can do with data science and will be able to see the impact the field has on our daily lives.', 'The best place to start learning about the different aspects of machine learning is the various article on Towards Data Science.', 'It’s time to dive deeper into machine learning. Your data is not going to be stationary; it’s often related to time somehow. Time series are data points ordered based on time.', ""Most commonly, time series are sequences of data taken at successive equally spaced points in time. Making them discrete-time data. Time series shows you how time changes your data. This allows you to gain insights about trends, periodicity in the data, and predict the data's future behavior."", 'When dealing with time series, you will need to work on two main parts:', ""Building models to predict future behavior is not enough; you need to validate this model's correctness. Here where you will learn how to build and test models efficiently."", 'Moreover, you will learn how to estimate the threshold of error for each project and how to keep your models within the acceptable ranges.', 'Neural networks (Artificial Neural Networks or ANN) are a biologically-inspired programming paradigm that enables a computer to learn from observational data.', ""ANNs started as an approach to mimic the human brain's architecture to perform different learning tasks. For an ANN to resemble the human brain, it was designed to contain the same components a human cell has."", 'So, ANN contains a collection of neurons; each neuron represents a node connected to another via links. These links correspond to the biological axon-synapse-dendrite connections. Moreover, each of these links has a weight that determines the strength one node has on another.', 'Learning ANN enables you to tackle a wider range of tasks, including recognizing handwriting, pattern recognition, and face identification.', 'ANN represent the basic logic you need to know to proceed to the next step in your data science journey, deep learning.', 'Neural networks are paradigms that power deep learning. Deep learning represents a powerful set of techniques that harness the learning power of neural networks.', 'You can use neural networks and deep learning to tackle the best solutions to many problems in various fields, including image recognition, speech recognition, and natural language processing.', 'By now, you’ll be familiar with many Python packages that deal with different aspects of data science. In this step, you will get the chance to try popular packages such as Keras and TensorFlow.', 'Also, by this step, you will be able to read recent research advances in data science and maybe develop your own.', 'You’re almost at the end. You can already see the finish sign. You have gone through many theoretical and practical concepts so far, from simple math to complex deep learning concepts.', 'So, what’s next?', 'My personal favorite sub-field of data science, which is natural language processing (NLP). Natural language processing is an exciting branch that enables you to use the power of machine learning to “teach” the computer to understand and process human languages.', 'This will include speech recognition, text to speech application — and vise versa — virtual assistance (like Siri and BERT), and all kinds of different conversational bots.', 'Conclusion', 'Here we are at the “end” of the road. End here between quotation, because just like any other technology-related field, there’s no end. The field is developing rapidly; new algorithms and techniques are under research as I type this article.', 'So, being a data scientist means you will be in a continuous learning stage. You will be developing your knowledge and your style as you go. You will probably feel more attracted to a specific sub-field than another and dig even deeper and maybe specialize in that sub-field.', 'The most important thing to know as you embark on this journey is, you can do it; you need to be open-minded and dedicate enough time and effort to achieve your end goals.']"
10/2020,How I became a Software Developer during the pandemic without a degree or a bootcamp,-,835,5,https://towardsdatascience.com/@federicomannucci_31459,https://towardsdatascience.com/how-i-became-a-software-developer-during-the-pandemic-without-a-degree-or-a-bootcamp-ef7a4184efde?source=collection_archive---------9-----------------------,14,1,['How I became a Software Developer during the pandemic without a degree or a bootcamp'],54,"['In 2018 I was depressed and unmotivated, I thought of myself as a failure and I thought I was too dumb to finish my degree or learn anything at all, I had no direction in life and just wanted everything to be over.Two years later, one spent working abroad and another dedicated to studying, I have a completely different perspective about myself and I just started my new exciting developer job on Monday.', 'It took a lot of courage (and argumentations to convince my parents) to leave my university after three years of studies to accept a job in a Lisbon without knowing anyone nor the language but it was a wonderful experience that helped me find myself.Again it took even more grit and determination to leave Lisbon and start studying again, but I did it because I knew my dream was to become a programmer.', 'I have no expertise in psychology and the best advice I have if you are in a dark place is to seek professional help, but I know what it feels to be lost and I want to help anyone that shares my same dream by writing this article offering actionable advice on how to achieve a career in software development.', 'I have spent several years trying to understand what I wanted to do as an adult and failing miserably at it, I spent a semester studying in Medical school when I realized I didn’t care at all about being a doctor, I attended three years of Engineering courses but I disliked the subjects of my major and in the end I decided I didn’t want to continue with it.', 'I was very lucky to receive a job offer while I was still studying and I immediately decided to take a chance and go for it, I had never really worked full time at the time so I wanted to challenge my fears of not being able to succeed in a professional environment.', 'Fortunately, I ended up absolutely loving my job and all the people I got to know thanks to it, it was actually one of my colleagues that introduced me to coding and got me interested in it.Since then I never looked back, I started dabbling with programming and I soon realized I wanted that to be my career, I incredibly enjoy the challenge and the intellectual stimulation that it can provide and I think everyone should try and see if they feel the same about it.', '… is the slogan of an old Apple campaign, and while I completely agree that anyone can learn the basics of programming, I ask myself, can everyone achieve a career in software development?Definitely not, although the profession is rising in popularity and there are more jobs continuously being created the field is still limited and many people don’t have the aptitude for spending long lonely (and sometimes frustrating) hours in front of a computer, in short not everyone should code.', 'Go elsewhere if you see this as a get rich quick opportunity, without genuinely enjoying writing software you will never get far enough to get a job, being committed for so long without seeing any tangible result and having no external obligation requires a lot of motivation and dedication that not everyone can replicate.', 'Mistrust the so-called experts or gurus that promise you an easy program for learning programming followed by easily finding a six figures salary, that is a complete lie and if someone presents you with this tale you should assume they don’t have your best interest at heart.', 'Learning how to code is a remarkably long and difficult process, it requires you to assimilate a completely different way of thinking and spend countless hours trying to understand difficult topics or debugging applications that aren’t working correctly.', 'Often people forget how difficult it was when they were just beginners, coding can become easy when you have put enough hours into it, but don’t make the mistake to assume that you are not cut for it just because it’s taking you a long time to understand recursion, every person has to learn at their own pace.', 'The answer is of course yes, if you browse the internet is not uncommon to run into stories of people changing their career even in their forties or dropping their education to pursue self-study and land the job they always hoped for, the latter is basically what happened to me, but is it the best way to pursue this career?', 'Let’s start by saying that if I still was nineteen I would no doubt attend university to study computer science, especially in my country Italy where education is cheap, the easiest way to get into the industry is definitely by getting a degree with a couple of internships under your belt.', 'The reason I decided to try the more difficult self learn route was that I already had professional experience and completed a good part of my engineering degree, those elements give me more credibility to the eyes of employees and a solid mathematical foundation to base my studies on.', 'I have no experience of bootcamps but my impression is that they often only provide shallow knowledge of one or two languages and some frameworks but leave giant gaping holes in your understanding of computer science and programming as a whole.Not saying they are all like this, joining one might be a great choice for you but don’t assume that you will leave knowing everything you need to.', 'There is an astounding number of resources to start with and choosing what path to take and which language to choose can be quite daunting, what if I make the wrong choice?', 'The secret is that there is no right or wrong choice, the real way to succeed is committing to something and bringing it to completion, don’t try a new course or tutorial every couple of days because you think you might like it more than what you are currently doing or someone promised you that it is the best out there.', 'That being said when asked what language to begin with I often say Python because I believe it’s the most beginner-friendly and is vast enough to allow you to explore many different specializations, from web development to machine learning, but if you are still unsure I suggest reading this great article to help you make up your mind.', 'The best sites to start are the ones that allow you to code while learning, reading about programming without actually doing it is as likely of making you a programmer as reading cookbooks without cooking is of making you a chef, here are some of my favorites:- FreeCodeCamp;- Codecademy;- Khan Academy;- SoloLearn.', 'The beginning is not difficult, you learn numerous new concepts and get to play a bit with your language of choice, problems arise when you want to do something that you haven’t done before without assistance, being proficient and independent requires far more than just knowing the basic logic and syntax of a programming language.', 'To progress to more complex and interesting topics there are several fantastic options, do you know that you can attend thousands of university courses from the best institutions in the world completely free of charge?There are websites like Class Central that you can use to find the course that interests you the most or you can directly explore other platforms like:- Udemy;- Coursera;- Edx;- MIT Open Courseware;- Plural Sight;- JetBrains Academy;- Udacity;- The Odin Project;- OSSU.', 'But starting a course and watching it passively is no guarantee that you will learn much, the best way to avoid “tutorial hell” and actually become competent is putting everything you learn to good use with projects based on the material you are currently studying.', 'It is not uncommon to feel lost having to choose amongst all this different content, it cannot be otherwise, there are many branches of development and every person will choose which one to pursue based on its interest, but isn’t there some essential material that every self-learner should go through anyway?', 'Whenever someone past the absolute beginner phase asks me, “Where do I go now?”, my answer always is cs50 from Harvard University.', 'There are numerous articles celebrating and reviewing in detail the program so I won’t go into details here, but there are several reasons why I have the highest praises for this course, here’s four:', 'To complete the course and obtain the certificate you will have to finish eight graded problem sets and at the end create a personal project of your choice that shows what you learned during your studies.', 'We are back talking about projects, and for good reasons, besides being vital to learning the subjects you are studying they are your best way to introduce yourself to potential employees, especially if you have no formal experience or education to speak of.', 'About what to exactly do, well the sky is the limit. The thing I love the most about programming is how it allows you to do whatever your imagination pushes you to do as long as you are capable enough (and your google skills are up to the task), this Github repository contains some very interesting ideas to get started with.', 'Speaking of Github, if you already didn’t you should definitely open an account and use it to showcase your favorite creations using a well-formatted Readme file, mine are my custom programming language, and this AI model to predict life expectancy.', 'There are plenty of different options for aspiring programmers and that makes it difficult to choose, are you interest in Web Development? You always aspired to program games? Or would you like to explore rising trends like Cloud Development or Artificial Intelligence?', 'It is a common opinion that Front-end Development has the lowest barrier to entry and beginners are often encouraged to learn it to be able to land a job as fast as possible, although this is sound advice I kinda dislike HTML and especially CSS, knowing I wouldn’t be passionate in it I put my effort in studying backend and I was able to land a job for it despite being more challenging on paper.My take away is that following your passion is more important than choosing the easy route.', 'But before actually setting your mind on a specific goal I encourage you to try as much as possible, I was sure Artificial Intelligence would be my passion when I realized that training models is actually quite boring and sometimes frustrating and that I would much rather work with the data infrastructure or write algorithms.', 'Great advice I received regarding this question is that you will never feel ready for your first job because you probably aren’t.Most companies, at least the good ones, hire juniors knowing well that they will lose them money for months before they are competent enough to bring profit, they are investing in you and committing to your improvement!', 'So don’t wait until you feel you reached perfection, but rather ask a friend who is in the field or even people you don’t know on social media but are willing to help if your portfolio and knowledge are at the level necessary to pass a technical interview.', '“Perfection is the enemy of progress”Winston Churchill', 'Let’s be honest, with this job market is going to be weeks or months before you receive an answer anyway, you might as well start searching and applying early, but not so much that you risk burning bridges by being completely unprepared when an opportunity eventually arrives.', 'There is plenty of advice on how to structure and fill your cv, but there are some that helped me in particular:', 'Finding a position that you’d be interested in is not difficult, hundreds of jobs are being posted every day and the big tech companies are always recruiting, for your applications you can use sites like Monster, Linkedin, Indeed, or the company site itself if you have a particular one in mind.', 'The problem is making a better impression than all the other candidates, you will have to compete with hundreds of people from all around the world, some of them with more experience or better education, to actually reach the interview phase where you will be able to show your passion and preparation.', 'The solution is networking, use your social media like Reddit, Twitter, and Linkedin actively, participate in job fairs and conventions (physical or virtual), try meeting people that work in the sector you aspire to be in and ask them for advice, even a cold message on Linkedin can do wonders if you keep the interaction polite.', 'I actually found my current company because the founder saw my profile on AngelList, a website for finding employment in startups, and liked it enough to message and ask me to send my resume, the fact that we had a personal interaction really helped me in passing the first selection stage.', 'So build your presence online and try to make as many connections as possible, remember it only takes one person giving you an opportunity to finally break into the industry!', 'Technical interviews are notoriously difficult, there is an entire market of books, courses, and websites with problems dedicated to preparing for them, often knowing the ins and outs of a programming language and a couple of frameworks won’t be enough to be successful.', 'Algorithms, Data Structures, and Leetcode style questions have become standard for many companies, especially if you dream of being a Facebook or Google engineer you will have to make sure your knowledge of these topics is excellent, to get started I suggest taking a solid algorithms course and then practicing Leetcode daily.', 'Don’t forget that your interviewers are people! Being able to communicate how you reason is more important than arriving immediately at the right solution, also be likable and confident in behavioral interviews and do your research, companies want people that are committed to working with them.', 'Let’s face it, every one of us is bound to fail in one way or another, just barely, like when in the last interview before getting the job the hr lady I was interacting with decided I didn’t seem motivated enough, or completely, for example when I utterly bombed my technical interview for a C# position for not being prepared enough.', 'I still think about those events but it’s fine because they really helped me understanding important lessons, that how you present yourself and your people skills are as important as your technical ones (!), and motivated me to get serious with my preparation.', 'The same is gonna happen to you, don’t let those important lessons get wasted and keep working hard, one day you will find the right opportunity and all the effort will prove worthwhile.', '“Whenever I get a stack of resumes, I throw half of them in the trash. I sure don’t want unlucky people on my team.”', 'Although this is just a joke I found on Reddit, it’s true that oftentimes all the advice and preparation in the world is not enough to get you where you want to be, and a bit of luck will be needed to reach the goal.', 'When your application gets ignored or you fail an interview remember that it’s often not your fault, being deemed not ready for an opportunity doesn’t mean you aren’t good enough but that you have to keep looking for the right place and the right people that will be able to see your potential.', 'This dream is not an easy one, you will probably spend months and even years before you are ready, there will be moments of doubt and seemingly insurmountable challenges, but in the end, you will be happy looking back.', 'So start coding today, and good luck in your journey!', 'Thanks for reading so far, leave a clap if you liked the post, and feel free to connect with me on Linkedin.']"
11/2020,I created my own YouTube algorithm (to stop me wasting time),Using the YouTube API and AWS Lambda,2.4K,31,https://towardsdatascience.com/@chris-lovejoy,https://towardsdatascience.com/i-created-my-own-youtube-algorithm-to-stop-me-wasting-time-afd170f4ca3a?source=collection_archive---------0-----------------------,9,10,"['I created my own YouTube algorithm (to stop me wasting time)', '🚀 Escaping the YouTube algorithm', '🗺️ The best laid plans', '⛵️ Navigating the YouTube API', '\U0001f9ee Finding valuable videos: defining the formula', '🔬 Testing my new tool', '🖋 Setting up the workflow', '🔖 Closing thoughts', '👣 Potential next steps', '🔗 Other useful links']",55,"['I love watching YouTube videos that improve my life in some tangible way. Unfortunately, the YouTube algorithm doesn’t agree. It likes to feed me clickbait and other garbage.', 'This isn’t all that surprising. The algorithm prioritises clicks and watch time.', 'So I set out on a mission: Can I write code that will automatically find me valuable videos, eliminating my dependence on the YouTube algorithm?', 'Here’s how it went.', 'I started by visualising what I wanted the tool to do. I wanted something that would (i) rank videos based on likely relevance for me and (ii) automatically send me suggested videos, which I could select from.', 'I figured I could make some major productivity gains if I could batch-decide the videos I was going to watch each week and eliminate infinity scrolling YouTube browsing.', 'I knew I’d need the YouTube API to get video information (what’s an API?). I’d then create a formula which processed that information to rank videos. For the final step, I planned to set up an automated email to myself using AWS Lambda, which would list the top-ranked videos.', 'That’s not exactly how it ended up, though.', '(If you want to skip the story and see the final code, click here.)', 'I wanted to find metrics that I could use to rank videos in terms of their likely interest to me.', 'I read through YouTube’s documentation here and saw that you can get information at the level of videos (title, when published, how many views, the thumbnail, etc) and at the channel level (number of subscribers, comments, views, channel playlists, etc).', 'Seeing this, I was pretty confident I could use this to define a metric and rank videos.', 'I obtained an API key through the developer console here and copied it into my Python script.', 'This enables you to initialise an API call and retrieve results with the following lines of code:', 'This would return a JSON object, which I could parse to find the appropriate information. For example, to find the date published, I could index results as follows:', 'There’s a helpful video series here which walks you through the process of using the YouTube API.', 'Now that I could query the appropriate information, I needed to use the values obtained to rank videos in terms of their interest to me.', 'This was a tricky one. What makes a good video? Is it the view count? The number of comments? The number of subscribers of the channel?', 'I decided to start with total view count, as a reasonable first-degree proxy of how valuable the video would be. In theory, videos that are interesting or well-explained will gain positive audience feedback, get promoted more and thus have more views.', 'However, there are a few things that total view count doesn’t take into account:', 'Firstly, if a channel has built up a large audience, then it will be much easier to get a comparable level of views compared to a smaller channel. Some of this may reflect more experience leading to better videos, but I didn’t want to discount potentially high-quality videos from smaller channels. A 100,000 view video from a channel with 10,000 subscribers is probably better than a 100,000 view video from a 1 million subscriber channel.', 'And secondly, videos can get lots of views for the wrong reasons, such as clickbait titles or thumbnails, or being controversial. I’m personally less interested in these types of video.', 'I needed to incorporate other metrics. The next one was subscriber count.', 'I tested ranking based solely on the view-to-subscriber ratio (ie. by dividing views by number of subscribers).', 'When I looked at the results, some of them looked promising. However, I did notice a problem: For videos with really small subscriber counts, the score would get heavily amplified and surface to the top.', 'I took some efforts to remove these negative edge cases:', 'I played around with various thresholds and these ones seemed to filter out these low-sub low-view videos pretty well. I tested the code on a few different topics and was starting to get pretty decent results.', 'However, there was another problem I noticed: Videos that had been published longer ago had a higher chance of getting more views. They simply had longer to accumulate them.', 'My plan was to run this code once a week, so I decided to restrict the search to videos published in the last 7 days.', 'I also added ‘days since published’ into the ranking metric. I decided to divide the previous score by the number of days, so that the final metric was proportionate to how long the video had been out for.', 'I tested my code further, and found I was quite consistently identifying great videos that I wanted to watch. I played around with different variations and weighting of different components of my formula, but I found it to be an inexact science, so I settled on the following formula which I found balanced simplicity with effectiveness:', 'First, I tested using the query term ‘medical school’. I got the following results:', 'I then went to YouTube and manually searched for videos related to medicine and medical school. I found that my tool had captured all the ones I’d be interested in watching. In particular, the second video by a doctor called Kevin Jabbal was one that enjoyed.', 'I tested on another search term; ‘productivity’, and was again pretty happy with the results:', 'The second video was a slightly rogue one — and not the type of video I was looking for. But I couldn’t think of a simple way to screen out these videos, which were picked up due to an alternative meaning of the search term.', 'Several months ago, OpenAI shared a really interesting new neural network called ‘GPT-3’. I decided to test my video finder with ‘GPT-3’ as a search term and found this video:', 'It’s an interesting video from a creator with only a few thousand subscribers.', 'If I make the same search on YouTube.com, I have to scroll past videos about GPT-3 from all the big channels before finally finding the above video in 31st place.', 'Finding these interesting videos with fresh perspectives is much easier using the Video Finder code I wrote.', 'Over the last few months, I’ve tried with multiple different search terms based on my interests, such as ‘artificial intelligence’, ‘medical AI’ and ‘Python programming’. Pretty much without fail, there’s been at least one interesting video in the top five that the Video Finder suggests.', 'I tidied up all my code and uploaded it to GitHub.', 'On a high-level, my code now worked as follows:', 'I wanted a way to automatically run this script and had decided to use AWS Lambda (a serverless platform). Lambda lets you write code which lies dormant until it is triggered (e.g. once per week, or based on an event).', 'My perfect workflow would have been to automatically email myself the list of videos every week using Lambda. That way I could pick out the videos from the past I wanted to watch the forthcoming week, and I’d never have to visit the YouTube home page again.', 'However, this didn’t work out.', 'This was my first time using Lambda and, try as I might, I just couldn’t get all the imported libraries to work at the same time. To execute, the code needed the boto3 email client, OAuth for the API call, Pandas for storing results and many sub-dependencies. Ordinarily, installing these packages is fairly trivial, but on Lambda there were extra challenges. Firstly, there are memory limits for uploads, so I needed to zip the libraries and unzip them after being uploaded. Secondly, it turns out that AWS Lambda uses custom linux which can make it trickier to import the correct, cross-compatible libraries. Thirdly, my Mac was behaving weirdly with its virtual environments.', 'After putting around 10–15 hours into scouring StackOverFlow, uploading and re-uploading different codebases and consulting several friends, I still couldn’t get it to run. So eventually, much to my frustration, I decided to give up. (if you have any good ideas, let me know!)', 'So instead, I settled for a plan B: I manually run the script on my local computer once a week (after an automatic email prompt). To be honest, it’s not the end of the world.', 'All-in-all this was a really fun project. I learnt how to use the YouTube API, gained familiarity with AWS Lambda and created a tool that I can use going forward.', 'Using my code to decide what videos to watch does seem to have boosted my productivity, as long as I have the discipline not to click on too many ‘follow-on’ links. It’s possible that I miss some interesting videos, but my aim isn’t to comprehensively catch all good videos worth watching (I don’t think that’s possible). Rather, I want to raise the bar on the quality of the videos I do watch.', 'This project is just one of many ideas I have around automating information processing. I believe there’s huge potential for us to boost our productivity and reclaim our time through intelligent digital minimalism.', 'If you’re interested in joining my journey, you can follow on my mailing list and YouTube channel.', 'On the whole, the project is still pretty rough and there’s a lot more I could do.', 'Originally published on ChrisLovejoy.me', 'Many thanks to Luke Harries, Josh Case, Oscar Bennett and Mustafa Sultan for their feedback on this blog and code.']"
11/2020,Python vs. R for Data Science,Opinion,648,12,https://towardsdatascience.com/@sukanta-saha,https://towardsdatascience.com/python-vs-r-for-data-science-cf2699dfff4b?source=collection_archive---------1-----------------------,5,6,"['Python vs. R for Data Science', 'TLDR;', 'Why do you want to learn?', 'Are your friends/colleagues already an expert at one of these languages?', 'Are you interested only in Statistics and Data Analysis, or want to learn other areas such as Machine Learning and AI?', 'Final Thoughts']",10,"['In short, what matters most as a beginner in Data Science is that you DO Data Science. So just go with either one of the languages and prioritize getting some projects done while sipping away at your choice of sugary beverage. That’s how you will learn the fastest.', 'While I may be tempted to just recommend Python straight-away (Python is my main, but I do have some working knowledge of R), I want to present an unbiased evaluation of the effectiveness of the two languages for a beginner. This is mainly because the right choice is most definitely going to depend on your own particular situation.', 'The first and probably the most important factor you must consider is the reason WHY you want to learn. If you are a trained biologist, for example, looking to pick up some programming skills so you can better understand your dataset, or you are familiar with other scientific programming languages like MATLAB, then you should consider watching some R tutorials on YouTube because it would be simpler and more intuitive for you\xa0than\xa0Python. Or if you are a software engineer proficient in other languages like C/C++ and Java and would like to pivot into Data Science, Python would be the one to go with as just like most other popular programming languages, Python is an Object-Oriented Programming (OOP) language and it would be much more intuitive to you than R. Or, maybe you have been reading up about the fascinating field of Data Science recently and would like to dabble into it. In that case, either would really be fine and it would depend more on the other factors than this one.', 'One massive advantage you may have if you are learning a new language is the support of the community. Getting help from the community is pretty much expected amongst programmers and is usually considered an important skill. As a beginner, it may be confusing to learn how to get help, especially because there aren’t many resources online in the art of getting help from the community. Building an intuition and knowing what to ask when there’s a bug in the code is essential. If you know someone who is proficient in Python, or if another researcher at your lab has been working with R, then your best bet would be to go with what they know because then you can always ask them questions if you get stuck.', 'One major difference in the utilities of Python and R is that the former is an extremely versatile language, compared to the later. Python is a full-fledged programming language, which means you can collect, store, analyze, and visualize data, while also creating and deploying Machine Learning pipelines into production or on websites, all using just Python. On the other hand, R is purely for statistics and data analysis, with graphs that are nicer and more customizable than those in Python. R uses the Grammar of Graphics approach to visualizing data in its #ggPlot2 library and this provides a great deal of intuitive customizability which Python lacks. Perhaps a little oversimplified, but it may be justified to say that if you want to be a Data Analyst R should be your preferred choice, while if you want to be a Data Scientist Python is the better option. It’s the dilemma of generalization vs. specialization.', 'Data Science as a distinct field emerged only in the last ten years and as a result, has been constantly evolving. But what has been consistent is that more and more of the data pipeline is being automated every day. Employees with a multitude of skills such as data engineering, data visualization, Machine Learning engineering, cloud service integration, and model deployment, are always going to be more in demand than those who specialize only in one aspect of the Data Science workflow. Much of the field’s progression has been shaped by automation and only employees with good programming skills are resistant to it. Specializing in building impressive Machine Learning models will not cut it in the near future unless of course, you are extremely good at it.', 'The landscape of the industry at the moment is such that, at the beginner level, there are too many candidates who are “pretty” decent for too few junior Data Science jobs that are available. But for the slightly more senior positions, there aren’t enough practitioners who are experienced or have the right skillsets. And in order to take the next step in your career, you will ultimately need to be able to understand and implement the other stages of the workflow to some degree. So why not give yourself the highest probability of success?', 'If you are still unsure about it, the best advice I could give is to just pick Python for now and start learning. Later on, after you have a fairly good working knowledge of it, you could also learn the basics of R. But if you really don’t feel comfortable with Python, then you know what to do. Your top priority as a beginner should be to get a feel for the core concepts of Data Science and understand how to apply these concepts in real-world scenarios first and foremost. Setting up the coding environment could be a somewhat daunting experience for someone with no previous programming or Computer Science background. However, setting it up and getting started with learning will be a much more seamless experience with R than with Python. Far too many of us dwell on the idea of being a Data Scientist, and not enough actually take actions to become one.', 'P.S. If you want more short, to the point articles on Data Science and how a biologist navigates his way through the Data revolution, consider following me.', 'Thank you!']"
11/2020,Don’t Use Recursion In Python Any More,Python Closure — A Pythonic technique you must know,753,9,https://towardsdatascience.com/@qiuyujx,https://towardsdatascience.com/dont-use-recursion-in-python-any-more-918aad95094c?source=collection_archive---------2-----------------------,8,7,"['Don’t Use Recursion In Python Any More', 'What is Python Closure?', 'Access Outer Variables from the Inner Function', 'Write a Fibonacci Function Using Closure', 'Compare the Performance', 'Other Use Cases of Closure', 'Summary']",41,"['I was such a programmer who likes recursive functions very much before, simply because it is very cool and can be used to show off my programming skills and intelligence. However, in most of the circumstances, recursive functions have very high complexity that we should avoid using.', 'One of the much better solutions is to use Dynamic Planning when possible, which is probably the best way to solve a problem that can be divided into sub-problems. One of my previous articles has illustrated the power of Dynamic Planning.', 'However, in this article, I’m going to introduce another technique in Python that can be utilised as an alternative to the recursive function. It won’t outperform Dynamic Planning, but much easier in term of thinking. In other words, we may sometimes be struggling to make Dynamic Planning works because of the abstraction of the ideas, but it will be much easier to use closure.', 'First of all, let me use a simple example to demonstrate what is a closure in Python. Look at the function below:', 'The function outer is defined with another function inner inside itself, and the function outer returns the function inner as the “return value” of the function.', 'In this case, the nested function is called a closure in Python. If we check the “return value” of the outer function, we will find that the returned value is a function.', 'What does closure do? Because it returned a function, we can run this function, of course.', 'OK, we can see that the inner function can access variables defined in the outer function. Usually, we don’t use closure in such a way shown as above, because it is kind of ugly. We usually want to define another function to hold the function returned by the closure.', 'Therefore, we can also say that in a Python closure, we defined a function that defines functions.', 'How can we use a closure to replace a recursive then? Don’t be too hurry. Let’s have a look at another problem here, which is accessing outer variables from the inner function.', 'In the closure above-shown, we want to add 1 to the outer variable x in the inner function. However, this won’t work straightforward.', 'By default, you won’t be able to access the outer variable from the inner function. However, just like how we define a global variable in Python, we can tell the inner function of a closure that the variable should not be considered as a “local variable”, by using the nonlocal keyword.', 'Now, let’s say we want to add the variable x by 1 for five times. We can simply write a for loop to achieve this.', 'Fibonacci is commonly used as a “hello world” example of recursive functions. If you don’t remember it, don’t worry, it is pretty simple to be explained.', 'A Fibonacci sequence is a series of numbers that every number is the sum of the two numbers before it. The first two numbers, X₀ and X₁, are special. They are 0 and 1 respectively. Since X₂, the patter is as above-mentioned, it is the sum of X₀ and X₁, so X₂=1. Then, X₃ is X₁ + X₂ =2, X₄ is X₂ + X₃=3, X₅ is X₃ + X₄ = 5, and so on.', 'The recursive function requires us to think reversely from the “current scenario” to the “previous scenario”, and eventually what are the terminate conditions. However, by using the closure, we can think about the problem more naturally.', 'See the code below that the Fibonacci function is implemented using a closure.', 'We know that the Fibonacci starts with two special number X₀=0 and X₁=1, so we just simply define them in the outer function. Then, the inner function get_next_number is simply return the sum of the two numbers it got from the outer function.', 'Additionally, don’t forget to update X₀ and X₁ with X₁ and X₂. In fact, we can simplify the code:', 'to', 'This is updating the two variables first and then return the second, which is equivalent to the previous code snippet.', 'Then, we can use this closure to calculate Fibonacci numbers. For example, we want to show the Fibonacci sequence up to the 20th number.', 'Alright, we knew that we can use closure to replace a recursive function in the previous section. How about the performance? Let’s compare them!', 'Firstly, let’s implement the Fibonacci function using a recursive function.', 'We can verify the function by output the 20th number of the Fibonacci sequence.', 'Then, let’s embed the closure version in a function for comparing purposes.', 'Now, let’s compare the speed.', '2.79ms v.s. 2.75µs. The closure method is 1000x faster than the recursive! The most intuitive reason is that all the temporary values for every level of recursion are stored in the memory separately, but the closure is actually updating the same variables in every loop.', 'Also, there is a depth limitation for recursion. For the closure, because it is basically a for loop, there will not be any constraints.', 'Here is an example of getting the 1000th Fibonacci number', 'That’s indeed a huge number, but the closure method can finish the calculation in about 100 µs, while the recursive function met its limitation.', 'Python closures are very useful not only for replacing the recursive functions. In some cases, it can also replace Python classes with a neater solution, especially there are not too many attributes and methods in a class.', 'Suppose we have a dictionary of students with their exam marks.', 'We want to have several functions that help us to filter the students by marks, to put them into different grade classes. However, the criteria might change over time. In this case, we can define a Python closure as follows:', 'The closure defines a function that defines other functions based on the parameters passed in dynamically. We will pass the lower bound and upper bound of the grade class, and the closure will return us a function does that.', 'The above code will give us 4 functions that will classify the student to the corresponding grade classes based on the boundaries we gave. Please be noted that we can change the boundary any time to make another function or overwrite current grade functions.', 'Let’s verify the functions now.', 'Very neat! Just bear in mind that we still need to define classes when the case is more complex.', 'In this article, I have introduced a technique called closure in Python. It can be utilised to rewrite recursive functions in most of the circumstances and outperform the latter to a huge extent.', 'Indeed, closure might not be the best solution for some problems from the performance perspective, especially when Dynamic Planning is applicable. However, it is much easier to come up with. Sometimes Dynamic Planning is a bit overkill when we are not very sensitive to the performance, but closure might be good enough.', 'Closure can also be used to replace some use cases that we may want to define a class to satisfy. It is much neater and elegant in those cases.']"
11/2020,All Machine Learning Algorithms You Should Know in 2021,Intuitive explanations of the most popular…,1.3K,4,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-in-2021-2e357dd494c7?source=collection_archive---------3-----------------------,10,14,"['All Machine Learning Algorithms You Should Know in 2021', 'Linear Regression', 'Logistic Regression', 'K-Nearest Neighbors', 'Naive Bayes', 'Support Vector Machines', 'Decision Tree', 'Random Forest', 'AdaBoost', 'Gradient Boost', 'XGBoost', 'LightGBM', 'CatBoost', 'Thanks for Reading!']",53,"['As my knowledge in machine learning grows, so does the number of machine learning algorithms! This article will cover machine learning algorithms that are commonly used in the data science community.', 'Keep in mind that I’ll be elaborating on some algorithms more than others simply because this article would be as long as a book if I thoroughly explained every algorithm! I’m also going to try to minimize the amount of math in this article because I know it can be pretty daunting for those who aren’t mathematically savvy. Instead, I’ll try to give a concise summary of each and point out some of the key features.', 'With that in mind, I’m going to start with some of the more fundamental algorithms and then dive into some newer algorithms like CatBoost, Gradient Boost, and XGBoost.', 'Linear Regression is one of the most fundamental algorithms used to model relationships between a dependent variable and one or more independent variables. In simpler terms, it involves finding the ‘line of best fit’ that represents two or more variables.', 'The line of best fit is found by minimizing the squared distances between the points and the line of best fit — this is known as minimizing the sum of squared residuals. A residual is simply equal to the predicted value minus the actual value.', 'In case it doesn’t make sense yet, consider the image above. Comparing the green line of best fit to the red line, notice how the vertical lines (the residuals) are much bigger for the green line than the red line. This makes sense because the green line is so far away from the points that it isn’t a good representation of the data at all!', 'If you want to learn more about the math behind linear regression, I would start off with Brilliant’s explanation.', 'Logistic regression is similar to linear regression but is used to model the probability of a discrete number of outcomes, typically two. At a glance, logistic regression sounds much more complicated than linear regression, but really only has one extra step.', 'First, you calculate a score using an equation similar to the equation for the line of best fit for linear regression.', 'The extra step is feeding the score that you previously calculated in the sigmoid function below so that you get a probability in return. This probability can then be converted to a binary output, either 1 or 0.', 'To find the weights of the initial equation to calculate the score, methods like gradient descent or maximum likelihood are used. Since it’s beyond the scope of this article, I won’t go into much more detail, but now you know how it works!', 'K-nearest neighbors is a simple idea. First, you start off with data that is already classified (i.e. the red and blue data points). Then when you add a new data point, you classify it by looking at the k nearest classified points. Whichever class gets the most votes determines what the new point gets classified as.', 'In this case, if we set k=1, we can see that the first nearest point to the grey sample is a red data point. Therefore, the point would be classified as red.', 'Something to keep in mind is that if the value of k is set too low, it can be subject to outliers. On the other hand, if the value of k is set too high then it might overlook classes with only a few samples.', 'Naive Bayes is a classification algorithm. This means that Naive Bayes is used when the output variable is discrete.', 'Naive Bayes can seem like a daunting algorithm because it requires preliminary mathematical knowledge in conditional probability and Bayes Theorem, but it’s an extremely simple and ‘naive’ concept, which I’ll do my best to explain with an example:', 'Suppose we have input data on the characteristics of the weather (outlook, temperature, humidity, windy) and whether you played golf or not (i.e. last column).', 'What Naive Bayes essentially does is compare the proportion between each input variable and the categories in the output variable. This can be shown in the table below.', 'To give an example to help you read this, in the temperature section, it was hot for two days out of the nine days that you played golf (i.e. yes).', 'In mathematical terms, you can write this as the probability of it being hot GIVEN that you played golf. The mathematical notation is P(hot|yes). This is known as conditional probability and is essential to understand the rest of what I’m about to say.', 'Once you have this, then you can predict whether you’ll play golf or not for any combination of weather characteristics.', 'Imagine that we have a new day with the following characteristics:', 'First, we’ll calculate the probability that you will play golf given X, P(yes|X) followed by the probability that you won’t play golf given X, P(no|X).', 'Using the chart above, we can get the following information:', 'Now we can simply input this information into the following formula:', 'Similarly, you would complete the same sequence of steps for P(no|X).', 'Since P(yes|X) > P(no|X), then you can predict that this person would play golf given that the outlook is sunny, the temperature is mild, the humidity is normal and it’s not windy.', 'This is the essence of Naive Bayes!', 'A Support Vector Machine is a supervised classification technique that can actually get pretty complicated but is pretty intuitive at the most fundamental level. For the sake of this article, we’ll keep it pretty high level.', 'Let’s assume that there are two classes of data. A support vector machine will find a hyperplane or a boundary between the two classes of data that maximizes the margin between the two classes (see above). There are many planes that can separate the two classes, but only one plane can maximize the margin or distance between the classes.', 'If you want to get into the math behind support vector machines, check out this series of articles.', 'Before understanding random forests, there are a couple of terms that you’ll need to know:', 'Now that you understand these terms, let’s dive into it.', 'Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree (bagging). What’s the point of this? By relying on a “majority wins” model, it reduces the risk of error from an individual tree.', 'For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all 4 decision trees, the predicted value would be 1. This is the power of random forests!', 'AdaBoost, or Adaptive Boost, is also an ensemble algorithm that leverages bagging and boosting methods to develop an enhanced predictor.', 'AdaBoost is similar to Random Forests in the sense that the predictions are taken from many decision trees. However, there are three main differences that make AdaBoost unique:', 'In essence, AdaBoost takes a more iterative approach in the sense that it seeks to iteratively improve from the mistakes that the previous stump(s) made.', 'If you want to learn more about the underlying math behind AdaBoost, check out my article ‘A Mathematical Explanation of AdaBoost in 5 Minutes’.', 'It’s no surprise that Gradient Boost is also an ensemble algorithm that uses boosting methods to develop an enhanced predictor. In many ways, Gradient Boost is similar to AdaBoost, but there are a couple of key differences:', 'While the last point may have been confusing, all that you need to know is that Gradient Boost starts by building one tree to try to fit the data, and the subsequent trees built after aim to reduce the residuals (error). It does this by concentrating on the areas where the existing learners performed poorly, similar to AdaBoost.', 'XGBoost is one of the most popular and widely used algorithms today because it is simply so powerful. It is similar to Gradient Boost but has a few extra features that make it that much stronger including…', 'I strongly recommend that you watch StatQuest’s video to understand how the algorithm works in greater detail.', 'If you thought XGBoost was the best algorithm out there, think again. LightGBM is another type of boosting algorithm that has shown to be faster and sometimes more accurate than XGBoost.', 'What makes LightGBM different is that it uses a unique technique called Gradient-based One-Side Sampling (GOSS) to filter out the data instances to find a split value. This is different than XGBoost which uses pre-sorted and histogram-based algorithms to find the best split.', 'Read more about Light GBM vs XGBoost here!', 'CatBoost is another algorithm based on Gradient Descent that has a few subtle differences that make it unique:', 'Overall, what makes CatBoost so powerful is its low latency requirements which translates to it being around eight times faster than XGBoost.', 'If you want to read about CatBoost in greater detail, check out this article.', 'If you made it to the end, congrats! You should now have a better idea of all of the different machine learning algorithms out there.', 'Don’t feel discouraged if you had a harder time understanding the last few algorithms — not only are they more complex but they’re also relatively new! So stay tuned for more resources that will go into these algorithms in greater depth.', 'As always, I wish you the best in your data science endeavors. If you liked this article, I’d appreciate it if you gave me a follow! :)', 'Not sure what to read next? I’ve picked another article for you:']"
11/2020,How to get an amazing Terminal,"In Windows and Linux; including prompts, fonts, and colors",364,2,https://towardsdatascience.com/@martinthoma,https://towardsdatascience.com/how-to-get-an-amazing-terminal-91619a0beeb7?source=collection_archive---------4-----------------------,5,9,"['How to get an amazing Terminal', 'Terminology', 'Fonts', 'Windows Terminal', 'Linux and Windows Terminal: Aminal', 'Linux: Gnome Terminal', 'Linux Shell: fish', 'Aliases', 'Summary']",26,"['As a developer with 10+ years of experience, I love using the shell. The commands never change, I can create custom shortcuts, it’s reliable and fast. The defaults are not great, though. After reading this article, you will know how to get an awesome shell + terminal on your system.', 'The shell is what actually executes the command. The terminal is a wrapper that runs the shell.', 'The terminal is where you set the font face, font size, color schemes, support for multiple tabs. Examples for terminal emulators are GNOME terminal, Konsole on KDE, Terminator, and XTerm. On Linux, I recommend keeping the default. On Windows, the Windows Terminal is awesome. On Mac, I’ve heard good things about iTerm 2.', 'The shell stores the history of entered commands, defines how you set environment variables, how you switch the current directory. Examples for shells on Linux are ZSH, Bash, fish. On Windows, the typical shells are PowerShell. You can see which shell you are running by executing echo $0 . On Linux, it’s most likely Bash.', 'Every shell has a prompt. The prompt is what is written before your cursor. It signalizes that you can enter a command and gives useful context information. In the example above, the prompt contains the user name moose , the current computer pc08 , the current working directory ~/GitHub/MartinThoma/flake8-simplify , the active git branch feature/19 and the fact that there are modifications ±.', 'No matter what you take, the font matters. You might want to have a monospace font. And you for sure want a powerline font; trust me with that one. The powerline font gives your shell the possibility to use characters that look like images. It can make the prompt way nicer.', 'I like Ubuntu Mono and Droid Sans Mono:', 'There are also “programming fonts” like Fira Code or Jetbrains Mono. I don’t like them, because they make it harder for me to really know what is written. They look nice, though.', 'First, make sure you have the Windows Terminal installed:', 'Launch a terminal and navigate to the settings. It’s this small downwards pointing “arrow”:', 'You should see a JSON file which you can change to fit your taste. I have the following:', 'Download and install all 4 “DejaVu Sans Mono Powerline” fonts. On all systems I know, installing a font is done by double-clicking it. Then a window opens which has an “Install” button.', 'Aminal is a Terminal Emulator written in Go. It can be used on Linux, Windows, and Mac. It allows configuration via a configuration file and includes the color and keyboard shortcuts in it.', 'First, you need to install and configure Go on your system. On Ubuntu, it works like this:', 'Then you can install and run aminal:', 'The Gnome terminal can be customized by editing the profile. Here I set the Ubuntu Mono derivate Powerline Regular with a font size of 12.', 'The command is set to zsh as this is my favorite shell.', 'The colors are set to solarized dark (left-to-right, top-line / bottom-line)', 'Install the fish shell:', 'Change the default shell in your terminal emulator to fish . Within Gnome terminal, it is called “custom command”.', 'Then install “Oh My Fish”:', 'And set the theme to agnoster:', 'For cool features of the fish shell, read Why I Use Fish Shell Over Bash and Zsh by Alec Brunelle.', ""A core part of making the terminal awesome is making common commands short. To do so, you create an alias for a command — a shorter version of the original command. The most common example is changing a directory to go one level up. For example, if you are in /home/user/foo/bar , you want to get to /home/user/foo . In most shells, you have to enter cd .. . I like to abbreviate that to .. . So I have the alias alias ..='cd ..' . The syntax may vary, depending on your shell. For Bash, ZSH, and fish it is"", 'For bash, you insert them in ~/.bashrc , for ZSH in ~/.zshrc . In fish, it is different.', 'Here are some aliases I like:']"
11/2020,7 Must-Haves in your Data Science CV,Looking for a new role? Be sure to check these critical points to…,1.2K,15,https://towardsdatascience.com/@elad-cohen,https://towardsdatascience.com/7-must-haves-in-your-data-science-cv-9316841aeb78?source=collection_archive---------5-----------------------,7,1,['7 Must-Haves in your Data Science CV'],16,"['Managing Riskified’s Data Science department entails a lot of recruiting — we’ve more than doubled in less than a year-and-a-half. As the hiring manager for several of the positions, I also read through a lot of CVs. Recruiters screen through a CV in 7.4 seconds, and after recruiting for several years my average time is pretty fast, but not that extreme. In this blog, I’m going to walk you through my personal heuristics (‘cheats’) that help me screen a resume. While I can’t guarantee that others use the same heuristics, and different roles will differ in the importance of each point, paying attention to these points can help you conquer the CV screen stage. Additionally, some of these heuristics may not seem fair or could potentially overlook qualified candidates. I agree that talented Machine Learning practitioners who don’t invest in their CV could get rejected with this screen, but it’s the best tradeoff considering the time. Remember, a highly sought after position may attract a hundred or more CVs. If you want an efficient process, the CV screen has to be quick.', 'Here are the 7 heuristics used to quickly screen your Data Science CV:', 'I’m going to quickly run through your CV to look at your previous positions and see which are marked as ‘Data Scientist’. There are some other adjacent terms (depending on the role I’m hiring for), such as ‘Machine Learning Engineer’, ‘Research Scientist’ or ‘Algorithm Engineer’. I don’t include ‘Data Analyst’ in this bucket as the day-to-day work is typically different from that of a Data Scientist and the Data Analyst title is an extremely broad term. If you’re doing data science work at your present job and you have some other creative job description, it’ll probably be in your best interest to have your title changed to a Data Scientist. This can be very true for Data Analysts who are de facto Data Scientists. Remember, even if the CV contains descriptions of the projects you’ve worked on (and they include machine learning), a title other than Data Scientist will add unnecessary ambiguity. Additionally, if you’ve undergone a data science bootcamp or full-time masters in the field, this will probably be considered the beginning of your data science experience (unless you worked in a similar role earlier, which will warrant questions at a later stage).', 'Ideally, I’d like to read what you did (technical aspects) and what the business outcome was. There’s a lack of technically savvy data scientists who can talk in business terms. If you can share the business KPIs that your work impacted, that’s a big thumbs-up in my book. For example, indicating your model’s improvement in AUC is alright, but addressing the conversion rate increase as a result of your model improvement means you ‘get it’ — the business impact is what really matters at the end of the day. Compare the following alternatives depicting the same work with a different emphasis (technical vs business):', 'a. Bank loan default rate model — improved model’s Precision-Recall AUC from 0.94 to 0.96.', 'b. Bank loan default rate model — increased business unit’s annual revenue by 3% ($500K annually) while maintaining constant default rates.', 'What’s your formal education and in what field. Is it a well-known institution? For more recent grads, I’ll also look at their GPA and whether they received any excellence awards or honors such as making the Rector’s or Dean’s list. Since Data Science is a wide-open field without any standardized tests or required knowledge, people can enter the field in various methods. In my last blog, I wrote about the 3 main paths taken into the field and based on your education and timing, I’ll figure out which one you probably took. Hence, the timing helps understand your story — how and when did you transition into data science. If you don’t have any formal education in data science, that’s fine, but you need to either demonstrate a track record of work in the field and/or advanced degrees in similar fields.', 'I’ve seen some beautiful CVs (I’ve saved a few of these for personal inspiration) but I’ve also received text files (.txt) that lack any formatting. Working on your CV can be a pain, and if you’ve chosen data science as your endeavor there’s a good chance you don’t enjoy creating aesthetic designs in your spare time. Without going overboard, you do want to look for a nice template that enables you to get everything across in limited space. Use the space wisely — it’s useful to split the page and highlight specific sections that don’t fall under the chronological work/education experience. This can include the tech stack you’re familiar with, a list of self-projects, links to your github or blog and others. A few simple icons can also help with emphasizing section headers. Many candidates use 1–5 stars or bar charts next to each language/tool they are familiar with. Personally, I’m not a big fan of this approach for several reasons:', 'I’ve also seen this approach abused even further by taking the subjective measures and turning them into a pie chart (30% python, 10% team-player, etc). While this was probably supposed to be a creative way to stand out, it demonstrates a lack of basic understanding behind the concepts of different charts.', 'Here are two examples of CVs I’ve found visually appealing, with details blurred for anonymity.', 'There are two types of variety I look for:', 'This can generally be broken down into languages, specific packages (scikit learn, pandas, dplyr, etc), clouds and their services (AWS, Azure, GCP) or other tools. Some candidates mix this up with algorithms or architectures they are familiar with (RNN, XGBoost, K-NN). On a personal note, I prefer that this revolve around technologies and tools; when a specific algorithm is mentioned it makes me wonder whether the candidate’s theoretical ML knowledge is limited to just those specific algorithms.Here, I’m looking for the relevance of the tech stack — are they from the last few years (a positive sign that the candidate is hands-on and learning new skills), the breadth of the stack (are they very limited to specific tools or are they familiar with quite a few things) and the fit with our stack (how much will we need to teach them).', 'Is there something you’ve worked on that you can share on GitHub? Any Kaggle competition or side-project can be very helpful, and enables looking at concise code, types of preprocessing, feature engineering, EDA, choice of algorithm and countless other issues that need to be addressed in a real-life project. Add a link to your GitHub and Kaggle account for interviewers to dive into your code. If you don’t have much experience, there’s a good chance you’ll be asked about one or more of these projects. In some interviews I had, the candidate didn’t remember much about the project and we couldn’t develop a conversation regarding the choices they made and the reason behind them. Be sure you brush up on the work you did or keep it out of the CV. Similarly, make sure you present your best work and you’ve put enough time and effort into it. It’s better to have 2–3 high-quality projects than 8–10 medium (or lower) quality.', 'If you’re looking for a new data science position, take some time and go through the points in this article. It’s fine if you can’t check off all of these marks, but the more you can, the better. Hopefully, these tips will help you stand out from the crowd and pass the CV screen with flying colors.', 'Good luck and happy job hunting!', 'Do you have a different approach when screening Data Science CVs? Please share in the comments section below!']"
11/2020,Want to Be a Data Scientist? Don’t Start With Machine Learning.,Opinion,1.4K,14,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/want-to-be-a-data-scientist-dont-learn-machine-learning-28e418d9af2f?source=collection_archive---------6-----------------------,5,4,"['Want to Be a Data Scientist? Don’t Start With Machine Learning.', 'So why shouldn’t you start with machine learning?', 'What should you do instead?', 'Thanks for Reading!']",23,"['The first thing most people think about when they hear the term “data science” is usually “machine learning”.', 'This was the case for me. My interest in data science sparked because I was first exposed to the idea of “machine learning” which sounded really cool. So when I was looking for a place to start learning about data science, you can guess where I started (hint: it rhymes with bean churning).', 'This was my biggest mistake and this leads me to my main point:', 'If you want to be a data scientist, don’t start with machine learning.', 'Bear with me here. Obviously, to be a “complete” data scientist, you’ll have to eventually learn about machine learning concepts. But you’d be surprised at how far you can get without it.', 'Data science and machine learning are like a square and a rectangle. Machine learning is (a part of) data science but data science isn’t necessarily machine learning, similar to how a square is a rectangle but a rectangle isn’t necessarily a square.', ""In reality, I’d say that machine learning modeling only makes up around 5–10% of a data scientist's job, where most of one’s time is spent elsewhere, which I’ll elaborate on later."", 'TLDR: By focusing on machine learning first, you’ll be putting in a lot of time and energy, and getting little in return.', 'At its core, machine learning is built on statistics, mathematics, and probability. The same way that you first learn about English grammar, figurative language, and so forth to write a good essay, you have to have these building blocks set in stone before you can learn machine learning.', 'To give some examples:', 'And so, I’ll conclude with two points. One, learning the fundamentals will make learning more advanced topics easier. Two, by learning the fundamentals, you will already have learned several machine learning concepts.', 'Many data scientists struggle with this, even myself. Similar to my initial point, most data scientists think that “data science” and “machine learning” go hand in hand. And so, when faced with a problem, the very first solution that they consider is a machine learning model.', 'But not every “data science” problem requires a machine learning model.', 'In some cases, a simple analysis with Excel or Pandas is more than enough to solve the problem at hand.', 'In other cases, the problem will be completely unrelated to machine learning. You may be required to clean and manipulate data using scripts, build data pipelines, or create interactive dashboards, all of which do not require machine learning.', 'If you’ve read my article, “How I’d Learn Data Science If I Had to Start Over,” you may have noticed that I suggested learning Mathematics, Statistics, and programming fundamentals. And I still stand by this.', 'Like I said before, learning the fundamentals will make learning more advanced topics easier, and by learning the fundamentals, you will already have learned several machine learning concepts.', 'I know it may feel like you’re not progressing to be a “data scientist” if you’re learning statistics, math, or programming fundamentals, but learning these fundamentals will only accelerate your learnings in the future.', 'You have to learn to walk before you can run.', 'If you would like some tangible next steps to start with instead, here are a couple:', 'This is an opinionated article so take what you want from it. My overall advice is that machine learning should not be the focus of your studies because it’s not a great use of time and will do little in helping you be a successful data scientist in the working world.', 'With that said, I wish you the best of luck in your endeavors!', 'Not sure what to read next? I’ve picked another article for you:']"
11/2020,What to Learn to Become a Data Scientist in 2021,and why the data science generalist will triumph,1.2K,9,https://towardsdatascience.com/@rebecca.vickery,https://towardsdatascience.com/what-to-learn-to-become-a-data-scientist-in-2021-95970d4b3b17?source=collection_archive---------7-----------------------,7,7,"['What to Learn to Become a Data Scientist in 2021', '1. Python 3', '2. Pandas', '3. SQL and NoSQL', '4. Cloud', '5. Airflow', '6. Software engineering']",29,"['When I started learning data science a few years ago most job ads requested a PhD, or at the very least a masters, in maths, statistics or a similar subject as an essential requirement.', 'Over the last couple of years, things have evolved. With the development of machine learning libraries that abstract away much of the complexity behind the algorithms, and a realisation that practically applying machine learning to solve business problems requires a set of skills that are not usually acquired through academic study alone. Companies are now hiring data scientists based on their ability to perform applied data science rather than research.', 'Applied data science that delivers value to a business in the fastest possible time requires a very practical skillset. Additionally, as more companies migrate their data and machine learning solutions to the cloud, It is becoming paramount for data scientists to have an understanding of the new tools and technology relating to this.', 'Additionally, I believe that the days of a data scientist working solely on data modelling, using data pulled together by data engineers, and then handing the model over to a team of software engineers to put into production are largely behind us. Particularly outside of the tech giants such as Amazon, Facebook and Google. In most companies, with the exception of some of the very big tech players, there either isn’t the resource available in those teams or the alignment of priorities are not there at the right time.', '“There is a saying, ‘A jack of all trades and a master of none.’ When it comes to being a data scientist you need to be a bit like this, but perhaps a better saying would be, ‘A jack of all trades and a master of some.’” Brendan Tierney, Principal Consultant at Oralytics.', 'In order for a data scientist to deliver maximum value to a business, they need to be able to work across the full model development life cycle. Having at least a working knowledge in developing data pipelines, performing data analysis, machine learning, maths, statistics, data engineering, cloud computing and software engineering. This means that as we move into 2021 the data scientist generalist is the preferred hire for most businesses.', '“The bigger the picture, the more unique the potential human contribution. Our greatest strength is the exact opposite of narrow specialization. It is the ability to integrate broadly.”, David Epstein, Why Generalists Triumph in a Specialized World.', 'This article doesn’t cover absolutely everything you need to be a data scientist in 2021. Instead, it covers the key skills, both new and old, that have become the most essential for every successful data scientist to have in the near future.', 'There are still some cases where data scientists may use R but generally speaking if you are doing applied data science these days, then Python is going to be the most valuable programming language to learn.', 'Python 3 (the latest version) has now firmly become the default version of the language for most applications as support for Python 2 was dropped by the majority of libraries on 1st January 2020. If you are learning Python for data science now it is important to choose a course that works with this version.', 'You will need a good understanding of the basic syntax of the language and how to write functions, loops and modules. Be familiar with both object-oriented and functional programming in Python, and be able to develop, execute and debug programs.', 'Pandas is still the number one Python library for data manipulation, processing and analysis. In 2021 this is still one of the most vital skills to have as a data scientist.', 'Data is at the very heart of any data science project and Pandas is the tool that will enable you to extract, clean, process and derive insights from it. Most machine learning libraries also generally take Pandas DataFrames as a standard input these days.', 'SQL has been around since the 1970’s but it still remains one of the most vital and saught after skills for data scientists. The vast majority of businesses use relational databases as their analytical data stores and as a data scientist SQL is the tool that will deliver you this data.', 'NoSQL (“not only SQL”) are databases that don’t store data as relational tables, instead data is stored as key value pairs, wide-columns or graphs. Example NoSQL databases include Google Cloud Bigtable and Amazon DynamoDB.', 'As the volumes of data collected by companies increases and unstructured data becomes more regularly used in machine learning models organisations are turning to NoSQL databases, either as a complement or as an alternative to, the traditional data warehouse. This trend is likely to continue into 2021 and as a data scientist it is important to gain at least a basic understanding of how to interact with data in this form.', 'According to a report from O’reilly in January this year, titled ‘Cloud adoption in 2020’, 88% of organisations were at this time using some form of cloud infrastructure. The impact of Covid-19 is likely to have further accelerated this adoption.', '“At first glance, cloud usage seems overwhelming. More than 88% percent of respondents use cloud in one form or another. Most respondent organizations also expect to grow their usage over the next 12 months.”, Cloud Adoption 2020, By Roger Magoulas and Steve Swoyer.', 'The use of cloud in other areas of a business usually goes hand in hand with cloud-based solutions for data storage, analytics and machine learning. The major cloud providers such as Google Cloud Platform, Amazon Web Services and Microsoft Azure are developing out tooling for training, deploying and serving machine learning models at a rapid pace.', 'As a data scientist working in 2021 and beyond it is very likely that you will be working with data housed in a cloud-based database such as Google BigQuery and developing cloud based machine learning models. Experience and skills in this area are likely to be in high demand as we move into 2021.', 'Apache Airflow, an open source workflow management tool, is rapidly being adopted by many businesses for the management of ETL processes and machine learning pipelines. Many large tech companies such as Google and Slack are using it and Google even built their cloud composer tool on top of this project.', 'I am noticing Airflow being mentioned more and more often as a desirable skill for data scientists on job adverts. As mentioned at the beginning of this article I believe it will become more important for data scientists to be able to build and manage their own data pipelines for analytics and machine learning. The growing popularity of Airflow is likely to continue at least in the short term, and as an open source tool, is definitely something that every budding data scientist should at learn.', 'Data science code is traditionally messy, not always well tested and lacking in adherence to styling conventions. This is fine for initial data exploration and quick analysis but when it comes to putting machine learning models into production then a data scientist will need to have a good understanding of software engineering principles.', 'If you are planning to work as a data scientist it is likely that you will either be putting models into production yourself or at least be involved heavily in the process. It is therefore essential to cover the following skills in any learning that you undertake.', 'In this article, I wanted to highlight some of the key trends emerging in terms of the skills required for data scientists. These insights have been gleaned from reviewing current data science job adverts, my own experience working as a data scientist and reading articles covering future trends in the field.', 'This is not meant as an exhaustive list, there are certainly a lot more skills and experience needed to become a successful data scientist. However, in this post, I wanted to cover some of the most important skills that are very likely to be required in the coming year.', 'For a more comprehensive list of skills that you should learn, if you are studying to be a data scientist, I wrote a series of articles giving a complete roadmap for learning. They are linked below.', 'Thanks for reading!', 'I send out a monthly newsletter if you would like to join please sign up via this link. Looking forward to being part of your learning journey!']"
11/2020,5 YouTubers Data Scientists And ML Engineers Should Subscribe To,#1 Lex Fridman,906,8,https://towardsdatascience.com/@richmondalake,https://towardsdatascience.com/5-youtubers-data-scientists-and-ml-engineers-should-subscribe-to-e5bf5e2e9167?source=collection_archive---------8-----------------------,5,8,"['5 YouTubers Data Scientists And ML Engineers Should Subscribe To', '1. Lex Fridman', '2. Ken Jee', '3. Yannic Kilcher', '4. Jordan Harrod', '5. 3Blue1Brown', 'Conclusion', 'I hope you found the article useful.']",37,"['Are you a Data Scientist or ML practitioner looking for a way to make use of your free time effectively?', 'Perhaps you are bored of watching endless cat videos on YouTube — do people still do this?', 'Stop watching cat videos!', 'Instead, you should subscribe to the Data Science and Machine Learning related YouTube channels that I present in this article.', 'Learn about how you can effectively navigate your data science career, or better yet, expand your intuition on the origins of the human mind and consciousness while you wait for your training loss to converge.', 'Lex Fridman’s Youtube channel is arguably the most insightful channel for machine learning practitioners to date.', 'The videos on this channel are simple conversations with between Lex Fridman himself and extraordinary individuals that are pioneers or top researchers within fields such as computing, machine learning, deep learning, AI and more.', 'The main reason I would recommend this channel to Data Scientist and Machine Learning Engineers is for the sole fact that Lex Fridman has had exchanges with notable pioneers of the machine learning and deep learning field.', 'Most deep learning practitioners will be familiar with some if not all of the deep learning pioneers I’m about to list.', 'Apart from the deep learning content on Lex’s channel, my favourite video is this fascinating conversation with Joscha Bach. This conversation will have you in awe at the speed at which Joscha delivers elegant responses to questions that dance around the topic of consciousness, the human mind and our Universe.', 'Don’t hesitate to find out why this channel currently boasts views of around 45 million!', 'If you are already a subscriber of this channel, feel free to link your favourite video in the comments.', 'One of the most entertaining Data Science YouTube personality, with five years of experience in the Data Science field.', 'Most data scientists might already be familiar with Ken Jee, perhaps you’ve come across one of his videos that goes through CV’s and portfolios, or maybe you have gone through his “Data Science Project from scratch” video series.', 'I’m a subscriber of Ken Jee’s videos for the fact that he interviews data scientists and machine learning practitioners. These interviews provide an account of the guest’s learnings and experiences within data science and related domains.', 'I’ve gained a wealth of information on how to navigate a career within the AI field from several interview guests on this channel.', 'Here are some reasons you should subscribe to this channel if you are a Data Scientist:', 'Yannic Kilcher’s channel is one of the few channels on YouTube that you can get an excellent explanation on research papers covering the state of the art machine learning techniques.', 'Below are a few videos that showcase the intuitive and straightforward approach Yannic takes to explaining research papers.', 'Not all machine learning practitioners have had previous MSc or PhD experience in which has enabled the development skills required to read and understand research papers.', 'A large number of data scientist and ML Engineers are self-taught and do not have the academic discipline required to analyse research papers effectively.', 'This is why Yannic channel is one of my top 5 YouTube channels that I will recommend Data Scientists and ML practitioners in gaining in-depth technical information into conventional and modern machine learning model and techniques should subscribe to.', 'Jordan Harrod is an MIT and Harvard graduate that creates video content on AI-related topics.', 'On Jordan’s channel, you can find tons of video explaining the direct application of AI-based technology to solving real-world problems.', 'Most of the videos on Jordan’s channel are concise and less than 10 minutes long. Yet, they contain so much relevant and current information that most machine learning practitioners would benefit from.', 'Jordan’s videos explore topics that have high interest value from a technical perspective, such as how AI is used to predict Covid-19, or the race biases within AI solutions.', 'The last YouTube channel in this list is probably one that even non-ML practitioners would have come across.', 'This channel content is second to none, and I can’t praise this channel’s creator, Grant Sanderson, enough.', 'For those who are curious as to the face and personality behind the channel, then check out these two interviews Lex Fridman has with Grant: Round 1, Round 2.', 'Those who are familiar with 3Blue1Brown will agree with me when I say that he manages to teach topics that would take University lecturers hours in 15–20 minutes short videos.', 'This channel taught me the basics of Machine Learning and Neural networks and made maths simple.', 'Within machine learning, it is essential to understand topics such as linear algebra, calculus, and partial differentiation. When studying Neural Networks, it is crucial to understand the fundamental components of a neural network.', 'It is also essential to understand concepts such as backpropagation, gradient descent, and neural networks in general.', 'That’s enough YouTube videos to keep you occupied during these final months of 2020 and well into the early months of 2021.', 'The YouTube channels included in this article are currently actively putting out new video content frequently.', 'Are there any other Data Science/ML YouTuber you feel should’ve made it on this list? Then I invite you to share with other readers within the comment section.', 'To connect with me or find more content similar to this article, do the following:']"
11/2020,Understand O.O.P. in Python with One Article,Dive into object-oriented programming to grow your…,282,4,https://towardsdatascience.com/@julianh,https://towardsdatascience.com/understand-o-o-p-in-python-with-one-article-bfa76f3ba48c?source=collection_archive---------9-----------------------,11,12,"['Understand O.O.P. in Python with One Article', 'Table of contents:', '1. Introduction to Object-oriented programming', '2. Defining a New Class', '3. Instance Methods', '4. Defining Constructors and Other Special Methods', '5. Code Reuse', 'Conclusion', 'Understand Loops in Python:', 'Create amazing visualizations in Python:', 'Creating stock’s price simulator:', 'Data Science for E-Commerce with Python:']",47,"['Another trick in software is to avoid rewriting the software by using a piece that’s already been written, so called component approach which the latest term for this in the most advanced form is what’s called Object Oriented Programming. — Bill Gates.', 'This quote from Bill Gates illustrates the purpose of this article. Understanding object-oriented programming allows you to develop a way of thinking and implementing code and my aim is to make it clear-sky simple to you. After all, one of the main reasons why we code, or at least want to learn how to do so, is to automate routine tasks. O.O.P. serves as a trick to achieve this purpose, as the Co-Founder of Microsoft says.', '1. Introduction to Object-oriented programming (3 min read)', '2. Defining a New Class (3 min read)', '3. Instance Methods (1 min read)', '4. Defining Constructors and Other Special Methods (2 min read)', '5. Code Reuse (2 min read)', 'In order to begin understanding the intuition behind this programming technique, let’s take a look at an initial example. Imagine that you have to describe a car to someone who’s never seen one before, how would you do it?', 'You might want to start saying that it’s a wheeled motor vehicle used for transportation. Also, you might say that there are several brands of car-maker companies, which make different types of cars that fulfill various needs. In an even more basic level you might say that it has four tires, that they carry up to five people in most cases and that they are mainly used to transport people, not goods.', 'When explaining to a computer what kind of object this is, it’s a good idea to approach it in a similar way. Computers have no innate idea of what a car is, why were they created or who uses them. And if you want your computer to correctly understand the object, a car in this case, you have to clearly explain which are its attributes.', 'To make it easier for computers to understand these new concepts, Python uses a programming pattern called object-oriented programming, which models concepts using classes and objects. This is a flexible, powerful paradigm where classes represent and define concepts, while objects are instances of classes. In the car example, we can create a class called car that defines its characteristics to the computer. We would be able to create thousands of instances of that class car, which are the individual objects of that class.', 'The idea of object-oriented programming might sound abstract and complex, but if you’ve programmed any software you might have already used objects without even realizing it. Almost everything in Python is an object, all of the numbers, strings, lists, and dictionaries are included in this type of element. The core concept of object-oriented programming comes down to attributes and methods associated with a type:', 'In the car example, color and brand are two attributes associated with every instance, or car, created with the program. On the other hand, methods would be actions performed with or by the car, such as driving. A more computer-oriented example would be a file in a directory, as every file has a name, a size and a date of when it was created.', 'As you can see in the image above, when we use the type function as we just did here, Python tells us which class the value or variable belongs to. And since integers and strings are classes, they have a bunch of attributes and methods associated with them. You can access attributes and methods of a class with the dir function in Python, as shown below:', 'In my previous article about strings, I explored a bunch of methods and attributes of the string class. Take a look at it for further insights about how to treat strings objects, which in that case will be a different instance of the string class. This means that they all have the same methods, although the content in the string is different.', 'These are called special methods and they aren’t usually called by those weird names. Instead, they’re called by some of the internal Python functions. for example, the __len__ method is called by the len function.', 'If you want to know what a specific method does, you should use the help function:', 'When we use the help function on any variable or value, we’re accessing all the documentation for the corresponding class. In this case, we’re looking at the documentation for the str and the int class.', 'Although Python comes with a lot of predefined classes for us, the power of object-oriented programming comes when we define our own classes with their own attributes and methods.', 'As mentioned earlier, the point of object oriented programming is to help define a real-world concept in a way that the computer understands. Let’s get hands on to build a new class with the car example:', 'Let’s clarify specific elements of the code:', 'How might we expand our definition of the Car class? It would probably have the same attributes that represent the information we want to associate with a car like brand and color.', 'Now, let’s proceed with the creation of an instance of the class Car, and assign it to a variable called “my_car”. To create a new instance of any class, we call the name of the class as if it were a function:', 'As you can see, I’m passing as an argument the brand and the color as I’ve configured my class to require both items in the creation of a new object of the class. As a result, we can call the attributes of the created instance and receive the value previously assigned:', 'The syntax used to access the attributes is called dot notation because of the dot used in the expression. Dot notation lets you access any of the abilities that the object might have, such as brand or color.', 'The attributes and methods of some objects can be other objects and can have attributes and methods of their own. For example, we could use the upper or the capitalize methods to modify both string attributes of my instance:', 'So far we’ve created one instance of the Car class and set its attributes. Now, we could create a new instance of the the class with different attributes:', 'Methods are essentially called to make objects do stuff. For example, upper and capitalize for strings. The key to learn the intuition of methods in O.O.P. is to understand that methods are functions that operate on the attributes of a specific instance of a class. When we call the lower method on the string, we’re making the contents of that specific string lowercase.', 'Let’s take a closer look by creating a new class called Dog and defining our own methods. First, we need to define the class and create an instance of it like we’ve done before with the Car class. While my dog might be great, it can’t perform any actions as long as I don’t define methods for them. Take a look at the example:', 'As shown in the image, we must start defining a method with the def keyword just like we would for a function, and indent to the right the body of the method, also as we would for a function.', 'The function is receiving a parameter called “self”. This parameter represents the instance that the method is being executed on.', 'Even though my dog barks, it will always do it in the same way. We can change how it will bark with a simple modification in the code, in order to gain flexibility in the attributes and methods that we configure to our classes:', 'Both classes created up to this paragraph contain default values as attributes and methods. This is not an ideal scenario as it creates redundant code for each attribute, and more importantly, as it makes it really easy to forget to set an important value.', ""So, when writing code it's a good idea to set attributes and methods that will vary with instances upon creating the class, in order to make sure that each instance contains the same important attributes. To do this, we need to use a special method called Constructor."", ""The constructor of the class is the method that's called when you call the name of the class. It's always named init."", 'It contains the keyword self, which refers to the instance being created, and the arguments that will be passed as attributes by the programmer once the instance is created, like we do in the example below:', 'The second method of the class is the repr method, which tells Python to print a predetermined statement every time an instance of the class is called, as in the image above.', 'Want to know which is a specific method’s function? Refer to the help function introduced previously. As built-in classes include a guide to help users understand the intuition behind each method or attribute, we could also do this on our own classes, methods, and functions. We can do that by adding a Docstring.', 'A Docstring is a brief text that explains what something does.', 'Once you include documentation to your classes and objects, you’ll get much more information about the methods created that will facilitate re-usability of the code and help other users to understand it. Remember that the docstring always has to be indented at the same level of the block it’s documenting.', 'Another important aspect of object-oriented programming is Inheritance. Just like people have parents, grandparents, and so on, objects have an ancestry. The principle of inheritance lets a programmer build relationships between concepts and group them together. In particular, this allows us to reduce code duplication by generalizing our code.', 'For example, how can we define a representation of “other means of transport” apart from the car that we already created, or other pets apart from my dog. This grouping characteristic allows us to create other classes that share some of the attributes of the existing classes, but not all of them, in order to add other similar instances without rewriting existing code.', 'In Python, we use parentheses in the class declaration to show an inheritance relationship. In the transportation example above, I’ve used Python’s syntax to tell the computer that both the car and the train inherit from the MyTransports class. Because of this, they automatically have the same constructor, which sets the color and brand attributes.', 'With the inheritance technique, we can use the transportation class to store information that applies to all kinds of transports available, and keep car or train specific attributes in their own classes. You can think of the MyTransports class as the parent class, and the Car and Train classes as siblings.', 'An example that is closer to the tasks you would perform in an IT department would be a system that handles the employees at your company. You may have a class called employee, which could have the attributes for things like full name of the person, the username used in company systems, the groups the employee belongs to, and so on. The system could also have a manager class which also is an employee, but additional information associated with it, like the employees that report to a specific manager.', 'In an object-oriented language like python real-world concepts are represented by classes. Instances of classes are usually called objects. Such objects have attributes which are used to store information about them and we can make objects do work by calling their methods. The objective of this article was to give you a clear notion of how useful O.O.P. is for a programmer as it allows us to model real world concepts.', 'Thanks for taking the time to read my article! If you have any questions or ideas to share, please feel free to contact me to my email, or you can find me in the following social networks for more related content:']"
12/2020,Apple’s New M1 Chip is a Machine Learning Beast,Speed testing a (nearly) top-spec Intel-based 16-inch…,1K,12,https://towardsdatascience.com/@mrdbourke,https://towardsdatascience.com/apples-new-m1-chip-is-a-machine-learning-beast-70ca8bfa6203?source=collection_archive---------0-----------------------,11,11,"['Apple’s New M1 Chip is a Machine Learning Beast', 'Why test/compare them', 'Mac specs', 'The tests', 'Experiment 1: Final Cut Pro video export', 'Experiment 2: CreateML machine learning model training', 'Experiment 3: TensorFlow macOS code', 'TensorFlow code results', 'Portability', 'For sale: 16-inch MacBook Pro', 'Conclusion']",79,"['I watched the keynote and saw the graphs, the battery life, the instant wake. And they got me. I started to think, how could one of these new M1-powered MacBooks make their way into my life?', 'Of course, I didn’t need one but I kept wondering what story could I tell myself to justify purchasing another computer? Then I had it. My 16-inch MacBook Pro is too heavy to carry around all the time. Yeah, that’ll do. This 2.0 kg aluminium powerhouse is too much to be galavanting.', 'Wait… 2.0 kg, as in, 4.4 pounds?', 'That’s it?', 'Yes.', 'Wow. It’s not even that heavy.', 'C’mon now… let’s not let the truth get in the way of a good story.', 'I had it. My reason for placing an order on a shiny new M1 MacBook (or two). My 16-inch MacBook is too heavy to lug around to cafes and write code, words, edit videos and check emails sporadically.', 'And Apple seems to think their new M1 chip is 11x, 15x, 12x, 3x faster on a bunch of different things. Thought-provoking numbers but I’ve never measured any of these in the past.', 'All I care about is: can I do what I need to do, fast.', 'The last word of the previous sentence is the most important. I’ve become conditioned. Speed is a part of me now. Ever since the transition from hard drives to solid-state drives. And I’m not going back.', 'I bought the 16-inch in February 2020. I’d just completed a large project and was flush with cash, so I decided to future proof my work station. Since I edit videos daily and hate lag, I opted for the biggest dawg I could buy and basically maxed everything except for the storage (see the specs below).', 'Thankfully I’ve still got a friend at Apple who was able to apply their employee discount to the beast (shout out to Joey).', 'Anyway, we’ve discussed my primary criteria: speed. Let’s consider the others:', 'Why not?', 'But really, I’m a nerd. And an Apple fan. Plus, I wanted to see how my big-dawg-almost-top-of-the-line 16-inch MacBook Pro faired against the new M1 chip-powered MacBook’s.', 'Plus, I can’t remember being this excited for a new computing device since the original iPhone.', 'Other reasons include: carrying around a lighter laptop and tax benefits (if I buy another machine before the end of the year, I can claim it on tax).', 'Whenever I buy a new machine, I usually upgrade the RAM and the storage at least a step or two from baseline.', '512GB storage and 16GB RAM seems to be the minimum for me these days (seriously, who is running a 128GB MacBook effectively?).', 'So for the M1 MacBook’s, I upgraded both of their RAM from 8GB to 16GB and for the 13-inch Pro, I upgraded from 256GB to 512GB storage.', 'The 16-inch MacBook is my current machine, which I’ve never had a problem with until running the tests below.', 'Apple’s graphs were impressive. And the GeekBench scores were even more impressive. But these are just numbers on a page to me. I wanted to see how these machines performed doing what I’d actually do day-to-day:', 'Reflecting on the above, I devised three tests:', 'Why not test more extensively?', 'These are enough for me. I’ve got other sh*t to do.', 'Alright, time for the results. The best results for each experiment have been highlighted in bold.', 'For this one, all machines were given ample time to pre-render the raw footage. So when the export button got clicked, they all should’ve been relatively on the same page.', 'Experiment details:', 'No surprise here, the 16-inch MacBook Pro exported in the fastest time. Most likely because of the dedicated 8GB GPU or 64GB of RAM.', 'However, it seems using the dedicated GPU came at the cost of battery life drain and fan speed (in the video you can hear the fans going off like a jet).', 'During the video export, the M1-powered MacBook Air and MacBook Pro 13-inch remained completely silent (the MacBook Air had no choice, it doesn’t have a fan but the MacBook Pro 13-inch’s fan never turned on).', 'I’ve never actually used a machine learning model trained by CreateML. However, I decided to see how one of Apple’s custom apps would leverage their new silicon.', 'For this test, each MacBook was setup with the following CreateML settings:', 'Potentially the most surprising result of all the tests is that the M1 MacBook Air won this one by a clear margin, both in training time and battery life. All the while without a fan and one less GPU than the MacBook Pro 13-inch (the MacBook Air I used has an 8-core CPU, 7-core GPU M1 chip).', 'It’s also quite clear the CreateML app has potentially been optimised for the M1 chip. As, despite having 8 CPU cores and a dedicated GPU, the 16-inch Intel-powered MacBook Pro ran out of juice before finishing the experiment.', 'During the M1 announcement keynote, Apple claimed their new silicon was capable of “running popular deep learning frameworks such as TensorFlow” at much greater speeds than previous generations.', 'Hearing this forced me to sit up a little straighter.', 'Did they just say TensorFlow? Natively?', 'I read back through the announcement.', 'Yes. They did. They said TensorFlow.', ""Then I found the blog posts by the TensorFlow team and Apple Machine Learning team showcasing the new results on the M1 chips and Intel-based Macs. Turns out, Apple recently released a fork of TensorFlow, tensorflow_macos which allows you to run native TensorFlow code right on your Mac (something which was previously a pain in the ass, actually, not really, I hear PlaidML has made it easier but I haven't tried that)."", 'Naturally, after hearing this news, I had to try it.', 'By some miracle, I installed Apple’s fork of TensorFlow into a Python 3.8 environment without 8–10 hours of troubleshooting and created the following mini-experiments:', 'I copied the CNN architecture on the CNN explainer website (TinyVGG). And used similar data to the CreateML test.', 'These days I rarely build models from scratch. I use either an existing untrained architecture and train it on my own data or a pretrained architecture like EfficientNet and fine-tune it on my own data.', 'Browsing Apple’s tensorflow_macos GitHub, I came across an issue thread containing a benchmark a fair few people had run on their various machines. So I decided to include it in my testing.', 'As well as running the above three experiments on all of the MacBook’s I had, I also ran them on a GPU-powered Google Colab instance as a control (my usual workflow is: experiment on Google Colab, scale up on larger cloud servers when needed).', 'The Google Colab GPU-powered instance performed the fastest across all three tests.', 'Notably, the M1 machines significantly outperformed the Intel machine in the Basic CNN and Transfer learning experiments.', 'However, the Intel-powered machine clawed back some ground on the tensorflow_macos benchmark. I believe this was due to explicitly telling TensorFlow to use the GPU, using the lines:', 'I tried running these lines with the previous two experiments and it didn’t make a difference. Perhaps, it’s something to do with the different data loading schemes used between the Basic CNN/Transfer learning setup and the tensorflow_macosbenchmark.', 'See all code experiments in the accompanying Google Colab Notebook.', 'If you’re buying a laptop, you want to be able to move it around. Perhaps write some words while looking at the beach or code up your latest experiment whilst sipping coffee at your local cafe.', 'So what we have here is a compilation of the various amounts of battery used over the three experiments, plus a new portability score I’ve invented.', 'The M1 Macs crushed it here. Finishing all tests with over 30% of battery left each. The MacBook Air was again the standout, using the least amount of power and scoring the lowest on the portability score.', 'Anyone want to buy a second-hand close-to-top-spec MacBook Pro 16-inch? Its been well looked after, I promise.', 'After running the above experiments and using the M1 MacBooks for a couple of weeks, it looks like the graphs Apple showed were pretty close to reality. And all those raving reviews? Well, in my experience, they’re correct too.', 'I originally bought the 16-inch MacBook Pro to be a powerhouse, a speed machine. And it is that. But so are the M1 machines. Except the M1 machines are lighter, quieter and have better battery life.', 'The only test the MacBook Pro 16-inch won on was the video rendering test (and of course screen size). And even then, the results weren’t dramatically better, certainly not “this machine costs 2.5x more better”.', 'So what next?', 'I’m keeping the 13-inch MacBook Pro. The little extra boost for video editing won it over the 13-inch Air (plus, I edited the entire video version of this article on the 13-inch Pro without a single hiccup). I’ll use it as a portable machine and probably set my 16-inch up as a permanent desktop.', 'But the Air… Oh the Air… I remember when I worked at Apple Retail, I’d tell customers to get the Air if they were only concerned with word processing and browsing the web. Now you can train machine learning models on it.', 'Which one should you get?', '“I write code, words and browse the web.” Or “I just want a portable, capable machine.”', 'Get the MacBook Air, you won’t be disappointed. It’s what I’d get if I didn’t edit videos daily.', '“I write code, words, browse the web and edit videos.”', 'From my tests, the 13-inch M1 MacBook Pro or M1 MacBook Air will perform at 70–90% of what a nearly-top-spec Intel 16-inch MacBook Pro can offer, so either of those would be phenomenal. For a slight edge on video editing, you might opt for the 13-inch M1 MacBook Pro.', '“I want a larger screen size and don’t care about cost.”', 'As of now, the only valid reason I’d consider buying an Intel-based 16-inch MacBook Pro would be if screen size was of utmost importance to you and you didn’t have a budget.', 'Screen size doesn’t matter so much to me as most of the time I’m running a single full-screen app or two apps split down the middle of the screen. Or if I do want to run multiple apps, I’ll plug my machine into an external monitor (note: for now the M1 Macs only support a single external monitor, so for the 3+ monitor fans out there, you’ll need an Intel-based Mac).', 'This being said, even if you were after a larger screen, I’d hold off and wait for the Apple silicon 16-inch.', 'As I said, I had no problems with 16-inch MacBook Pro. But in comparison to these new M1 MacBook’s, it feels dated.', 'I haven’t been this impressed with a new computer since I first switched from hard drive to solid-state drive.', 'I’m writing this on the 13-inch M1 MacBook Pro and it feels like butter. The little things, the instance wake, the new keyboard style, the native Apple apps, the battery life, they all add up.', 'If Apple were capable of pulling off these kinds of performance improvements with a 1st-generation chip in a laptop (even one without a fan), I can’t imagine what’s going to happen on the machines without power constraints (the Mac mini hints at the potential here).', 'How about a 16-inch with an M2?', 'Hopefully they wait at least another year, I mean, my Intel-based 16-inch isn’t even a year old yet…', 'PS you can see the video version of this article, including all of the tests above being run on YouTube:']"
12/2020,Noam Chomsky on the Future of Deep Learning,-,2.3K,27,https://towardsdatascience.com/@dr00bot,https://towardsdatascience.com/noam-chomsky-on-the-future-of-deep-learning-2beb37815a3e?source=collection_archive---------1-----------------------,8,5,"['Noam Chomsky on the Future of Deep Learning', 'Who Is Noam Chomsky?', 'Universal Grammar', 'Artificial Neural Networks', 'Moral Relativism']",11,"['For the past few weeks, I’ve been engaged in an email exchange with my favourite anarcho-syndicalist Noam Chomsky. I reached out to him initially to ask whether recent developments in ANNs (artificial neural networks) had caused him to reconsider his famous linguistic theory Universal Grammar. Our conversation touched on the possible limitations of Deep Learning, how well ANNs really model biological brains and also meandered into more philosophical territory. I’m not going to quote Professor Chomsky directly in this article as our discussion was informal but I will attempt to summarise the key take-aways.', 'Noam Chomsky is first and foremost a professor of linguistics (considered by many to be “the father of modern linguistics”) but he is probably better known outside of academic circles as an activist, philosopher and historian. He is the author of over 100 books and was voted the world’s leading public intellectual in a 2005 poll conducted by magazines Foreign Policy and Prospect.', 'For the record, I am an admirer of Chomsky’s work, particularly his critiques of American imperialism, neo-liberalism and the media. Where our views have diverged slightly is in relation to his dismissal of continental philosophers (especially the French post-structuralists). Perhaps I have been poisoned by drawing too often from the wells of Foucault, Lacan and Derrida in early adulthood but I’ve always found Chomsky’s analytical approach to philosophy morally appealing but a little too “clean” to satisfactorily explain our world. While his disdain for these post-structuralist luminaries is conspicuous, Chomsky’s philosophical views are more nuanced than his detractors give him credit for.', 'I will declare from the outset that I am not a linguist, but in this section, I will try to give an overview of Universal Grammar theory. Before Chomsky, the predominant hypothesis in linguistics was that humans are born with minds that are “tabula rasa” (like a blank slate) and acquire language through reinforcement. That is, children hear their parents speak, they mimic the sounds that they hear and when they correctly use a word or structure a sentence they are praised. What Chomsky showed was that reinforcement is only part of the story and that there must be innate structures within the human brain that are universal and that facilitate language acquisition. His primary arguments were:', 'This theory of a genetically hard-coded language faculty became widely accepted in the scientific community, but the obvious next question was “what does this Universal Grammar actually look like?”. Intrepid researchers soon set out to discover shared properties across all human languages but there remains no consensus on what form our innate linguistic capacities take. It’s safe to assume that Universal Grammar does not consist of concrete syntactic rules, but is more likely to be a fundamental cognitive function. Chomsky has postulated that at some point in our history, humans developed the ability to perform a simple, recursive process called “Merge” and this is responsible for the properties and constraints of the syntactic structures we see within human languages. It’s a little bit abstract (and too involved to address properly here), but essentially “Merge” is the process of taking two objects and combining them to form a new object. While seemingly prosaic, the ability to mentally combine concepts, and to do this recursively, is deceptively powerful and allows us to construct an “infinite variety of hierarchically structured expressions”. Not only may this small but crucial genetic leap forward explain our aptitude for verbal communication, it also follows that it could be responsible (at least in part) for our mathematical talents and human creativity more broadly. This “Merge” mutation that occurred in one of our ancestors ~100k years ago, might be one of the key things that separate humans from other animals.', 'The primary reason I got in touch with Professor Chomsky, was because I wanted to hear his views on Artificial Neural Networks (a topic I know materially more about than linguistics). ANNs are a subset of machine learning models that are loosely modelled on the human brain and learn in a similar way (by seeing lots of examples). These models require very little hard-coding and can perform quite a broad array of complex tasks (e.g. image tagging, voice recognition, text generation) with relatively simple architectures. An instructive example of this approach is the AlphaGo Zero model developed by Google, which learnt to play the game Go (a complex and challenging board game) and ultimately became unbeatable by human world champions. Most impressively, it was trained to do all of this with no hard-coding or human intervention, that is “tabula rasa”. While ANNs are certainly not a perfect analogy for the human brain, I asked Professor Chomsky whether these models suggest that in fact we do not need hard-coded cognitive structures to learn from scattered data.', 'Chomsky correctly pointed out that ANNs are useful for highly specialised tasks, but these tasks must be sharply constrained (although their scope can appear vast given the memory and speed of modern computers). He compared ANNs to a massive crane working on a high rise building; while certainly impressive, both tools exist in systems with fixed bounds. This line of reasoning is congruent with my observation that all of the deep learning breakthroughs I have witnessed have occurred in very specific domains and we do not appear to be approaching anything like artificial general intelligence (whatever that means). Chomsky also pointed to mounting evidence that ANNs do not accurately model human cognition, which is so comparatively rich that the computational systems involved may even extend to the cellular level.', 'If Chomsky is right (and for what it’s worth I think he is) what are the implications for deep learning research moving forward? Ultimately there is nothing magical about the human brain. It is simply a physical structure composed of atoms and therefore it is entirely rational to believe that at some point in the future we may be able to create an artificial version that is capable of general intelligence. With that said, current ANNs offer only a simulacrum of this kind of cognition and by Chomsky’s logic, we won’t reach this next frontier without first improving our understanding of how organic neural networks operate.', 'The ethical use of AI is a salient concern for modern data scientists, but at times this domain can feel vague and subjective in an otherwise concrete field. Not only does Chomsky’s work provide a unique technical perspective on the future of deep learning, Universal Grammar also has profound moral implications since language is how we discuss and interpret the world. For example, Chomsky’s view is that the aforementioned innate neural structures preclude moral relativism and that there must exist universal moral constraints. There are many different flavours of moral relativism, but the core tenet is that there can be no objective basis for ethical determinations. Moral relativists assert that while we might believe deeply in statements such as “slavery is immoral”, we have no empirical way of proving this to somebody who disagrees since any proof will necessarily rely on value judgements and our values are ultimately exogenous and determined by culture and experience.', 'Chomsky contends that morality manifests in the brain and is, therefore, by definition, a biological system. All biological systems have variation (natural and due to divergent stimuli) but they also have limits. Consider the human visual system: experiments have shown that it contains some plasticity and is shaped by experience (especially in early childhood). By varying the data provided to the human visual system, you can literally alter the distribution of receptors and thereby change the way that an individual perceives horizontal and vertical lines. What you can not do, however, is turn a human eye into an insect eye, or give somebody the ability to see X-rays. According to Chomsky, biological systems (including morals) can vary quite widely but not infinitely. He goes on to say that even if you believe that our morality is entirely derived from culture, you still need to obtain that culture in the same way that you acquire any system (as a result of innate cognitive structures that are universal).', 'My initial reservation with this reading is that if we assume morality is simply a consequence of “Merge” (or something equally primitive), then while this may impose theoretical constraints, my intuition is that our morals may vary so wildly that it is practically impossible to make universal statements. In the past, Chomsky has discussed how moral progress appears to follow certain trends (e.g. acceptance of difference, rejection of oppression etc.) but I struggle to see how these broad trends would emerge consistently from such simple atomic cognitive structures. When I put this to Professor Chomsky, he argued that this view was illusory and that when we don’t understand things, they seem more diverse and complex than they really are. He gave the example of the variance seen in animal body plans since the Cambrian explosion. Merely 60 years ago, the dominant view in biology was that organisms vary so drastically that each must be studied on an individual basis, but we now know that this is completely wrong and that genetic variation between species is fairly slight. Variation in complex acquired systems must be minimal, otherwise we wouldn’t be able to acquire them.']"
12/2020,How Fast Is C++ Compared to Python?,PROGRAMMING,1.92K,51,https://towardsdatascience.com/@tamimi-naser,https://towardsdatascience.com/how-fast-is-c-compared-to-python-978f18f474c7?source=collection_archive---------2-----------------------,4,4,"['How Fast Is C++ Compared to Python?', 'A Short Introduction to DNA K-mers', 'The Challenge', 'Comparing Solutions']",16,"['There are millions of reasons to love Python (especially for data scientists). But how is Python different from more professional low-level programming languages like C or C++? I guess this is a question that many data scientists or Python users asked or will ask themselves one day. There are many differences between Python and languages like C++. For this article, I am going to show you how fast C++ is compared to Python with a super simple example.', 'To show the difference, I decided to go with a simple and practical task instead of an imaginary task. The task that I am going to accomplish is to generate all possible DNA k-mers for a fixed value of “k”. If you don’t know about DNA k-mers, I explain it in plain language in the next section. I chose this example because many genomic-related data processing and analysis tasks (e.g. k-mers generation) are considered computationally intensive. That’s a reason why many data scientists in the field of bioinformatics are interested in C++ (in addition to Python).', 'IMPORTANT NOTE: The goal of this article is not to compare C++ and Python in their most efficient way. Both codes could be written in a more efficient way and using much better approaches. The only goal of this article is to compare these two languages when they are going through exactly the same algorithm and instructions.', 'A DNA is a long chain of units called nucleotides. In DNA, there are 4 types of nucleotides shown with letters A, C, G, and T. Humans (or more precisely Homo Sapiens) have 3 billion nucleotide pairs. For example, a small portion of human DNA could be something like: ACTAGGGATCATGAAGATAATGTTGGTGTTTGTATGGTTTTCAGACAATT', 'In this example, if you choose any 4 consecutive nucleotides (i.e. letters) from this string, it will be a k-mer with a length of 4 (we call it a 4-mer). Here are some examples of 4-mers derived from the example.', 'ACTA, CTAG, TAGG, AGGG, GGGA, etc.', ""For this article, let's generate all possible 13-mers. Mathematically it is a permutation with a replacement problem. Therefore, we have 4¹³ (=67,108,864) possible 13-mers. I use a simple algorithm to generate results in C++ and Python. Let’s take a look at the solutions and comparing them."", 'To compare C++ and Python for this specific challenge easily, I used exactly the same algorithm for both languages. Both codes are intentionally designed to be simple and similar. I avoided using complex data structures or third-party packages or libraries. The first code is written in Python.', 'If you run the Python code, it will take 61.23 seconds to generate all 67 million 13-mers. To have a fair comparison, I commented out the lines that display k-mers (lines 25 and 37). If you like to display k-mers while they are being generated, you can uncomment those two lines. Note: it takes a long time to display all of them. Please use CTRL+C to abort the code if you need it.', 'Now, let’s take a look at the same algorithm in C++.', 'After compiling, if you run the code, it takes about 2.42 seconds to generate all 67 million 13-mers. It means Python takes 25 times more time to run the same algorithm compared to C++. I repeated the experiment for 14-mers and 15-mers (you need to change lines 12 in the Python code and 22 in the C++ code). Table 1 summarizes the results.', 'Clearly, C++ is much faster than Python in running the same algorithm and instructions. It is not a surprise to most programmers and data scientists, but the example shows that the difference is significant.', 'I want to emphasize again, that both codes are written in their simplest (and probably most inefficient) ways. There are tens of other approaches in Python that can improve the code performance and you must try them. As an example, we can write a simple code in Python that is almost as fast as C++ for this purpose.', 'Moreover, in this example, we did not use CPU or GPU parallelization, which must be done for these types of problems (embarrassingly parallel problems). Also, in this example, we did not involve memory heavily. If we stored the results (for example for some specific reasons), then the memory management could even make a more significant difference between C++ and Python runtimes.', 'This example and thousands of other challenges suggest that even data scientists should know about languages like C++ if they are working with a large amount of data or exponentially growing processes. Also, the article shows that there are many ways to write efficient codes in Python that should be explored and tested, before going after C++.', 'Follow me on Twitter for the latest stories: https://twitter.com/TamimiNas']"
12/2020,Microservice Architecture and its 10 Most Important Design Patterns,Microservice Architecture…,1K,8,https://towardsdatascience.com/@md.kamaruzzaman,https://towardsdatascience.com/microservice-architecture-and-its-10-most-important-design-patterns-824952d7fa41?source=collection_archive---------3-----------------------,20,5,"['Microservice Architecture and its 10 Most Important Design Patterns', 'Microservice Architecture', 'Design Patterns for Microservice Architecture', 'Conclusion', 'Similar Articles']",110,"[""Tackling complexity in large Software Systems was always a daunting task since the early days of Software development (1960's). Over the years, Software Engineers and Architects made many attempts to tackle the complexities of Software Systems: Modularity and Information Hiding by David Parnas (1972), Separation of Concern by Edsger W. Dijkstra (1974), Service Oriented Architecture (1998)."", 'All of them used the age-old and proven technique to tackle the complexity of a large system: divide and conquer. Since the 2010s, those techniques proved insufficient to tackle the complexities of Web-Scale applications or modern large-scale Enterprise applications. As a result, Architects and Engineers developed a new approach to tackle the complexity of Software Systems in modern times: Microservice Architecture. It also uses the same old “Divide and Conquer” technique, albeit in a novel way.', 'Software Design Patterns are general, reusable solutions to the commonly occurring problem in Software Design. Design Patterns help us share a common vocabulary and use a battle-tested solution instead of reinventing the wheel. In a previous article: Effective Microservices: 10 Best Practices, I have described a set of best practices to develop Effective Microservices. Here, I will describe a set of Design Patterns to help you implement those best practices. If you are new to Microservice Architecture, then no worries, I will introduce you to Microservice Architecture.', 'By reading this article, you will learn:', 'Please note that most of the Design Patterns of this listing have several contexts and can be used in non-Microservice Architecture. But I will describe them in the context of Microservice Architecture.', 'I have covered Microservice Architecture in details in my previous Blog Posts: Microservice Architecture: A brief overview and why you should use it in your next project and Is Modular Monolithic Software Architecture Really Dead?. If you are interested, then you can read them to have a deeper look.', 'What is a Microservice Architecture. There are many definitions of Microservice Architecture. Here is my definition:', 'Microservice Architecture is about splitting a large, complex systems vertically (per functional or business requirements) into smaller sub-systems which are processes (hence independently deployable) and these sub-systems communicates with each other via lightweight, language-agnostic network calls either synchronous (e.g. REST, gRPC) or asynchronous (via Messaging) way.', 'Here is the Component View of a Business Web Application with Microservice Architecture:', 'Important Characteristics of Microservice Architecture:', 'Advantages of Microservice Architecture:', 'Disadvantages of Microservice Architecture:', 'When to use Microservice Architecture:', 'Once a company replaces the large monolithic system with many smaller microservices, the most important decision it faces is regarding the Database. In a monolithic architecture, a large, central database is used. Many architects favor keeping the database as it is, even when they move to microservice architecture. While it gives some short-term benefit, it is an anti-pattern, especially in a large-scale system, as the microservices will be tightly coupled in the database layer. The whole object of moving to microservice will fail (e.g., team empowerment, independent development).', 'A better approach is to provide every Microservice its own Data store, so that there is no strong-coupling between services in the database layer. Here I am using the term database to show a logical separation of data, i.e., the Microservices can share the same physical database, but they should use separate Schema/collection/table. It will also ensure that the Microservices are correctly segregated according to the Domain-Driven-Design.', 'Pros', 'Cons', 'When to use Database per Microservice', 'When not to use Database per Microservice', 'Enabling Technology Examples', 'All SQL, NoSQL databases offer logical separation of data (e.g., separate tables, collections, schemas, databases).', 'Further Reading', 'In a Microservice Architecture, especially with Database per Microservice, the Microservices need to exchange data. For resilient, highly scalable, and fault-tolerant systems, they should communicate asynchronously by exchanging Events. In such a case, you may want to have Atomic operations, e.g., update the Database and send the message. If you have SQL databases and want to have distributed transactions for a high volume of data, you cannot use the two-phase locking (2PL) as it does not scale. If you use NoSQL Databases and want to have a distributed transaction, you cannot use 2PL as many NoSQL databases do not support two-phase locking.', 'In such scenarios, use Event based Architecture with Event Sourcing. In traditional databases, the Business Entity with the current “state” is directly stored. In Event Sourcing, any state-changing event or other significant events are stored instead of the entities. It means the modifications of a Business Entity is saved as a series of immutable events. The State of a Business entity is deducted by reprocessing all the Events of that Business entity at a given time. Because data is stored as a series of events rather than via direct updates to data stores, various services can replay events from the event store to compute the appropriate state of their respective data stores.', 'Pros', 'Cons', 'When to use Event Sourcing', 'When not to use Event Sourcing', 'Enabling Technology Examples', 'Event Store: EventStoreDB, Apache Kafka, Confluent Cloud, AWS Kinesis, Azure Event Hub, GCP Pub/Sub, Azure Cosmos DB, MongoDB, Cassandra. Amazon DynamoDB,', 'Frameworks: Lagom, Akka, Spring, akkatecture, Axon, Eventuate', 'Further Reading', 'If we use Event Sourcing, then reading data from the Event Store becomes challenging. To fetch an entity from the Data store, we need to process all the entity events. Also, sometimes we have different consistency and throughput requirements for reading and write operations.', ""In such use cases, we can use the CQRS pattern. In the CQRS pattern, the system's data modification part (Command) is separated from the data read (Query) part. CQRS pattern has two forms: simple and advanced, which lead to some confusion among the software engineers."", 'In its simple form, distinct entity or ORM models are used for Reading and Write, as shown below:', 'It helps to enforce the Single Responsibility Principle and Separation of Concern, which lead to a cleaner design.', ""In its advanced form, different data stores are used for reading and write operations. The advanced CQRS is used with Event Sourcing. Depending on the use case, different types of Write Data Store and Read Data store are used. The Write Data Store is the “System of Records,” i.e., the entire system's golden source."", 'For the Read-heavy applications or Microservice Architecture, OLTP database (any SQL or NoSQL database offering ACID transaction guarantee) or Distributed Messaging Platform is used as Write Store. For the Write-heavy applications (high write scalability and throughput), a horizontally write-scalable database is used (public cloud global Databases). The normalized data is saved in the Write Data Store.', 'NoSQL Database optimized for searching (e.g., Apache Solr, Elasticsearch) or reading (Key-Value data store, Document Data Store) is used as Read Store. In many cases, read-scalable SQL databases are used where SQL query is desired. The denormalized and optimized data is saved in the Read Store.', 'Data is copied from the Write store to the read store asynchronously. As a result, the Read Store lags the Write store and is Eventual Consistent.', 'Cons', 'When to use CQRS', 'When not to use CQRS', 'Enabling Technology Examples', 'Write Store: EventStoreDB, Apache Kafka, Confluent Cloud, AWS Kinesis, Azure Event Hub, GCP Pub/Sub, Azure Cosmos DB, MongoDB, Cassandra. Amazon DynamoDB', 'Read Store: Elastic Search, Solr, Cloud Spanner, Amazon Aurora, Azure Cosmos DB, Neo4j', 'Frameworks: Lagom, Akka, Spring, akkatecture, Axon, Eventuate', 'Further Reading', 'If you use Microservice Architecture with Database per Microservice, then managing consistency via distributed transactions is challenging. You cannot use the traditional Two-phase commit protocol as it either does not scale (SQL Databases) or is not supported (many NoSQL Databases).', 'You can use the Saga pattern for distributed transactions in Microservice Architecture. Saga is an old pattern developed in 1987 as a conceptual alternative for long-running database transactions in SQL databases. But a modern variation of this pattern works amazingly for the distributed transaction as well. Saga pattern is a local transaction sequence where each transaction updates data in the Data Store within a single Microservice and publishes an Event or Message. The first transaction in a saga is initiated by an external request (Event or Action). Once the local transaction is complete (data is stored in Data Store, and message or event is published), the published message/event triggers the next local transaction in the Saga.', ""If the local transaction fails, Saga executes a series of compensating transactions that undo the preceding local transactions' changes."", 'There are mainly two variations of Saga transactions co-ordinations:', 'Cons', 'When to use Saga', 'When not to use Saga', 'Enabling Technology Examples', 'Axon, Eventuate, Narayana', 'Further Reading', ""In modern business application developments and especially in Microservice Architecture, the Frontend and the Backend applications are decoupled and separate Services. They are connected via API or GraphQL. If the application also has a Mobile App client, then using the same backend Microservice for both the Web and the Mobile client becomes problematic. The Mobile client's API requirements are usually different from Web client as they have different screen size, display, performance, energy source, and network bandwidth."", 'Backends for Frontends pattern could be used in scenarios where each UI gets a separate backend customized for the specific UI. It also provides other advantages, like acting as a Facade for downstream Microservices, thus reducing the chatty communication between the UI and downstream Microservices. Also, in a highly secured scenario where downstream Microservices are deployed in a DMZ network, the BFF’s are used to provide higher security.', 'Cons', 'When to use Backends for Frontends', 'When not to use Backends for Frontends', 'Enabling Technology Examples', 'Any Backend frameworks (Node.js, Spring, Django, Laravel, Flask, Play, …..) supports it.', 'Further Reading', 'In Microservice Architecture, the UI usually connects with multiple Microservices. If the Microservices are finely grained (FaaS), the Client may need to connect with lots of Microservices, which becomes chatty and challenging. Also, the services, including their APIs, can evolve. Large enterprises will like to have other cross-cutting concerns (SSL termination, authentication, authorization, throttling, logging, etc.).', ""One possible way to solve these issues is to use API Gateway. API Gateway sits between the Client APP and the Backend Microservices and acts as a facade. It can work as a reverse proxy, routing the Client request to the appropriate Backend Microservice. It can also support the client request's fanning-out to multiple Microservices and then return the aggregated responses to the Client. It additionally supports essential cross-cutting concerns."", 'Cons', 'When to use API Gateway', 'When not to use API Gateway', 'Enabling Technology Examples', 'Amazon API Gateway, Azure API Management, Apigee, Kong, WSO2 API Manager', 'Further Reading', 'If we want to use Microservice Architecture in a brownfield project, we need to migrate legacy or existing Monolithic applications to Microservices. Moving an existing, large, in-production Monolithic applications to Microservices is quite challenging as it may disrupt the application’s availability.', ""One solution is to use the Strangler pattern. Strangler pattern means incrementally migrating a Monolithic application to Microservice Architecture by gradually replacing specific functionality with new Microservices. Also, new functionalities are only added in Microservices, bypassing the legacy Monolithic application. A Facade (API Gateway) is then configured to route the requests between the legacy Monolith and Microservices. Once the functionality is migrated from the Monolith to Microservices, the Facade then intercepts the client request and route to the new Microservices. Once all the legacy monolith's functionalities are migrated, the legacy Monolithic application is “strangled,” i.e., decommissioned."", 'Cons', 'When to use Strangler', 'When not to use Strangler', 'Enabling Technology', 'Backend application frameworks with API Gateway.', 'Further Reading', 'In Microservice Architecture, where the Microservices communicates Synchronously, a Microservice usually calls other services to fulfill business requirements. Call to another service can fail due to transient faults (slow network connection, timeouts, or temporal unavailability). In such cases, retrying calls can solve fix the issue. However, if there is a severe issue (complete failure of the Microservice), then the Microservice is unavailable for a longer time. Retrying is pointless and wastes precious resources (thread is blocked, waste of CPU cycles) in such scenarios. Also, the failure of one service might lead to cascading failures throughout the application. In such scenarios, fail immediately is a better approach.', 'The Circuit Breaker pattern can come to the rescue for such use cases. A Microservice should request another Microservice via a proxy that works similarly to an Electrical Circuit Breaker. The proxy should count the number of recent failures that have occurred and use it to decide whether to allow the operation to proceed or simply return an exception immediately.', 'The Circuit Breaker can have the following three states:', 'Cons', 'When to use Circuit Breaker', 'When not to use Circuit Breaker', 'Enabling Technology', 'API Gateway, Service Mesh, various Circuit Breaker Libraries (Hystrix, Reselience4J, Polly.', 'Further Reading', 'Every business Application has many configuration parameters for various Infrastructure (e.g., Database, Network, connected Service addresses, credentials, certificate path). Also, in an enterprise environment, the application is usually deployed in various runtimes (Local, Dev, Prod). One way to achieve this is via the Internal Configuration, which is a fatal bad practice. It can lead to severe security risk as production credentials can easily be compromised. Also, any change in configuration parameter needs to rebuild the Application. This is even more critical in Microservice Architecture as we potentially have hundreds of services.', 'The better approach is to externalize all the Configurations. As a result, the build process is separated from the runtime environment. Also, it minimizes the security risk as the Production configuration file is only used during runtime or via environment variables.', 'Cons', 'When to use Externalized Configuration', 'When not to use Externalized Configuration', 'Enabling Technology', 'Almost all enterprise-grade, modern frameworks support Externalized Configuration.', 'Further Reading', 'In Microservice Architecture, there are many Microservices often developed by separate teams. These microservices work together to fulfill a business requirement (e.g., customer request) and communicate with each other Synchronously or Asynchronously. Integration testing of a Consumer Microservice is challenging. Usually, TestDouble is used in such scenarios for a faster and cheaper test run. But TestDouble often does not represent the real Provider Microservice. Also, if the Provider Microservice changes its API or Message, then TestDouble fails to acknowledge that. The other option is to make end-to-end testing. While end-to-end testing is mandatory before production, it is brittle, slow, expensive, and is no replacement for Integration testing (Test Pyramid).', 'Consumer-Driven contract testing can help us in this regard. Here, the Consumer Microservice owner team write a test suite containing its Request and expected Response (for Synchronous communication) or expected messages (for Asynchronous communication) for a particular Provider Microservice. These test suites are called explicit Contracts. For a Provider Microservice, all the Contract test suites of its Consumers are added in its automated test. When the automated test for a particular Provider Microservice is performed, it runs its own tests and the Contracts and verifies the Contract. In such a way, the contract test can help maintain the integrity of the Microservice Communication in an automated way.', 'Cons', 'When to use Consumer-Driven Contract Testing', 'When not to use Consumer-Driven Contract Testing', 'Enabling Technology', 'Pact, Postman, Spring Cloud Contract', 'Further Reading', 'In the modern large-scale enterprise Software development, Microservice Architecture can help development scaling with many long-term benefits. But Microservice Architecture is no Silver Bullet that can be used in every use case. If it is used in the wrong type of application, Microservice Architecture can give more pains as gains. The development team that wants to adopt Microservice Architecture should follow a set of best practices and use a set of reusable, battle-hardened design patterns.', 'The most vital design pattern in Microservice Architecture is the Database per Microservice. Implementing this design pattern is challenging and needs several other closely related design patterns (Event Sourcing, CQRS, Saga). In typical business applications with multiple Clients (Web, Mobile, Desktop, Smart Devices), the communications between Client and Microservices can be chatty and may require Central control with added Security. The design patterns Backends for Frontends and API Gateway are very useful in such scenarios. Also, the Circuit Breaker pattern can greatly help to handle error scenarios in such applications. Migrating legacy Monolithic application to Microservices is quite challenging, and the Strangler pattern can help the migration. The Consumer-Driven Contract Test is an instrumental pattern for the Integration Testing of Microservices. At the same time, Externalize Configuration is a mandatory pattern in any modern application development.', 'This list is not all-inclusive, and depending on your use case, you may need other design patterns. But this list will give you an excellent introduction to Microservice Architecture Design Patterns.']"
12/2020,How To Create A Fully Automated AI Based Trading System With Python,End-to-end project: get the data…,1.3K,6,https://towardsdatascience.com/@ruromgar,https://towardsdatascience.com/how-to-create-a-fully-automated-ai-based-trading-system-with-python-708503c1a907?source=collection_archive---------4-----------------------,12,6,"['How To Create A Fully Automated AI Based Trading System With Python', 'Getting the data', 'Adding the AI', 'Connecting to the broker', 'Deploy and monitoring', 'References:']",52,"['A couple of weeks ago I was casually chatting with a friend, masks on, social distance, the usual stuff. He was telling me how he was trying to, and I quote, detox from the broker app he was using. I asked him about the meaning of the word detox in this particular context, worrying that he might go broke, but nah: he told me that he was constantly trading. “If a particular stock has been going up for more than one hour or so and I’m already over the 1% profit threshold then I sell”, he said, “among other personal rules I’ve been following”. Leaving aside the slight pseudoscientific aspect of those rules, I understood what he meant by detox: following them implied checking the phone an astronomically high number of times.', 'So I started wondering: would it be possible to automate the set of rules this guy has in mind? And actually — would it be possible to automate a saner set of rules, so I let the system do the trading for me? Since you’re reading this I assume you got caught by the title, so you’ve probably already guessed that the answer is yes. Let’s elaborate on that, but first of all: time is gold and I don’t want to clickbait anyone. This is what we’re going to do:', 'And what are we going to need?', 'Everything I’ve coded is available here. Okay! So, without further ado, let’s go for the first part: getting the data.', 'Getting the data is not easy. Some years ago there was an official Yahoo! Finance API, as well as alternatives like Google Finance — sadly, both have been discontinued for years now. But don’t worry, there’s still plenty of alternatives in the market. My personal requirements were:', 'With that list in mind, I went for yfinance — the unofficial alternative to the old Yahoo Finance API. Bear in mind that for a real system, and based on the awesome list provided by Patrick Collins, I would definitely choose the Alpha Vantage API — but let’s keep it simple for now.', 'The yfinance library was developed by Ran Aroussi to get access to the Yahoo! Finance data when the official API was shut down. Quoting from the GitHub repository,', 'Ever since Yahoo! finance decommissioned their historical data API, many programs that relied on it to stop working.', 'yfinance aimes to solve this problem by offering a reliable, threaded, and Pythonic way to download historical market data from Yahoo! finance.', 'Sweet, good enough for me. How does it work? First we need to install it:', 'And then we can access everything using the Ticker object:', 'That method is quite fast, slightly above 0.005 seconds on average, and returns LOTS of info about the stock; for instance, google.info contains 123 fields, including the following:', 'There is more info available through several methods: dividends, splits, balance_sheet or earnings among others. Most of these methods return the data in a pandas DataFrame object, so we’ll need to play with it a bit to get whatever we want. For now I just need the information of the stock price through the time; the history method is the best one for that purpose. We can select both the period or the interval dates and the frequency of the data down to one minute — note that intraday information is only available if the period is minor than 60 days, and that only 7 days worth of 1m granularity data are allowed to be fetched per request. The transposed data of the last entry with a 1m interval is as follows:', 'We can see how it’s indexed by the datetime and every entry has seven features: four fixed points of the stock price during that minute (open, high, low and close) plus the volume, dividends and stock splits. I’m going to use just the low, so let’s keep that data:', 'Finally, since we’re going to use the data just for the last day, let’s reindex the dataframe to remove the date and timezone components and keep just the time one:', 'Looking good! We already know how to fetch the latest info from yfinance — we’ll later feed our algorithm with this. But for that, we need an algorithm to feed: let’s go for the next part.', 'I said it before but I’ll say this again: don’t try this at home. What I’m going to do here is fitting a VERY simple ARIMA model to forecast the next value of the stock price; think of it as a dummy model. If you want to use this for real trading, I’d recommend to look for better and stronger models, but be aware: if it were easy, everyone would do it.', 'First let’s split the dataframe into train and test, so we can use the test set to validate the results of the dummy model — I’m going to keep the last 10% of the data as the test set:', 'If we plot it, we get:', 'Now let’s fit the model with the training data and get the forecast. Note that the hyperparameters of the model are fixed whereas in the real world you should use cross-validation to get the optimal ones — check out this awesome tutorial about How To Grid Search ARIMA Hyperparameters With Python. I’m using a 5, 0, 1 configuration and getting the forecast for the moment immediately after the training data ends:', 'Let’s see how well performed our dummy model:', 'That’s not bad — we can work with it. With this info we can define a set of rules based on whatever we want to do, like holding if it’s going up or selling if it’s going down. I’m not going to elaborate on this part because I don’t want y’all to sue me saying you lost all your money, so please go ahead and define your own set of rules :) In the meantime, I’m going to explain the next part: connecting to the broker.', 'As you probably have guessed, this part highly depends on the broker you’re using. I’m covering here two brokers, RobinHood and Alpaca; the reason is that both of them:', 'Depending on the type of your account you might have some limits: for instance, RobinHood allows just 3 trades over a 5 day period if your account balance is below 25000$; Alpaca allows far more requests but still has a limit of 200 requests per minute per API key.', 'There are several libraries that wrap the RobinHood API, but sadly, as far as I know no one of them is official. Sanko’s library was the biggest one, with 1.5k stars in GitHub, but it has been discontinued; LichAmnesia’s has continued Sanko’s path, but has just 99 stars so far. I’m going to use robin_stocks library, which has a little over 670 stars at the moment of writing this. Let’s install it:', 'Not all actions require login, but most of them do, so it’s useful to login before doing anything else. RobinHood requires MFA, so it’s necessary to set it up: go to your account, turn on the two factor authentication and select “other” when asked about the app you want to use. You will be presented with an alphanumeric code, which you will use in the code below:', 'To buy or sell is pretty easy:', 'Check the docs for advanced usage and examples.', 'For Alpaca we are going to use the alpaca-trade-api library, which has over 700 stars in GitHub. To install:', 'After signing in your account you’ll get an API key ID and a secret key; both are needed for login:', 'Submitting orders is slightly more complex than with RobinHood:', 'That’s it! Note that leaving your credentials in plain text is a very, VERY bad thing to do — do not worry though, we’ll switch in the next step to environment variables, which is far safer. Now let’s deploy everything to the cloud and monitor it.', 'We are going to deploy everything in AWS Lambda. This wouldn’t be the best option for a production system, obviously, since Lambda does not have storage and we would want to store the trained model somewhere, for instance in S3. However, this will do for now — we’ll schedule the Lambda to run daily, training the model every time with the data from the current day. For monitoring purposes we’ll set up a Telegram bot that will send a message with the action to be taken and its outcome. Note that AWS Lambda is free up to a certain limit, but be aware of the quotas in case you want to send lots of messages.', 'The first thing on the to-do list is creating a bot. I followed the official instructions from Telegram:', 'Next step: deployment. There are several ways of deploying to Lambda. I’m going to use the serverless framework, so let’s install it and create a template:', 'That will create a scheduled_tg_bot folder with three files: .gitignore, serverless.yml, and handler.py. The serverless file defines the deployment: what, when, and how it is going to be run. The handler file will contain the code to run:', 'You need to change CHAT_ID to the ID of the group, the channel, or the conversation you want the bot to interact with. Here you can find how to get the ID from a channel and here is how to get the ID from a group.', 'Now, we’re going to define how to run the code. Open serverless.yml and write:', 'This code tells AWS the kind of runtime we want and propagates the Telegram token from our own environment so we don’t have to deploy it. Afterwards, we’re defining the cron to run the function daily at 21:00 UTC time.', 'The only thing left is to get the AWS credentials and set them, along with the token and the rest of variables, as environment variables before deploying. Getting the credentials is fairly easy:', 'From your AWS console:', 'That’s it. Now, let’s export the AWS credentials and the Telegram token. Open a terminal and write:', 'Install the necessary packages locally and finally, deploy everything to AWS:', 'We’re done! The bot will trade for us every day at 21:00 UTC time and will message us with the action performed. Not bad for a proof of concept — now I can tell my friend he can stop frantically checking his phone to trade :)', 'Note that all the resources we’ve used through this tutorial have their own documentation: I encourage y’all to go deeper on whatever you think is interesting — remember that this is just a toy system! However, as a toy system, I believe it is a good starting point for a richer, more complex product. Happy coding!', 'You can check the code in GitHub.', 'Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. In this case, as the author himself points out: do not attempt to trade without seeking professional advice. See our Reader Terms for details.', '[1] P. Collins, Best Stock APIs and Industry Landscape in 2020 (2020), Medium', '[2] R. Aroussi, Reliably download historical market data from Yahoo! Finance with Python (2019), Aroussy.com', '[3] J. Brownlee, How to Grid Search ARIMA Model Hyperparameters with Python (2017), Machine Learning Mastery', '[4] J. Brownlee, How to Make Out-of-Sample Forecasts with ARIMA in Python (2017), Machine Learning Mastery', '[5] Serverless team, AWS Python Scheduled Cron Example, GitHub']"
12/2020,Finding it difficult to learn programming? Here’s why.,You’ve tried and given up way too many times…,1.5K,13,https://towardsdatascience.com/@natassha6789,https://towardsdatascience.com/finding-it-difficult-to-learn-programming-heres-why-639024be0a13?source=collection_archive---------5-----------------------,7,3,"['Finding it difficult to learn programming? Here’s why.', 'Background — My Experience', 'How to become a good programmer']",65,"['You have spent countless hours doing YouTube tutorials, taking paid online courses, and reading introductory programming articles. Yet, it feels like there is a barrier you simply can’t break through. There are people out there writing complex code you don’t understand, and solving complex programming problems.', '“I can never become like them,” you think, awestruck. “How did they learn to do it?”', 'I’ll tell you one thing — they certainly weren’t born knowing how to code, neither are they more intelligent than you.', 'In this article, I will break down the steps you can take to overcome the fear of programming.', 'If you are passionate about entering a field that requires programming knowledge (like data science or software development), it is really important for you to overcome this fear. More than anything, coding anxiety is something that can hold you back from progress for years. Yet, it is something very few people talk about.', 'I was a straight A student in school.', 'I took pride in my ability to solve problems, and loved subjects like mathematics and science. I was a fast learner, and I barely made mistakes.', 'However, things changed when I finished high school.', 'Programming isn’t the same as the subjects taught in a high school class. The only way to learn is by making mistakes. As someone who wasn’t used to this, I was surprised at how long it took me to learn how to code.', '“I’m making way too many mistakes,” I thought.', 'Suddenly, I wasn’t the best at what I did anymore. I struggled at seemingly simple tasks — even setting up a programming environment.', 'I started thinking I wasn’t cut out for coding.', 'Everyone seemed better at it than me. I couldn’t even compile code on the Internet without errors, let alone understand it or write out my own program.', 'This led to a lot of frustration, and I gave up.', 'And it didn’t just happen once.', 'I think I tried learning how to code, and took online courses in different programming languages over 10 times.', 'And each time, thinking I wasn’t good enough, I gave up.', 'The problem I faced wasn’t lack of confidence. It was the opposite. I was too confident. I was so confident, that when things didn’t happen the way I wanted them to, I got frustrated and gave up.', 'My biggest mistake?', 'Thinking that programming was something that could be learnt in a short period of time, and not acknowledging that there was a learning curve. If I had understood and accepted that learning how to code from scratch was something that required effort and patience, I could have saved myself a lot of frustration and time.', 'In this article, I will break down the type of attitude you need to have in order to succeed in this field.', 'When I was giving up on learning to code, I thought that there were two types of people in the world — people who were cut out for programming, and people who weren’t.', 'It turns out I was right.', 'The people who are cut out for programming, however, aren’t necessarily more intelligent than you.', 'They just have a certain mindset, and an attitude that has helped them excel in the field. With the same attitude, you can get there too.', 'You will first need to acknowledge that you aren’t any good at it.', 'All you have done is taken an online course or two, and done some programming tutorials. Of course you aren’t good at it yet.', 'There are people who have dedicated their entire lives to the field, and you are just getting started.', 'Remember that the next time you see a bunch of complicated code you don’t understand. Instead of getting overwhelmed by how good the other person is at solving the problem, and how you’ll probably never get there, think about the time and effort they would’ve spent to get to their level.', 'It isn’t a competition. Just because they managed to solve a complex problem that you couldn’t, doesn’t mean they’re smarter than you.', 'They have put in more time and effort than you. To get there, you need to do the same.', 'Patience is arguably one of the most important character traits of a programmer. You need to be someone who has the ability to stare at a computer screen for hours.', 'A seemingly simple problem can take hours, or even days to solve.', 'You can only learn by sitting down and spending hours debugging code.', 'I found it really difficult to develop the patience to cultivate this habit.', 'If you are like me, and get bored easily or have a short attention span, you will need to spend a lot of time training yourself to be patient.', 'Once you start to learn the art of staring at the computer screen and putting in hours to solve a problem, you are a lot closer to reaching your goal of being a good programmer.', 'Stubbornness: dogged determination not to change one’s attitude or position on something.', 'Remember when you were a child, and your parents refused to buy you a toy you asked for?', 'You clamored, cried, and whined for hours. You refused to leave the store with them until they bought it for you, and caused a scene.', 'Finally, your parents relented. They gave up and bought you the toy.', 'That is exactly the kind of stubbornness you need when learning to code.', 'Refuse to take no for an answer.', 'Every time you get frustrated because you don’t know how to proceed, or a bunch of code doesn’t run, just do not give up.', 'Stay there, and show the same determination you did when you were a child. If you want this at least half as bad as you wanted the toy as a kid, then you will stay there and finish what you started.', 'Remember: If you really want it, no matter how frustrating things get, you will stay there and get through the difficult part.', 'Over-confidence is bad.', 'Over-confidence will prevent you from making progress, because you expect too much out of yourself.', 'There are people out there who are a lot better at it than you are. When you look at these people, or read their codes, it is natural to feel incompetent.', 'The first step towards making any kind of progress is acknowledging that they are better than you are.', 'And of course they are.', 'They have spent more time than you have, and have dedicated years to learning how to code. If you want to get better, you need to put in the time and effort too.', 'In fact, even seasoned programmers know that they aren’t great at coding. The ability to acknowledge that there is a lot left to learn is one of the most important traits of any programmer.', 'There is no way you can possibly know everything that the field entails. The tech industry is constantly evolving, and there is always going to be something new to learn.', 'Understand that you can’t possibly learn everything. At the same time, try to gain as much knowledge as you possibly can by keeping up with evolving technology.', 'Learning to code takes a lot of effort.', 'You will need to have the right attitude, and develop good habits. These habits that you develop will take you a long way in your career — more than intelligence, capability, or memory capacity.', 'In order to get over the fear of programming, you first need to understand that there is a learning curve. Embrace the fact that you aren’t great at it yet, and know that it is okay.', 'You learnt to ride a bike by falling down many times and getting back up again.', 'Think of programming as a bike.', 'You’ll fall down way too many times, but that is the only way to learn. As time passes, you will get the hang of it, and become better and better.', 'You just need to have the right amount of patience and stubbornness to get back up and continue every time you fall down.', 'That’s all for this article! I hope it helps you in your programming journey, and in overcoming any kind of anxiety you might have about learning to code.', 'Thanks for reading!', 'The difference between impossible and possible lies in a person’s determination — Tommy Lasorda']"
12/2020,How I Switched to Data Science,"My Journey, Mistakes, and Learnings",379,2,https://towardsdatascience.com/@sucky00,https://towardsdatascience.com/how-i-switched-to-data-science-f070d2b5954c?source=collection_archive---------6-----------------------,8,1,['How I Switched to Data Science'],36,"['This is very common to switch to data science. Most data scientists I know out there do not have a degree in data science. They switched from another area. I also know many people who are trying to switch from another major. I meet many people being confused if it is the right career track for them. Well, the decision is yours. Everyone may have a different journey and may have different opinions. In this article, I decided to share my own experience. For some people, it could be an interesting read and for some people, it might be helpful information.', 'It was a long process. I am starting with my background a little bit. If it feels too long for you, please feel free to skip to the data science area after two sections.', ""I do not have a data science or computer science background. My bachelor was in Civil Engineering. I only knew Microsoft Office and a little bit of Auto CAD as my computer skills. I worked in different construction companies for five years and never liked it. Then started my master's in Environmental Engineering in a public school in Texas. I worked as a research assistant for almost three years. That was a nice experience. After that, I was in the teaching profession for a couple of years."", 'I had no coding experience at all.', 'While I was teaching part-time, I decided to learn to code. Because there is a lot of buzz about how important it is to have coding skills in the twenty-first century!', 'I started searching for some free resources on Google. Because I had no idea at all how coding looks and feels like. I just wanted some free experience before spending any money on it. I came across the LaunchCode website. They had some free courses and practice platforms. I even didn’t have to install anything. The first course was an HTML course. As I did not have any idea and even did not talk about it to anyone, I started with HTML. When I learned to code a few lines of HTML and I saw some output, it was very thrilling. So I kept doing it. After HTML, I also took the CSS and JavaScript courses for free in the LaunchCode platform. Now I know that all was just introductory level courses.', 'At that time I was so naive that I thought I had learned enough to get a job and earn a lot of money!', 'I started applying for jobs and for sure didn’t even get an interview.', 'One morning I received an email that LaunchCode is offering a free BootCamp in Miami that is called CS50. Miami Dade College will host it. That was Harvard University’s introduction to the computer science course. They videotape their classrooms and LaunchCode uses that for their Bootcamp. That course is also available in edx and can be taken for free using the audit option.', 'I made a community of coders and got familiar with the essentials of coding like data structures and algorithms. They also teach some HTML and CSS. For the first time, I used SQL there. That experience gave me the realization that I have a long way to go before I can call myself a software developer. Yes, after LaunchCode, that was the goal. I wanted to be a web-based software developer.', 'I secretly wanted to work in the Data Science and Artificial Intelligence industry but I thought that’s probably too hard. I thought I needed more coding expertise before I can start that.', 'After Bootcamp, I kept practicing PHP, SQL, JavaScript, and WordPress! I was still teaching and was hating my job. I was desperate to find another career. I think most people start thinking of a different career because they are not happy in their current job or not making enough money. And then because of a lot of buzz about Data Science, focus shifts there.', 'I took more classes in Udemy and Udacity and was applying for jobs as a web developer. I worked as a web developer in a startup for a couple of years. While working there, I thought I probably know enough to try Data Science now.', 'I started learning Python in Udacity. Then I took a series of courses in Coursera:', 'Applied Data Science with Python', 'Natural Language Processing in TensorFlow', 'Convolutional Neural Networks in Tensorflow', 'All these courses teach how to use different libraries to analyze data and make predictive models. But I wanted to learn to develop the machine learning models from scratch, not only from a library. I found another great machine learning course offered by Professor Andrew Ng of Stanford University.', 'While taking this machine learning course, I realized I need to develop more statistics knowledge. I took a probability and statistics course in college. But that wasn’t enough. This course on Coursera helped me a lot:', 'Statistics in Python, offered by the University of Michigan.', 'It has three courses. The first course is more about exploratory data analysis. The second one is inferential statistics and the third one is model fitting.', 'As you can see I use Coursera a lot! It has a lot of great courses. But it takes some time to find a good course that is suitable for you. A lot of time I started a course and after I have done it halfway I realized this is not for me. So, there will be some time laps there. If you are totally new to Coursera and do not know how to take courses for free there, here is a six minutes tutorial for that:', 'After taking some of the courses, I did an internship and a few projects. I attended some conferences, presented some personal projects there, joined several Meetups, and made some good friends, and learned a lot from them. Currently, I am doing my MS in Applied Data Analytics at Boston University.', ""If you are a beginner, you may think, after learning that much why I felt the need for doing a master's."", 'I will write a separate article on that. But if you ask me, if I find it useful. My answer is yes. I am loving it. It was a great decision. On the other hand, If you ask me if it is possible to become a successful data scientist without a degree. My answer to that is also yes. I have met many data scientists in the meetups and data science conferences who do not have any data science or a computer science degree.', 'The first mistake I made was getting scared. I spent so much time learning web development and also worked as a front-end web developer knowing that I actually wanted to be a Data Scientist. I was desperate to change my teaching job as fast as I could.', 'Instead of getting desperate and being scared of trying something new, if I would just get into it, it would save me a few years.', ""The second mistake was to think that data science does not require much programming problem solving or algorithm skills. In fact, some experienced software engineers told me that. But as I went deeper into it, I realized I needed more algorithm skills. I went back and had to learn some more. Boston University’s Applied Data Analytics master's program starts with an advanced algorithm class. Also, the professor told us, we did not have enough time to go further and we needed to learn more in the course. Now, I am learning more by myself."", 'These were the two major mistakes.', 'I do not claim to be a hugely successful Data Scientist yet. But after I spent that much time learning, I think I am eligible to share some suggestions.', '3. If you do not have any math or technical background, you may have to spend some extra time. But there is nothing to be pessimistic about. If you want something, you need to spend time and put some effort into it.', 'Data Science is a multidisciplined zone. This profession can use people from different backgrounds.', 'I know people from a business background doing very well as a data scientist.', ""4. Network, network, and network! Join different data scientist's meetups, seminars, and conferences. It will give you a lot of perspectives and ideas. Also motivations. Only thing is, different people will keep giving you different suggestions. Don’t lose your focus. You need to make a track for yourself and stay on that. Because everyone is different. Whatever worked for somebody else may not work for you. So don’t change track seeing someone else’s experience. That way you will keep changing track every other month. Learn a little bit of everything but not enough of anything. That happens to a lot of self-learners."", 'I shared my own journey, some great resources I used on the way, and some of my ideas in this article. I hope it will be helpful for some of you. I am sure many people will relate to some of my experiences. Please feel free to ask if you have any questions for me.', 'Feel free to follow me on Twitter and like my Facebook page.']"
12/2020,Three Functions to Know in Python,"Learn how to use the map, filter, and reduce functions in python",811,6,https://towardsdatascience.com/@lmatalka90,https://towardsdatascience.com/three-functions-to-know-in-python-4f2d27a4d05?source=collection_archive---------7-----------------------,13,7,"['Three Functions to Know in Python', 'Lambda Expressions', 'Map Function', 'Filter Function', 'List Comprehensions vs. Map and Filter', 'Reduce Function', 'Conclusion']",65,"['There are three functions in python that provide vast practicality and usefulness when programming. These three functions, which provide a functional programming style within the object-oriented python language, are the map(), filter(), and reduce() functions. Not only can these functions be used individually, but they can also be combined to provide even more utility.', 'In this tutorial, we will cover these three functions and some examples of when they can be useful. But before we start, we need to review what lambda (or anonymous) functions are and how to write them.', 'Lambda expressions are used to create anonymous functions, or functions without a name. They are useful when we need to create a function that will only need to be used once (a throw-away function) and can be written in one line. As such, they can be very useful to use in functions that take in another function as an argument.', 'Lambda functions can have any number of parameters but can only have one expression. They generally have this format which yields a function object:', 'lambda parameters: expression', 'Lambda Expression with One ParameterLet’s create a lambda expression that returns the square of a number:', 'And that’s it! We first start with the lambda keyword, then the parameter num, a colon, and what you want that function to return, which is num**2.', 'Note that this function is anonymous, or does not have a name. So we cannot invoke the function at a later point. In addition, we did not write the return keyword. Everything after the colon is part of the expression that will be returned.', 'If we want to assign a lambda function to a variable so that it can be invoked later, we can do so by using an assignment operator:', 'We can then invoke or call this function the same way we would with a function that was defined with the def keyword. For example:', 'Lambda Expression with Multiple ParametersLet’s write a lambda function that has two parameters instead of just one. For example, we can write a lambda expression that returns the sum of two numbers:', 'As we can see, if we want our function to have multiple parameters in a lambda expression, we would just separate those parameters by a comma.', 'Conditional Statements in Lambda ExpressionsWe can also include if else statements in lambda expressions. We would just need to make sure that it is all on one line. For example, let’s create a function that takes in two numbers and returns the greater of those numbers:', 'Our lambda expression takes in two numbers, num1 and num2, and returns num1 if num1 is greater than num2, else, it returns num2. Obviously this function doesn’t take into account if the numbers are equal, as it will just return num2 in that case, however, we are just illustrating how we can use conditional statements within a lambda expression.', 'For more on lambda expressions:', 'Now that we understand what lambda functions are, let’s talk about the first function on today’s agenda: the map function.', 'Simply put, the map function provides us a way to apply a function to each element in an iterable object, such as lists, strings, tuples, etc… Thus, the map function takes in two arguments: the function we want to apply, and the iterable object we want to apply it to.', 'map(function, iterable)', 'For example, we have a list of numbers (the iterable object), and we want to create a new list that contains the squares of those numbers.', 'The map function will return a map object, which is an iterator. If we want to create a list from this map object, we would need to pass in our map object to the built-in list function.', 'What happened in this line of code?', 'The map function took the first element from num_list, which is a 1, and passed it in as an argument to the lambda function (since we passed that function in as the first argument to the map function). The lambda function took that argument, 1, and returned 1**2, or 1 squared, and that was added to our map object. The map function then took the second element from num_list, which is 2, and passed it in as an argument to the lambda function. The lambda function returned the square of 2, which is 4, which was then added to our map object. After it finished going through the num_list elements and the rest of our squared numbers were added to the map object, the list function casts this map object onto a list, and that list was assigned to the variable squared_list.', 'Let’s try a slightly more interesting example involving cryptography, specifically the caesar cipher. The caesar cipher encrypts a message by taking each letter in the message, and replacing it with a shifted letter, or a letter that is a specified number of spaces away in the alphabet. So if we choose the number of spaces to be 1, then each letter will be replaced with the letter 1 space away in the alphabet. So the letter a will be replaced with the letter b, the letter b will be replaced with the letter c, and so on. If we choose the number of spaces to be 2, then a will be replaced with c, and b will be replaced with d.', 'If while we are counting spaces we get to the end of the alphabet, then we go back to the beginning of the alphabet. In other words, the letter z will be replaced with the letter a (if we shift 1 space), or with the letter b (if we shift 2 spaces).', 'For example, if the message we want to encrypt is ‘abc’ and we choose the number of spaces to be 1, then the encrypted message will be ‘bcd’. If the message is ‘xyz’, then the encrypted message will be ‘yza’.', 'How can we use the map() function to accomplish this? Well, we are applying something to each element of an iterable object. In this case, the iterable object is a string, and we would like to replace each letter/element in our string by a different one. And the map function can do exactly that!', 'Let’s assume that all messages will be lowercase letters only. And the number of spaces will be a number between 0–26. Remember, we only want to replace letters with other letters. Thus, any non-letter elements, such as a space or symbol, will be unchanged.', 'We first need access to the lowercase alphabet. We can either write out a string with all the lowercase letters, or we can use the string module as follows:', 'Then, we can write our encrypt function as follows:', 'We create the function encrypt with two parameters: the message we want to encrypt, msg, and the number of spaces n we want to shift the letters by. The iterable we pass in to the map function is the message, msg. The function we pass in to the map function will be a lambda function, which takes each element from the msg string, and if the element is a letter in the alphabet, it replaces it with the shifted letter depending on the n value we pass in. It does so by taking the current index of that letter in the alphabet, or abc.index(x), adds the n value to it, and then takes the modulus of that sum. The modulus operator is used to start back at the beginning of the alphabet if we get to the end (if abc.index(x)+n is a number greater than 25). In other words, if the original letter was z (which will have the index 25 in the abc string we created above), and the n value is 2, then (abc.index(x)+n)%26 will end up being 27%26, and that will yield a remainder of 1. Thus replacing the letter z with the letter of the index 1 in the alphabet, which is b.', 'Remember that the map function will return a map object. Thus, we can use the string method join, which takes in an iterable (which the map object is, since it is an iterator, and all iterators are iterable), and then joins it into a string. The function then returns this string.', 'To decrypt a message, we can use the following decrypt function (notice how we are subtract n from abc.index(x) instead of adding it):', 'Simply put, the filter function will “filter out” an iterable object based on a specified condition. This condition will be decided by the function that we pass in.', 'filter(function, iterable)', 'The filter function takes in two arguments: the function that checks for a specific condition and the iterable we want to apply it to (such as a list). The filter function takes each element from our list (or other iterable) and passes it in to the function we give it. If the function with that specific element as an argument returns True, the filter function will add that value to the filter object (that we can then create a list from just like we did with the map object above). If the function returns False, then that element will not be added to our filter object. In other words, we can think of the filter function as filtering our list or sequence based on some condition.', 'For example, we have a list of numbers, and we want to create a new list that contains only the even numbers from our list.', 'This filter function will return a filter object, which is an iterator. If we want to create a list from this filter object, we would need to pass in our filter object to the built-in list function (just like we did with the map object).', 'So let’s break down what happened in this line of code:', 'The filter function took the first element from num_list, which is a 1, and passed it in as an argument to the lambda function (since we passed that function in as the first argument to the filter function). The lambda function then returns False, since 1 is not even, and 1 is not added to our filter object. The filter function then took the second element from num_list, which is 2, and passed it in as an argument to the lambda function. The lambda function returns True, since 2 is even, and thus 2 is added to our filter object. After it goes through the rest of the elements in num_list and the rest of the even numbers are added to our filter object, the list function casts this filter object onto a list, and that list was assigned to the variable list_of_even_nums.', 'If we recall, list comprehensions are used to create lists out of other sequences, either by applying some operation to the elements, by filtering through the elements, or some combination of both. In other words, list comprehensions can have the same functionality as the built-in map and filter functions. The operation applied to each element is similar to the map function, and if we add a condition to which elements are added to the list in the list comprehension, that’s similar to the filter function. Also, the expression that is added in the beginning of a list comprehension is similar to the lambda expression that can be used inside the map and filter functions.', 'This is a list comprehension that adds the square of the elements from 0 to 9, only if the element is even:', 'We can use the map and filter functions, along with lambda functions, to accomplish the same thing:', 'The function passed into the map function is a lambda expression that takes input x and returns its square. The list passed into the map function is a filtered list that contains the even elements from 0 to 9.', 'For more on list comprehensions:', 'Simply put, the reduce function is taking an iterable, such as a list, and reduces it down to a single cumulative value. The reduce function can take in three arguments, two of which are required. The two required arguments are: a function (that itself takes in two arguments), and an iterable (such as a list). The third argument, which is an initializer, is optional.', 'reduce(function, iterable[, initializer])', 'To use the reduce function, we need to import it as follows:', 'The first argument that reduce takes in, the function, must itself take in two arguments. Reduce will then apply this function cumulatively to the elements of the iterable (from left to right), and reduces it to a single value.', 'Let’s say the iterable we use is a list of numbers, such as num_list:', 'We would like to take the product of all the numbers in num_list. We can do so as follows:', 'Our iterable object is num_list, which is the list: [1,2,3,4,5]. Our lambda function takes in two arguments, x and y. Reduce will start by taking the first two elements of num_list, 1 and 2, and passes them in to the lambda function as the x and y arguments. The lambda function returns their product, or 1 * 2, which equals 2. Reduce will then use this accumulated value of 2 as the new or updated x value, and uses the next element in num_list, which is 3, as our new or updated y value. It then sends these two values (2 and 3) as x and y to our lambda function, which then returns their product, 2 * 3, or 6. This 6 will then be used as our new or updated x value, and the next element in num_list will be used as our new or updated y value, which is 4. It then sends 6 and 4 as our x and y values to the lambda function, which returns 24. Thus, our new x value is 24, and our new y value is the next element from num_list, or 5. These two values are passed to the lambda function as our x and y values, and it returns their product, 24 * 5, which equals 120. Thus, reduce took an iterable object, in this case num_list, and reduced it down to a single value, 120. This value is then assigned to the variable product. In other words, the x argument gets updated with the accumulated value, and the y argument gets updated from the iterable.', 'Mathematically, you can think of reduce as doing the following:', 'f(f(f(f(f(x,y),y),y),y),y)', 'So the first time the lambda function runs, it takes x and y (the first two elements in the list), and returns an output. That output for f(x,y) will then be used as the new x for the next iteration, and y will be the next element in the list, thus f(f(x,y),y). And so on.', 'Remember how we said that reduce can take in an optional third argument, the initializer? The default value for it is None. If we pass in an initializer, it will be used as the first x value by reduce (instead of x being the first element of the iterable). So if we pass in the number 2 in the above example as the initializer:', 'Then the first two values or arguments for x and y will be 2 and 1, respectively. All subsequent steps will be the same. In other words, the initializer is placed before the elements of our iterable in the calculation.', 'There are so many other scenarios of when we can use reduce.', 'For example, we can find the sum of the numbers in a list:', 'Or we can find the maximum number in a list:', 'Or the minimum number in a list:', 'And so many other applications!', 'Note: Python does have built-in functions such as max(), min(), and sum() that would have been easier to use for these three examples. However, the goal was to show how reduce() can be used to accomplish many different tasks.', 'We can also combine the reduce function along with the filter or map function. For example, if we have a list of numbers, but we only want to return the product of the odd numbers only, we can do so using reduce and filter as follows:', 'In other words, the iterable we pass in to the reduce function is a filter object, which only includes the odd numbers. We can think of it as a list of only the odd numbers. Then, the reduce function will take that filtered list and returns the product of those numbers.', 'In this tutorial, we first looked at how we can create lambda (or anonymous) functions. We then learned about the map function, which gives a way to apply some function to each element in an iterable object. We then looked at the filter function, and how it can be used to filter an iterable object based on some condition. Furthermore, we compared the map and filter functions to list comprehensions. Next, we looked at how the reduce function can be used to reduce an entire iterable object down into a single value. We also saw examples of all of these functions being used, including an example involving cryptography. Lastly, we learned how these functions can be combined in order to provide even more functionality.']"
12/2020,A Full-Length Machine Learning Course in Python for Free,Andrew Ng’s Machine Learning Course in Python,1.7K,7,https://towardsdatascience.com/@sucky00,https://towardsdatascience.com/a-full-length-machine-learning-course-in-python-for-free-f2732954f35f?source=collection_archive---------8-----------------------,7,1,['A Full-Length Machine Learning Course in Python for Free'],27,"['One of the most popular Machine-Leaning course is Andrew Ng’s machine learning course in Coursera offered by Stanford University. I tried a few other machine learning courses before but I thought he is the best to break the concepts into pieces make them very understandable.', 'But I think, there is just only one problem. That is, all the assignments and instructions are in Matlab. I am a Python user and did not want to learn Matlab. So, I just learned the concepts from the lectures and developed all the algorithms in Python.', 'I explained all the algorithms in my own way(as simply as I could) and demonstrated the development of almost all the algorithms in the different articles before. I thought I should summarise them all on one page so that if anyone wants to follow, it is easier for them. Sometimes a little help goes a long way.', 'If you want to take Andrew Ng’s Machine Learning course, you can audit the complete course for free as many times as you want.', 'Let’s dive in!', 'The most basic machine learning algorithm. This algorithm is based on the very basic straight line formula we all learned in school:', 'Y = AX + B', 'Remember? If not, no problem. This is a very simple formula. Here is the complete article that explains how this simple formula can be used to make predictions.', 'The article above works on only the datasets with a single variable. But in real life, most datasets have multiple variables. Using the same simple formula, you can develop the algorithm with multiple variables:', 'This one is also a sister of linear regression. But polynomial regression is able to find the relationship between the input variables and the output variable more precisely, even if the relationship between them is not linear:', 'Logistic regression is developed on linear regression. It also uses the same simple formula of a straight line. This is a widely used, powerful, and popular machine learning algorithm. It is used to predict a categorical variable. The following article explains the development of logistic regression step by step for binary classification:', 'Based on the concept of binary classification, it is possible to develop a logistic regression for multiclass classification. At the same time, Python has some optimization functions that help to do the calculation a lot faster. In the following article, I worked on both the methods to perform a multiclass classification task on a digit recognition dataset:', 'Neural Network has been getting more and more popular nowadays. If you are reading this article, I guess you heard of neural networks.', 'A neural network works much faster and much efficiently in more complex datasets. This one also involves the same formula of a straight line but the development of the algorithm is a bit more complicated than the previous ones. If you are Andrew Ng’s course, probably, you know the concepts already. Otherwise, I tried to break down the concepts as much as I could. Hopefully, it is helpful:', 'What if you spent all that time and developed an algorithm and then, it does not work the way you wanted. How do you fix it? You need to figure out first where the problem is. Is your algorithm faulty or you need more data to train the model or you need more features? So many questions, right? But if you do not figure out the problem first and keep moving in any direction, it may kill too much time unnecessarily. Here is how you may find the problem:', 'On the other hand, if the dataset is too skewed that is another type of challenge. For example, if you are working on a classification problem, where 95% of cases it is positive and only 5% of cases are negative. In that case, if you just randomly put all the output as positive, you are 95% correct. On the other hand, if the machine learning algorithm turns out to be 90% accurate, it is still not efficient, right? Because without a machine learning algorithm, you can predict with 95% accuracy. Here are some ideas to deal with these types of situation:', 'One of the most popular and old unsupervised learning algorithms. This algorithm does not make predictions like the previous algorithms. It makes clusters based on the similarities amongst the data. It is more like understanding the current data more effectively. Then whenever the algorithm sees new data, based on its characteristics, it decides which cluster it belongs to. This algorithm has other importance as well. It can be used for the dimensionality reduction of images.', 'Why do we need dimensionality reduction of an image?', 'Think, when we need to input a lot of images to an algorithm to train an image classification model. Very high-resolution images could be too heavy and the training process can be too slow. In that case, a lower-dimensional picture will do the job with less time. This is just one example. You probably can imagine, there are a lot of uses for the same reason.', 'This article is a complete tutorial on how to develop a K mean clustering algorithm and how to use that algorithm for dimensionality reduction of an image:', 'Another core machine learning task. Used in credit card fraud detection, to detect faulty manufacturing or even any rare disease detection or cancer cell detection. Using the Gaussian distribution(or normal distribution) method or even more simply a probability formula it can be done. Here is a complete step by step guide for developing an anomaly detection algorithm using the Gaussian distribution concepts:', 'If you need a refresher on a Gaussian distribution method, please check this one:', 'The recommendation system is everywhere. If you buy something on Amazon, it will recommend you some more products you may like, YouTube recommends the video you may like, Facebook recommends people you may know. So, we see it everywhere.', 'Andrew Ng’s course teaches how to develop a recommender system using the same formula we used in linear regression. Here is the step by step process of developing a movie recommendation algorithm:', 'Hopefully, this article will help some people to start with machine learning. The best way is by doing. If you notice most of the algorithms are based on a very simple basic formula. I see a notion that machine learning or Artificial Intelligence requires very heavy programming knowledge and very difficult math. That’s not always true. With simple codes, basic math, and stats knowledge, you can go a long way. At the same time, keep improving your programming skills to do more complex tasks.', 'If you are interested in machine learning, just take some time and start working on it.', 'Feel free to follow me on Twitter and like my Facebook page.']"
12/2020,A Complete 52 Week Curriculum to Become a Data Scientist in 2021,Learn something every week for 52…,2.5K,10,https://towardsdatascience.com/@terenceshin,https://towardsdatascience.com/a-complete-52-week-curriculum-to-become-a-data-scientist-in-2021-2b5fc77bd160?source=collection_archive---------9-----------------------,10,13,"['A Complete 52 Week Curriculum to Become a Data Scientist in 2021', 'Introduction', 'Course Structure', 'Statistics & Probability', 'Mathematics', 'SQL', 'Python and Programming', 'Pandas', 'Visualizing Data', 'Data Exploration and Preparation', 'Machine Learning', 'Week 52: Data Science Project', 'Thanks for Reading!']",40,"['“Everyone wants to eat, but few are willing to hunt”', 'If you want to be a data scientist but haven’t yet made the commitment, now is the time.', 'Last year, I made a commitment to learn something new about data science every week for 52 weeks, and I think that was one of the best decisions I’ve ever made. You’d be surprised at how far you can get in a year.', 'And so, I’m presenting to you a complete 52 weeks curriculum that you can do in 2021 as a new years resolution! It is crammed and it will be a grind, but it will be worth it.', 'Immediately, you’ll notice that this guide does not start with machine learning, and I have good reasons for that. If you want to learn more about why machine learning comes in the later weeks, check out my article:', 'A couple of notes before we dive into it:', 'With that said, let’s dive into it!', 'Data science and machine learning are essentially a modern version of statistics. By learning statistics first, you’ll have a much easier time when it comes to learning machine learning concepts and algorithms! Even though it may seem like you’re not getting anything tangible out of the first few weeks, it will be worth it in the later weeks.', 'Like statistics, many data science concepts build on fundamental mathematical concepts.', 'In order to understand cost functions, you need to know differential calculus. In order to understand hypothesis testing, you need to understand integration. And to give more one more example, linear algebra is essential to learning deep learning concepts, recommendation systems, and principal component analysis.', 'SQL is arguably the most important skill to learn across any type of data-related profession, whether you’re a data scientist, data engineer, data analyst, business analyst, the list goes on.', 'At its core, SQL is used to extract (or query) specific data from a database, so that you can do things like analyze the data, visualize the data, model the data, etc. Therefore, developing strong SQL skills will allow you to take your analyses, visualizations, and modeling to the next level because you will be able to extract and manipulate the data in advanced ways.', 'I came across Mode’s curriculum a while back and it is fantastic! So I would first get familiar with using SQL in Mode and then you’ll be able to go through the topics below!', 'I started with Python, and I’ll probably stick with Python for the rest of my life. It’s so far ahead in terms of open source contributions, and it’s straightforward to learn. Feel free to go with R if you want, but I have no opinions or advice to provide regarding R.', 'Arguably the most important library to know in Python is Pandas, which is specifically meant for data manipulation and analysis.', 'The ability to visualize data and insights is so important because it’s the easiest way to communicate intricate information and a lot of information at once. As a data scientist, you’re always selling yourself and your ideas, whether your pitching a new project or convincing others why your model should be productionalized — data visualizations are a great tool to help you with that.', 'There are dozens of data visualization libraries out there, but I’m going to focus on two: Matplotlib and Plotly.', '“Garbage in, garbage out”', 'The models that you create can only be as good as the data that you feed into it. To understand what the state of your data is in, i.e. whether it’s “good” or not, you have to explore the data and prepare the data. Therefore, for the next four weeks, I’m going to provide several amazing resources for you to go through and get a better understanding of what data exploration and preparation entails.', 'Exploratory Data Analysis (EDA) can be difficult because there’s no one set way of doing it — but that’s also what keeps it exciting. Generally, you want to…', 'Check out a beginner’s guide to EDA here.', 'Everything that you’ve learned has led up to this point! Not only is machine learning interesting and exciting, but it is also a skill that all data scientists have. It’s true that modeling makes up a small portion of a data scientist’s time, but it doesn’t take away from its importance.', 'Later in your career, you might notice that I left out some machine learning algorithms, like K Nearest-Neighbors, Gradient Boost, and CatBoost. This is completely intentional — if you can understand the following machine learning concepts, you’ll have the skills to learn any other machine learning algorithms in the future.', 'If you feel comfortable with the materials above, you’re definitely ready to start your own data science project! Just in case, I’ve provided three ideas that you can use as inspiration to get started, but feel free to do whatever you like.', 'Link to the case.', 'The objective of this case is to determine the cause for a drop in user engagement for a social network called Yammer. Before diving into the data, you should read the overview of what Yammer does here. There are 4 tables that you should work with.', 'The link to the case above will provide you with much more detail pertaining to the problem, the data, and the questions that should be answered.', 'Check out how I approached this case study here if you’d like guidance.', 'Skills You’ll Develop', 'Learning how to webscrape data is simple to learn and extremely useful, especially when it comes to collecting data for personal projects. Scraping a customer review website, like Trustpilot, is valuable for a company as it allows them to understand review trends (getting better or worse) and see what customers are saying via NLP.', 'First I would get familiar with how Trustpilot is organized, and decide upon which kinds of businesses to analyze. Then I would take a look at this walkthrough of how to scrape Trustpilot reviews.', 'Skills You’ll Develop', 'In my opinion, there’s no better way of showing that you’re ready for a data science job than to showcase your code through competitions. Kaggle hosts a variety of competitions that involves building a model to optimize a certain metric, one of them being the Titanic Machine Learning Competition.', 'If you want to get some inspiration and guidance, check out this step-by-step walkthrough of one of the solutions.', 'Skills You’ll Develop', 'I hope you found this useful! If you managed to get through this, you should have a strong understanding of the fundamentals in Statistics, Mathematics, SQL, Python/Pandas, and several machine learning algorithms!', 'I hope this inspired you to continue learning too — there are so many things that you can continue to explore like more advanced models (eg. CatBoost), deep learning, experimental design, Bayesian modeling, cloud architecture, and the list goes on.', 'If you like this and want to see future content, be sure to give me a follow on Medium. And as always, I wish you the best in your data science endeavors.', 'Not sure what to read next? I’ve picked another article for you:', 'and another!']"
